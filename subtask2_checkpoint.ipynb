{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ErN5cQXY2Rl"
      },
      "source": [
        "Installing transformers library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iarqCnFuHHAs",
        "outputId": "43cd0dc6-0d17-4830-8c7e-7ea9d61f4645"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/ubuntu/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/ubuntu/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: transformers in /home/ubuntu/anaconda3/lib/python3.8/site-packages (4.26.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from transformers) (1.24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from transformers) (2021.4.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from transformers) (4.59.0)\n",
            "Requirement already satisfied: filelock in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: requests in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from requests->transformers) (1.26.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2020.12.5)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/ubuntu/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/ubuntu/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/ubuntu/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/ubuntu/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGU0qNGjh3hW"
      },
      "source": [
        "*Split-sentence*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEGm5nXchdFq"
      },
      "outputs": [],
      "source": [
        "#this function takes a sentence as an argumente and returns the tokens or words by splitting the sentence using space and comma.\n",
        "def splitSentence(sentence):\n",
        "  split_sentence = sentence.split(\",\") # split by commas\n",
        "  split_sentence = [word.split() for word in split_sentence] # split each word by space\n",
        "  split_sentence = [word for sublist in split_sentence for word in sublist] # flatten the list of lists\n",
        "\n",
        "  return split_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-yT908GY2Rs"
      },
      "source": [
        "Importing the necesssary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9Yv08skK8w-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import spacy\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBcIEgyjZCtb",
        "outputId": "757fdeb9-92a0-468a-e90f-4b85ea2d6b91"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "T-eqLTX2WLRp",
        "outputId": "55a6b09b-8f8c-4fd4-bcad-e5c96300d687"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Aspect Term</th>\n",
              "      <th>polarity</th>\n",
              "      <th>from</th>\n",
              "      <th>to</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3121</td>\n",
              "      <td>But the staff was so horrible to us.</td>\n",
              "      <td>staff</td>\n",
              "      <td>negative</td>\n",
              "      <td>8</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2777</td>\n",
              "      <td>To be completely fair, the only redeeming fact...</td>\n",
              "      <td>food</td>\n",
              "      <td>positive</td>\n",
              "      <td>57</td>\n",
              "      <td>61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1634</td>\n",
              "      <td>The food is uniformly exceptional, with a very...</td>\n",
              "      <td>food</td>\n",
              "      <td>positive</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1634</td>\n",
              "      <td>The food is uniformly exceptional, with a very...</td>\n",
              "      <td>kitchen</td>\n",
              "      <td>positive</td>\n",
              "      <td>55</td>\n",
              "      <td>62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1634</td>\n",
              "      <td>The food is uniformly exceptional, with a very...</td>\n",
              "      <td>menu</td>\n",
              "      <td>neutral</td>\n",
              "      <td>141</td>\n",
              "      <td>145</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     id                                           Sentence Aspect Term  \\\n",
              "0  3121               But the staff was so horrible to us.       staff   \n",
              "1  2777  To be completely fair, the only redeeming fact...        food   \n",
              "2  1634  The food is uniformly exceptional, with a very...        food   \n",
              "3  1634  The food is uniformly exceptional, with a very...     kitchen   \n",
              "4  1634  The food is uniformly exceptional, with a very...        menu   \n",
              "\n",
              "   polarity  from   to  \n",
              "0  negative     8   13  \n",
              "1  positive    57   61  \n",
              "2  positive     4    8  \n",
              "3  positive    55   62  \n",
              "4   neutral   141  145  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"Restaurants_Train_v2.csv\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "tDQzvbOSWNJR",
        "outputId": "fbeb936d-8a40-4788-a9de-332a0915fe50"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Aspect Term</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>But the staff was so horrible to us.</td>\n",
              "      <td>staff</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>To be completely fair, the only redeeming fact...</td>\n",
              "      <td>food</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The food is uniformly exceptional, with a very...</td>\n",
              "      <td>food</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The food is uniformly exceptional, with a very...</td>\n",
              "      <td>kitchen</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The food is uniformly exceptional, with a very...</td>\n",
              "      <td>menu</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3688</th>\n",
              "      <td>Each table has a pot of boiling water sunken i...</td>\n",
              "      <td>pot of boiling water</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3689</th>\n",
              "      <td>Each table has a pot of boiling water sunken i...</td>\n",
              "      <td>meats</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3690</th>\n",
              "      <td>Each table has a pot of boiling water sunken i...</td>\n",
              "      <td>vegetables</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3691</th>\n",
              "      <td>Each table has a pot of boiling water sunken i...</td>\n",
              "      <td>rice</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3692</th>\n",
              "      <td>Each table has a pot of boiling water sunken i...</td>\n",
              "      <td>glass noodles</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3693 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Sentence           Aspect Term  \\\n",
              "0                  But the staff was so horrible to us.                 staff   \n",
              "1     To be completely fair, the only redeeming fact...                  food   \n",
              "2     The food is uniformly exceptional, with a very...                  food   \n",
              "3     The food is uniformly exceptional, with a very...               kitchen   \n",
              "4     The food is uniformly exceptional, with a very...                  menu   \n",
              "...                                                 ...                   ...   \n",
              "3688  Each table has a pot of boiling water sunken i...  pot of boiling water   \n",
              "3689  Each table has a pot of boiling water sunken i...                 meats   \n",
              "3690  Each table has a pot of boiling water sunken i...            vegetables   \n",
              "3691  Each table has a pot of boiling water sunken i...                  rice   \n",
              "3692  Each table has a pot of boiling water sunken i...         glass noodles   \n",
              "\n",
              "      polarity  \n",
              "0     negative  \n",
              "1     positive  \n",
              "2     positive  \n",
              "3     positive  \n",
              "4      neutral  \n",
              "...        ...  \n",
              "3688   neutral  \n",
              "3689   neutral  \n",
              "3690   neutral  \n",
              "3691   neutral  \n",
              "3692   neutral  \n",
              "\n",
              "[3693 rows x 3 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = df.drop('id',axis=1)\n",
        "df = df.drop('from',axis=1)\n",
        "df = df.drop('to',axis=1)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "gtMBfPRhjaG_",
        "outputId": "039baed2-ca20-4290-feec-28c764b75241"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Aspect Term</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>But the staff was so horrible to us.</td>\n",
              "      <td>staff</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>To be completely fair, the only redeeming fact...</td>\n",
              "      <td>food</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The food is uniformly exceptional, with a very...</td>\n",
              "      <td>food</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The food is uniformly exceptional, with a very...</td>\n",
              "      <td>kitchen</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The food is uniformly exceptional, with a very...</td>\n",
              "      <td>menu</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3688</th>\n",
              "      <td>Each table has a pot of boiling water sunken i...</td>\n",
              "      <td>pot of boiling water</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3689</th>\n",
              "      <td>Each table has a pot of boiling water sunken i...</td>\n",
              "      <td>meats</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3690</th>\n",
              "      <td>Each table has a pot of boiling water sunken i...</td>\n",
              "      <td>vegetables</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3691</th>\n",
              "      <td>Each table has a pot of boiling water sunken i...</td>\n",
              "      <td>rice</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3692</th>\n",
              "      <td>Each table has a pot of boiling water sunken i...</td>\n",
              "      <td>glass noodles</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3693 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Sentence           Aspect Term  \\\n",
              "0                  But the staff was so horrible to us.                 staff   \n",
              "1     To be completely fair, the only redeeming fact...                  food   \n",
              "2     The food is uniformly exceptional, with a very...                  food   \n",
              "3     The food is uniformly exceptional, with a very...               kitchen   \n",
              "4     The food is uniformly exceptional, with a very...                  menu   \n",
              "...                                                 ...                   ...   \n",
              "3688  Each table has a pot of boiling water sunken i...  pot of boiling water   \n",
              "3689  Each table has a pot of boiling water sunken i...                 meats   \n",
              "3690  Each table has a pot of boiling water sunken i...            vegetables   \n",
              "3691  Each table has a pot of boiling water sunken i...                  rice   \n",
              "3692  Each table has a pot of boiling water sunken i...         glass noodles   \n",
              "\n",
              "      polarity  \n",
              "0            2  \n",
              "1            1  \n",
              "2            1  \n",
              "3            1  \n",
              "4            0  \n",
              "...        ...  \n",
              "3688         0  \n",
              "3689         0  \n",
              "3690         0  \n",
              "3691         0  \n",
              "3692         0  \n",
              "\n",
              "[3693 rows x 3 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def map_to_int(x):\n",
        "    if x=='positive':\n",
        "        return 1\n",
        "    elif x=='negative':\n",
        "        return 2\n",
        "    else:\n",
        "        return 0\n",
        "df['polarity'] = df['polarity'].apply(map_to_int)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIAEi4dYunkA"
      },
      "source": [
        "**In-domain vectors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2GsyGeoY2Rv",
        "outputId": "59e5d3fa-fe3b-47ac-f4e4-11411e67076c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/ubuntu/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/ubuntu/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: gensim in /home/ubuntu/anaconda3/lib/python3.8/site-packages (3.5.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from gensim) (6.3.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from gensim) (1.24.2)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from gensim) (1.15.0)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/ubuntu/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/ubuntu/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/ubuntu/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/ubuntu/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/ubuntu/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/ubuntu/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy in /home/ubuntu/anaconda3/lib/python3.8/site-packages (1.24.2)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/ubuntu/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/ubuntu/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/ubuntu/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/ubuntu/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install gensim\n",
        "!pip install --upgrade numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8zmAB8Guv4F"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the restaurant-related text dataset\n",
        "restaurant_data = df\n",
        "# Step 2: Preprocess the data\n",
        "processed_data = restaurant_data['Sentence'].apply(lambda x: gensim.utils.simple_preprocess(x))\n",
        "\n",
        "# Step 3: Train the FastText model\n",
        "model1 = gensim.models.FastText(processed_data, size=100, window=5, min_count=1, workers=4, sg=1)\n",
        "\n",
        "def indomain_vector(text):\n",
        "    # Step 4: Generate the embeddings\n",
        "    embeddings = []\n",
        "    for word in splitSentence(text):\n",
        "        try:\n",
        "            embeddings.append(model1.wv.get_vector(word))\n",
        "        except KeyError:\n",
        "            embeddings.append(np.zeros(100))\n",
        "\n",
        "    # Example: Print the embeddings for each word\n",
        "    return torch.tensor(embeddings)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZE8cvICnIFOx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OchX390hBf1"
      },
      "source": [
        "*BERT-TOKENS*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219,
          "referenced_widgets": [
            "7ec3c94a0f6c4dafa8442ef23e9522da",
            "0afcd6aedf2245f4aa4a41f00f7992a6",
            "11183adc0a6b4662983dfb5fe0d86f9d",
            "bb28c4e3770a4c06bdbc2f5a10a79bbc",
            "55749590bc144702ab5eb1de41c32c1d",
            "139e87f0f91647b9a31c2800b59899de",
            "07afa3bae8eb4d499ba44057bf4d1729",
            "f4df0aa828964dcbac960fca72bd88c7",
            "28a630d8e5a145d9abbf1ca944809868",
            "49d74b5216f54cb78e7a0802dc11e25e",
            "cdfe7f3f08a9423eadccefd54736ff6b",
            "d89c852473614128befc36221213b715",
            "83b56a0d7d274f9ca7feb29cde8c19d2",
            "e95f67a7379f44c888d993931b685eed",
            "e4024ea7de2347169bb3600a15f2a64a",
            "7ef9684cb3f343048e971f151beea119",
            "91d2d06fdd624999aa54b226246f8b55",
            "4b612366b70b4d26925a89674395f168",
            "6ef75cefa44e45c9a6d3fd89b538ec2f",
            "1abf1d792e11463188f323cd0468656c",
            "6ff01c1322cd4e2ca267be063ca3ce19",
            "0cc7c03a57ba4569b8af7a4939206bc0",
            "c4e23b59cb6f4b079b83300d20e28fd3",
            "b4d567d4cd9d413bb19ea4868a66350d",
            "495b218b6b6a4c59bb2e328ed83c2583",
            "0dd48d60237d486c8c0a274e69309e89",
            "335deb6c40f64a63a0347a74b00a8449",
            "de217e7d59c243b1a902585819ffdc89",
            "2c29233516d340288f928763a87667a9",
            "7379fa65626146fab080586dd3e45995",
            "5d4386ef0bfd4894bc243879d5c14d79",
            "d43202fd4d34479db3e1a6f573cbb210",
            "2de829e4e6b7462abf294ee5a6b86cf0",
            "3c732965b90e446fba84ad20c5b007cb",
            "421e09afa9b94cc88eb698dfeedb5457",
            "6e719729b0ef4bf0b8d422ed5855c677",
            "24fb0386d894454aa5c75299875fc4f0",
            "5bb2ba2488674762b152c3621f0adcd8",
            "2aa065965e8047fbb6a39dc0d80f30d1",
            "981831e97d174d45a3b5550a71569fa8",
            "6ea01a1d38a54aa2b384fbf1cb4462ec",
            "a0cba292c6e44af68dde328d9b90a348",
            "b88ed6bc4c63441a891d4f7fecae8ff6",
            "a6df883a88574552b0daa35e53a0dc55"
          ]
        },
        "id": "Po6teKhivmE1",
        "outputId": "bd15ca22-35be-4e1d-ae7c-67a72542e0a3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "model2 = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') \n",
        "def bert_to_token(sentence):\n",
        "   \n",
        "\n",
        "  # Define the sentence to be tokenized\n",
        "\n",
        "  # Tokenize the sentence\n",
        "  tokens = splitSentence(sentence)\n",
        "  # print(len(tokens))\n",
        "  # Convert tokens to ids\n",
        "  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "  # print(len(input_ids))\n",
        "  # Add special tokens\n",
        "  # Convert input_ids to tensor\n",
        "  input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
        "  # print(len(input_ids))\n",
        "  # Get the 768 dimensional vectors for each token\n",
        "  outputs = model2(input_ids)\n",
        "  # print(outputs,len(outputs))\n",
        "  token_vectors = outputs.last_hidden_state.squeeze(0)\n",
        "  return token_vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqKmN9WcETMx"
      },
      "source": [
        "*BERT+In-domain*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SHw3g3hESf2"
      },
      "outputs": [],
      "source": [
        "# sentences = []\n",
        "# for i in range(2800):\n",
        "#   sentence = df['Sentence'][i]\n",
        "#   ten1 = np.array(indomain_vector(sentence))\n",
        "#   ten2 = bert_to_token(sentence).detach().numpy()\n",
        "#   vec = np.concatenate((ten1, ten2), axis=1)\n",
        "#   sentences.append(vec)\n",
        "def create_embeddings(sentences):\n",
        "    embeddings = []\n",
        "    for sentence in sentences:\n",
        "        ten1 = indomain_vector(sentence)\n",
        "        ten2 = bert_to_token(sentence).detach()\n",
        "        vec = torch.cat([ten1, ten2], dim=1).numpy()\n",
        "        embeddings.append(vec)\n",
        "\n",
        "    embeddings_array = np.array(embeddings)\n",
        "    return torch.from_numpy(embeddings_array)\n",
        "  \n",
        "sentences = []\n",
        "for i in range(2800):\n",
        "    sentence = df['Sentence'][i]\n",
        "    embeddings = create_embeddings([sentence])\n",
        "    sentences.append(np.array(embeddings[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c23HowkLXWt"
      },
      "outputs": [],
      "source": [
        "# sentence = df['Sentence'][0]\n",
        "# ten1 = np.array(indomain_vector(sentence))\n",
        "# ten2 = bert_to_token(sentence).detach().numpy()\n",
        "# vec = np.concatenate((ten1, ten2), axis=1)\n",
        "# print(vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKxlN_yFhHYU"
      },
      "source": [
        "*padding sentences for making input length as same*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVMPu0WYtr8E"
      },
      "outputs": [],
      "source": [
        "# Calculate the maximum length of sentences\n",
        "max_length = max(len(s) for s in sentences)\n",
        "\n",
        "# Pad each sentence to the maximum length\n",
        "padded_sentences = np.zeros((len(sentences), max_length, 868))\n",
        "for i, s in enumerate(sentences):\n",
        "    padded_sentences[i, :len(s), :] = s\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2-aahAANNsr"
      },
      "outputs": [],
      "source": [
        "label = np.array(df['polarity'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C__Ivjx6UsXU"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQnIW2TpMhiO",
        "outputId": "09170815-32fc-4b2e-999b-f1e50b35f71d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Loss: 0.9891\n",
            "Epoch [2/100], Loss: 1.0024\n",
            "Epoch [3/100], Loss: 1.0228\n",
            "Epoch [4/100], Loss: 1.0366\n",
            "Epoch [5/100], Loss: 1.0424\n",
            "Epoch [6/100], Loss: 1.0451\n",
            "Epoch [7/100], Loss: 1.0469\n",
            "Epoch [8/100], Loss: 1.0486\n",
            "Epoch [9/100], Loss: 1.0500\n",
            "Epoch [10/100], Loss: 1.0512\n",
            "Epoch [11/100], Loss: 1.0522\n",
            "Epoch [12/100], Loss: 1.0530\n",
            "Epoch [13/100], Loss: 1.0534\n",
            "Epoch [14/100], Loss: 1.0538\n",
            "Epoch [15/100], Loss: 1.0537\n",
            "Epoch [16/100], Loss: 1.0539\n",
            "Epoch [17/100], Loss: 1.0538\n",
            "Epoch [18/100], Loss: 1.0536\n",
            "Epoch [19/100], Loss: 1.0529\n",
            "Epoch [20/100], Loss: 1.0529\n",
            "Epoch [21/100], Loss: 1.0529\n",
            "Epoch [22/100], Loss: 1.0529\n",
            "Epoch [23/100], Loss: 1.0526\n",
            "Epoch [24/100], Loss: 1.0530\n",
            "Epoch [25/100], Loss: 1.0527\n",
            "Epoch [26/100], Loss: 1.0531\n",
            "Epoch [27/100], Loss: 1.0532\n",
            "Epoch [28/100], Loss: 1.0535\n",
            "Epoch [29/100], Loss: 1.0533\n",
            "Epoch [30/100], Loss: 1.0537\n",
            "Epoch [31/100], Loss: 1.0538\n",
            "Epoch [32/100], Loss: 1.0539\n",
            "Epoch [33/100], Loss: 1.0531\n",
            "Epoch [34/100], Loss: 1.0534\n",
            "Epoch [35/100], Loss: 1.0540\n",
            "Epoch [36/100], Loss: 1.0541\n",
            "Epoch [37/100], Loss: 1.0541\n",
            "Epoch [38/100], Loss: 1.0538\n",
            "Epoch [39/100], Loss: 1.0542\n",
            "Epoch [40/100], Loss: 1.0533\n",
            "Epoch [41/100], Loss: 1.0535\n",
            "Epoch [42/100], Loss: 1.0543\n",
            "Epoch [43/100], Loss: 1.0539\n",
            "Epoch [44/100], Loss: 1.0541\n",
            "Epoch [45/100], Loss: 1.0539\n",
            "Epoch [46/100], Loss: 1.0541\n",
            "Epoch [47/100], Loss: 1.0532\n",
            "Epoch [48/100], Loss: 1.0533\n",
            "Epoch [49/100], Loss: 1.0539\n",
            "Epoch [50/100], Loss: 1.0538\n",
            "Epoch [51/100], Loss: 1.0539\n",
            "Epoch [52/100], Loss: 1.0532\n",
            "Epoch [53/100], Loss: 1.0531\n",
            "Epoch [54/100], Loss: 1.0533\n",
            "Epoch [55/100], Loss: 1.0529\n",
            "Epoch [56/100], Loss: 1.0528\n",
            "Epoch [57/100], Loss: 1.0521\n",
            "Epoch [58/100], Loss: 1.0519\n",
            "Epoch [59/100], Loss: 1.0526\n",
            "Epoch [60/100], Loss: 1.0527\n",
            "Epoch [61/100], Loss: 1.0519\n",
            "Epoch [62/100], Loss: 1.0528\n",
            "Epoch [63/100], Loss: 1.0525\n",
            "Epoch [64/100], Loss: 1.0521\n",
            "Epoch [65/100], Loss: 1.0529\n",
            "Epoch [66/100], Loss: 1.0524\n",
            "Epoch [67/100], Loss: 1.0525\n",
            "Epoch [68/100], Loss: 1.0520\n",
            "Epoch [69/100], Loss: 1.0527\n",
            "Epoch [70/100], Loss: 1.0527\n",
            "Epoch [71/100], Loss: 1.0528\n",
            "Epoch [72/100], Loss: 1.0525\n",
            "Epoch [73/100], Loss: 1.0521\n",
            "Epoch [74/100], Loss: 1.0521\n",
            "Epoch [75/100], Loss: 1.0526\n",
            "Epoch [76/100], Loss: 1.0516\n",
            "Epoch [77/100], Loss: 1.0520\n",
            "Epoch [78/100], Loss: 1.0521\n",
            "Epoch [79/100], Loss: 1.0520\n",
            "Epoch [80/100], Loss: 1.0522\n",
            "Epoch [81/100], Loss: 1.0520\n",
            "Epoch [82/100], Loss: 1.0518\n",
            "Epoch [83/100], Loss: 1.0513\n",
            "Epoch [84/100], Loss: 1.0513\n",
            "Epoch [85/100], Loss: 1.0517\n",
            "Epoch [86/100], Loss: 1.0509\n",
            "Epoch [87/100], Loss: 1.0517\n",
            "Epoch [88/100], Loss: 1.0520\n",
            "Epoch [89/100], Loss: 1.0508\n",
            "Epoch [90/100], Loss: 1.0507\n",
            "Epoch [91/100], Loss: 1.0516\n",
            "Epoch [92/100], Loss: 1.0511\n",
            "Epoch [93/100], Loss: 1.0508\n",
            "Epoch [94/100], Loss: 1.0511\n",
            "Epoch [95/100], Loss: 1.0505\n",
            "Epoch [96/100], Loss: 1.0505\n",
            "Epoch [97/100], Loss: 1.0487\n",
            "Epoch [98/100], Loss: 1.0475\n",
            "Epoch [99/100], Loss: 1.0499\n",
            "Epoch [100/100], Loss: 1.0486\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Define the LSTM model\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        last_hidden_state = lstm_out[:, -1, :]\n",
        "        out = self.fc(last_hidden_state)\n",
        "        return out\n",
        "\n",
        "# Define the input sentences and labels\n",
        "sentences1 = padded_sentences\n",
        "labels = label[:2800]\n",
        "\n",
        "# Convert the labels to one-hot encoding\n",
        "one_hot_labels = np.zeros((len(labels), 3))\n",
        "for i in range(len(labels)):\n",
        "    one_hot_labels[i, labels[i]] = 1\n",
        "\n",
        "# Convert the input sentences to PyTorch tensors\n",
        "sentences1 = torch.tensor(sentences1, dtype=torch.float32)\n",
        "one_hot_labels = torch.tensor(one_hot_labels, dtype=torch.float32)\n",
        "\n",
        "# Define the hyperparameters\n",
        "input_size = 868\n",
        "hidden_size = 64\n",
        "num_classes = 3\n",
        "learning_rate = 0.001\n",
        "num_epochs = 100\n",
        "batch_size = 32\n",
        "\n",
        "# Initialize the model and optimizer\n",
        "model = LSTMClassifier(input_size, hidden_size, num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(0, len(sentences1), batch_size):\n",
        "        batch_sentences = sentences1[i:i+batch_size]\n",
        "        batch_labels = torch.argmax(one_hot_labels[i:i+batch_size], dim=1)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_sentences)\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "\n",
        "torch.save(model.state_dict(), 'lstm.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n7rGdJVOc1z"
      },
      "source": [
        "*Prediction*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAdxhKCAOREY"
      },
      "outputs": [],
      "source": [
        "def predicted(sen):\n",
        "  # sen = df['Sentence'][1]\n",
        "    t1 = np.array(indomain_vector(sen))\n",
        "    t2 = bert_to_token(sen).detach().numpy()\n",
        "    v = np.concatenate((t1, t2), axis=1)\n",
        "    test_sentence = v\n",
        "    padded_sentence = np.zeros((1, max_length, 868))\n",
        "    padded_sentence[0, :len(test_sentence), :] = test_sentence\n",
        "\n",
        "    # Convert the padded sentence to a PyTorch tensor\n",
        "    padded_sentence = torch.tensor(padded_sentence, dtype=torch.float32)\n",
        "\n",
        "    # Pass the padded sentence through the trained model to get the predicted class\n",
        "    with torch.no_grad():\n",
        "        output = model(padded_sentence)\n",
        "        predicted_class = torch.argmax(output).item()\n",
        "\n",
        "    # Print the predicted class\n",
        "    return predicted_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8TAxp1oSeSX"
      },
      "outputs": [],
      "source": [
        "s = df['Sentence']\n",
        "predict = []\n",
        "count = 0\n",
        "for i in range(2800,3600):\n",
        "  p =predicted(s[i])\n",
        "  if p!=df['polarity'][i]:\n",
        "    count = count + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePQoJHvbfDVg",
        "outputId": "48ebddbe-194e-4f8a-f7c7-1350f6cacdc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy= 62.625 %\n"
          ]
        }
      ],
      "source": [
        "print(\"accuracy=\",(800-count)*100/800,\"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktRbgxCCHHqF"
      },
      "source": [
        "# TD-LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enNcjBH040Wy"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0XWh1kpSjTC"
      },
      "outputs": [],
      "source": [
        "#finding start and end index of aspect_term in sentence. example: sentence=['the','fried','rice,'is','good'] , aspect_term=['fried','rice']. Then function returns [1,2] . \n",
        "def ind(sentence,aspect_term):\n",
        "    sentence = splitSentence(sentence)\n",
        "    aspect_term = splitSentence(aspect_term)\n",
        "    try:\n",
        "        start_index = sentence.index(aspect_term[0])\n",
        "        end_index = start_index + len(aspect_term) - 1\n",
        "    except ValueError:\n",
        "        start_index = len(sentence) - 1\n",
        "        end_index = start_index -1\n",
        "    return [start_index,end_index]\n",
        "\n",
        "#this below code gives start and end index of aspect_term in sentence for all sentences.\n",
        "indices = []\n",
        "s = df['Sentence']\n",
        "a = df['Aspect Term']\n",
        "for i in range(len(s)):\n",
        "    indices.append(ind(s[i],a[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hh9qSC7G95su"
      },
      "outputs": [],
      "source": [
        "#this code will return 2d-array consists of tokens for left of aspect terms and tokens of 868d vectors for right of aspect terms in sentence.\n",
        "def pair(i):\n",
        "    s1 = np.array(sentences[i])\n",
        "    answer = []\n",
        "    i1 = indices[i][0]\n",
        "    j1 = indices[i][1]\n",
        "    answer.append(s1[:i1+1])\n",
        "    answer.append(s1[j1-1:])\n",
        "    return answer\n",
        "# for all sentences splitting left aspect terms and right aspect terms\n",
        "input_pairs = []\n",
        "for i in range(len(sentences)):\n",
        "    input_pairs.append(pair(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEKW1Y1y80Um"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZ8cvWBWUc_l",
        "outputId": "f8d79681-6076-4a7f-d491-f64472a01302"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Step [100/3600], Loss: 0.2925\n",
            "Epoch [1/100], Step [200/3600], Loss: 0.2353\n",
            "Epoch [1/100], Step [300/3600], Loss: 0.7469\n",
            "Epoch [1/100], Step [400/3600], Loss: 1.5018\n",
            "Epoch [1/100], Step [500/3600], Loss: 1.1529\n",
            "Epoch [1/100], Step [600/3600], Loss: 0.1134\n",
            "Epoch [1/100], Step [700/3600], Loss: 0.3605\n",
            "Epoch [1/100], Step [800/3600], Loss: 0.3439\n",
            "Epoch [1/100], Step [900/3600], Loss: 0.3373\n",
            "Epoch [1/100], Step [1000/3600], Loss: 0.8123\n",
            "Epoch [1/100], Step [1100/3600], Loss: 0.3549\n",
            "Epoch [1/100], Step [1200/3600], Loss: 0.5038\n",
            "Epoch [1/100], Step [1300/3600], Loss: 0.7802\n",
            "Epoch [1/100], Step [1400/3600], Loss: 0.3669\n",
            "Epoch [1/100], Step [1500/3600], Loss: 1.4596\n",
            "Epoch [1/100], Step [1600/3600], Loss: 1.6686\n",
            "Epoch [1/100], Step [1700/3600], Loss: 1.0733\n",
            "Epoch [1/100], Step [1800/3600], Loss: 0.3146\n",
            "Epoch [1/100], Step [1900/3600], Loss: 0.3141\n",
            "Epoch [1/100], Step [2000/3600], Loss: 0.1974\n",
            "Epoch [1/100], Step [2100/3600], Loss: 1.2755\n",
            "Epoch [1/100], Step [2200/3600], Loss: 0.9532\n",
            "Epoch [1/100], Step [2300/3600], Loss: 1.0630\n",
            "Epoch [1/100], Step [2400/3600], Loss: 0.1589\n",
            "Epoch [1/100], Step [2500/3600], Loss: 0.8664\n",
            "Epoch [1/100], Step [2600/3600], Loss: 1.4934\n",
            "Epoch [1/100], Step [2700/3600], Loss: 0.5346\n",
            "Epoch [1/100], Step [2800/3600], Loss: 1.2625\n",
            "Epoch [2/100], Step [100/3600], Loss: 0.1475\n",
            "Epoch [2/100], Step [200/3600], Loss: 0.2316\n",
            "Epoch [2/100], Step [300/3600], Loss: 0.7591\n",
            "Epoch [2/100], Step [400/3600], Loss: 1.1127\n",
            "Epoch [2/100], Step [500/3600], Loss: 0.8938\n",
            "Epoch [2/100], Step [600/3600], Loss: 0.0473\n",
            "Epoch [2/100], Step [700/3600], Loss: 0.1697\n",
            "Epoch [2/100], Step [800/3600], Loss: 0.2344\n",
            "Epoch [2/100], Step [900/3600], Loss: 0.7968\n",
            "Epoch [2/100], Step [1000/3600], Loss: 0.5302\n",
            "Epoch [2/100], Step [1100/3600], Loss: 0.2350\n",
            "Epoch [2/100], Step [1200/3600], Loss: 0.2002\n",
            "Epoch [2/100], Step [1300/3600], Loss: 0.4009\n",
            "Epoch [2/100], Step [1400/3600], Loss: 0.3433\n",
            "Epoch [2/100], Step [1500/3600], Loss: 1.3758\n",
            "Epoch [2/100], Step [1600/3600], Loss: 1.9460\n",
            "Epoch [2/100], Step [1700/3600], Loss: 0.7411\n",
            "Epoch [2/100], Step [1800/3600], Loss: 0.1107\n",
            "Epoch [2/100], Step [1900/3600], Loss: 0.1909\n",
            "Epoch [2/100], Step [2000/3600], Loss: 0.2281\n",
            "Epoch [2/100], Step [2100/3600], Loss: 0.6982\n",
            "Epoch [2/100], Step [2200/3600], Loss: 1.2382\n",
            "Epoch [2/100], Step [2300/3600], Loss: 0.6952\n",
            "Epoch [2/100], Step [2400/3600], Loss: 0.0445\n",
            "Epoch [2/100], Step [2500/3600], Loss: 0.8121\n",
            "Epoch [2/100], Step [2600/3600], Loss: 0.9403\n",
            "Epoch [2/100], Step [2700/3600], Loss: 0.3767\n",
            "Epoch [2/100], Step [2800/3600], Loss: 1.5082\n",
            "Epoch [3/100], Step [100/3600], Loss: 0.0490\n",
            "Epoch [3/100], Step [200/3600], Loss: 0.1145\n",
            "Epoch [3/100], Step [300/3600], Loss: 0.6519\n",
            "Epoch [3/100], Step [400/3600], Loss: 0.9478\n",
            "Epoch [3/100], Step [500/3600], Loss: 0.6289\n",
            "Epoch [3/100], Step [600/3600], Loss: 0.0420\n",
            "Epoch [3/100], Step [700/3600], Loss: 0.0624\n",
            "Epoch [3/100], Step [800/3600], Loss: 0.1509\n",
            "Epoch [3/100], Step [900/3600], Loss: 0.4900\n",
            "Epoch [3/100], Step [1000/3600], Loss: 0.3630\n",
            "Epoch [3/100], Step [1100/3600], Loss: 0.1097\n",
            "Epoch [3/100], Step [1200/3600], Loss: 0.2108\n",
            "Epoch [3/100], Step [1300/3600], Loss: 0.1685\n",
            "Epoch [3/100], Step [1400/3600], Loss: 0.2125\n",
            "Epoch [3/100], Step [1500/3600], Loss: 1.3737\n",
            "Epoch [3/100], Step [1600/3600], Loss: 1.0065\n",
            "Epoch [3/100], Step [1700/3600], Loss: 0.4140\n",
            "Epoch [3/100], Step [1800/3600], Loss: 0.0860\n",
            "Epoch [3/100], Step [1900/3600], Loss: 0.1919\n",
            "Epoch [3/100], Step [2000/3600], Loss: 0.0893\n",
            "Epoch [3/100], Step [2100/3600], Loss: 0.2602\n",
            "Epoch [3/100], Step [2200/3600], Loss: 1.3348\n",
            "Epoch [3/100], Step [2300/3600], Loss: 0.5527\n",
            "Epoch [3/100], Step [2400/3600], Loss: 0.0166\n",
            "Epoch [3/100], Step [2500/3600], Loss: 0.6010\n",
            "Epoch [3/100], Step [2600/3600], Loss: 0.6664\n",
            "Epoch [3/100], Step [2700/3600], Loss: 0.4064\n",
            "Epoch [3/100], Step [2800/3600], Loss: 0.9871\n",
            "Epoch [4/100], Step [100/3600], Loss: 0.0236\n",
            "Epoch [4/100], Step [200/3600], Loss: 0.0816\n",
            "Epoch [4/100], Step [300/3600], Loss: 0.5554\n",
            "Epoch [4/100], Step [400/3600], Loss: 0.6780\n",
            "Epoch [4/100], Step [500/3600], Loss: 0.5268\n",
            "Epoch [4/100], Step [600/3600], Loss: 0.0168\n",
            "Epoch [4/100], Step [700/3600], Loss: 0.0906\n",
            "Epoch [4/100], Step [800/3600], Loss: 0.0896\n",
            "Epoch [4/100], Step [900/3600], Loss: 0.2477\n",
            "Epoch [4/100], Step [1000/3600], Loss: 0.3221\n",
            "Epoch [4/100], Step [1100/3600], Loss: 0.0356\n",
            "Epoch [4/100], Step [1200/3600], Loss: 0.4408\n",
            "Epoch [4/100], Step [1300/3600], Loss: 0.1439\n",
            "Epoch [4/100], Step [1400/3600], Loss: 0.2099\n",
            "Epoch [4/100], Step [1500/3600], Loss: 0.9328\n",
            "Epoch [4/100], Step [1600/3600], Loss: 0.6827\n",
            "Epoch [4/100], Step [1700/3600], Loss: 0.1680\n",
            "Epoch [4/100], Step [1800/3600], Loss: 0.1254\n",
            "Epoch [4/100], Step [1900/3600], Loss: 0.1377\n",
            "Epoch [4/100], Step [2000/3600], Loss: 0.1110\n",
            "Epoch [4/100], Step [2100/3600], Loss: 0.2624\n",
            "Epoch [4/100], Step [2200/3600], Loss: 0.9126\n",
            "Epoch [4/100], Step [2300/3600], Loss: 0.3608\n",
            "Epoch [4/100], Step [2400/3600], Loss: 0.0474\n",
            "Epoch [4/100], Step [2500/3600], Loss: 0.5058\n",
            "Epoch [4/100], Step [2600/3600], Loss: 0.5546\n",
            "Epoch [4/100], Step [2700/3600], Loss: 0.2827\n",
            "Epoch [4/100], Step [2800/3600], Loss: 0.6814\n",
            "Epoch [5/100], Step [100/3600], Loss: 0.0249\n",
            "Epoch [5/100], Step [200/3600], Loss: 0.1045\n",
            "Epoch [5/100], Step [300/3600], Loss: 0.3550\n",
            "Epoch [5/100], Step [400/3600], Loss: 0.6454\n",
            "Epoch [5/100], Step [500/3600], Loss: 0.4163\n",
            "Epoch [5/100], Step [600/3600], Loss: 0.0033\n",
            "Epoch [5/100], Step [700/3600], Loss: 0.0683\n",
            "Epoch [5/100], Step [800/3600], Loss: 0.0597\n",
            "Epoch [5/100], Step [900/3600], Loss: 0.0612\n",
            "Epoch [5/100], Step [1000/3600], Loss: 0.1951\n",
            "Epoch [5/100], Step [1100/3600], Loss: 0.0368\n",
            "Epoch [5/100], Step [1200/3600], Loss: 0.3921\n",
            "Epoch [5/100], Step [1300/3600], Loss: 0.0533\n",
            "Epoch [5/100], Step [1400/3600], Loss: 0.1737\n",
            "Epoch [5/100], Step [1500/3600], Loss: 1.1842\n",
            "Epoch [5/100], Step [1600/3600], Loss: 0.2984\n",
            "Epoch [5/100], Step [1700/3600], Loss: 0.1359\n",
            "Epoch [5/100], Step [1800/3600], Loss: 0.0830\n",
            "Epoch [5/100], Step [1900/3600], Loss: 0.0715\n",
            "Epoch [5/100], Step [2000/3600], Loss: 0.0557\n",
            "Epoch [5/100], Step [2100/3600], Loss: 0.0556\n",
            "Epoch [5/100], Step [2200/3600], Loss: 1.3169\n",
            "Epoch [5/100], Step [2300/3600], Loss: 0.1124\n",
            "Epoch [5/100], Step [2400/3600], Loss: 0.0278\n",
            "Epoch [5/100], Step [2500/3600], Loss: 0.6050\n",
            "Epoch [5/100], Step [2600/3600], Loss: 0.3567\n",
            "Epoch [5/100], Step [2700/3600], Loss: 0.3845\n",
            "Epoch [5/100], Step [2800/3600], Loss: 0.6255\n",
            "Epoch [6/100], Step [100/3600], Loss: 0.0309\n",
            "Epoch [6/100], Step [200/3600], Loss: 0.1124\n",
            "Epoch [6/100], Step [300/3600], Loss: 0.3437\n",
            "Epoch [6/100], Step [400/3600], Loss: 0.5627\n",
            "Epoch [6/100], Step [500/3600], Loss: 0.5942\n",
            "Epoch [6/100], Step [600/3600], Loss: 0.0027\n",
            "Epoch [6/100], Step [700/3600], Loss: 0.0231\n",
            "Epoch [6/100], Step [800/3600], Loss: 0.0227\n",
            "Epoch [6/100], Step [900/3600], Loss: 0.0685\n",
            "Epoch [6/100], Step [1000/3600], Loss: 0.1357\n",
            "Epoch [6/100], Step [1100/3600], Loss: 0.0292\n",
            "Epoch [6/100], Step [1200/3600], Loss: 0.2752\n",
            "Epoch [6/100], Step [1300/3600], Loss: 0.0106\n",
            "Epoch [6/100], Step [1400/3600], Loss: 0.1770\n",
            "Epoch [6/100], Step [1500/3600], Loss: 0.4808\n",
            "Epoch [6/100], Step [1600/3600], Loss: 0.4551\n",
            "Epoch [6/100], Step [1700/3600], Loss: 0.0619\n",
            "Epoch [6/100], Step [1800/3600], Loss: 0.1575\n",
            "Epoch [6/100], Step [1900/3600], Loss: 0.1643\n",
            "Epoch [6/100], Step [2000/3600], Loss: 0.0284\n",
            "Epoch [6/100], Step [2100/3600], Loss: 0.0411\n",
            "Epoch [6/100], Step [2200/3600], Loss: 0.9428\n",
            "Epoch [6/100], Step [2300/3600], Loss: 0.0200\n",
            "Epoch [6/100], Step [2400/3600], Loss: 0.0256\n",
            "Epoch [6/100], Step [2500/3600], Loss: 0.2311\n",
            "Epoch [6/100], Step [2600/3600], Loss: 0.2465\n",
            "Epoch [6/100], Step [2700/3600], Loss: 0.3327\n",
            "Epoch [6/100], Step [2800/3600], Loss: 0.2932\n",
            "Epoch [7/100], Step [100/3600], Loss: 0.0042\n",
            "Epoch [7/100], Step [200/3600], Loss: 0.0535\n",
            "Epoch [7/100], Step [300/3600], Loss: 0.3015\n",
            "Epoch [7/100], Step [400/3600], Loss: 0.4093\n",
            "Epoch [7/100], Step [500/3600], Loss: 0.6056\n",
            "Epoch [7/100], Step [600/3600], Loss: 0.0012\n",
            "Epoch [7/100], Step [700/3600], Loss: 0.0248\n",
            "Epoch [7/100], Step [800/3600], Loss: 0.0444\n",
            "Epoch [7/100], Step [900/3600], Loss: 0.0263\n",
            "Epoch [7/100], Step [1000/3600], Loss: 0.1205\n",
            "Epoch [7/100], Step [1100/3600], Loss: 0.0251\n",
            "Epoch [7/100], Step [1200/3600], Loss: 0.5439\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [7/100], Step [1300/3600], Loss: 0.0244\n",
            "Epoch [7/100], Step [1400/3600], Loss: 0.1594\n",
            "Epoch [7/100], Step [1500/3600], Loss: 0.2198\n",
            "Epoch [7/100], Step [1600/3600], Loss: 0.1145\n",
            "Epoch [7/100], Step [1700/3600], Loss: 0.0363\n",
            "Epoch [7/100], Step [1800/3600], Loss: 0.0407\n",
            "Epoch [7/100], Step [1900/3600], Loss: 0.1389\n",
            "Epoch [7/100], Step [2000/3600], Loss: 0.0156\n",
            "Epoch [7/100], Step [2100/3600], Loss: 0.0122\n",
            "Epoch [7/100], Step [2200/3600], Loss: 0.3685\n",
            "Epoch [7/100], Step [2300/3600], Loss: 0.0400\n",
            "Epoch [7/100], Step [2400/3600], Loss: 0.0364\n",
            "Epoch [7/100], Step [2500/3600], Loss: 0.1301\n",
            "Epoch [7/100], Step [2600/3600], Loss: 0.3093\n",
            "Epoch [7/100], Step [2700/3600], Loss: 0.2365\n",
            "Epoch [7/100], Step [2800/3600], Loss: 0.4716\n",
            "Epoch [8/100], Step [100/3600], Loss: 0.0016\n",
            "Epoch [8/100], Step [200/3600], Loss: 0.1126\n",
            "Epoch [8/100], Step [300/3600], Loss: 0.1608\n",
            "Epoch [8/100], Step [400/3600], Loss: 0.1506\n",
            "Epoch [8/100], Step [500/3600], Loss: 0.1227\n",
            "Epoch [8/100], Step [600/3600], Loss: 0.0031\n",
            "Epoch [8/100], Step [700/3600], Loss: 0.0193\n",
            "Epoch [8/100], Step [800/3600], Loss: 0.0387\n",
            "Epoch [8/100], Step [900/3600], Loss: 0.0064\n",
            "Epoch [8/100], Step [1000/3600], Loss: 0.0802\n",
            "Epoch [8/100], Step [1100/3600], Loss: 0.0213\n",
            "Epoch [8/100], Step [1200/3600], Loss: 0.3895\n",
            "Epoch [8/100], Step [1300/3600], Loss: 0.0069\n",
            "Epoch [8/100], Step [1400/3600], Loss: 0.2833\n",
            "Epoch [8/100], Step [1500/3600], Loss: 0.0503\n",
            "Epoch [8/100], Step [1600/3600], Loss: 0.0440\n",
            "Epoch [8/100], Step [1700/3600], Loss: 0.0403\n",
            "Epoch [8/100], Step [1800/3600], Loss: 0.0708\n",
            "Epoch [8/100], Step [1900/3600], Loss: 0.0291\n",
            "Epoch [8/100], Step [2000/3600], Loss: 0.0015\n",
            "Epoch [8/100], Step [2100/3600], Loss: 0.0377\n",
            "Epoch [8/100], Step [2200/3600], Loss: 0.0895\n",
            "Epoch [8/100], Step [2300/3600], Loss: 0.0697\n",
            "Epoch [8/100], Step [2400/3600], Loss: 0.0478\n",
            "Epoch [8/100], Step [2500/3600], Loss: 0.0178\n",
            "Epoch [8/100], Step [2600/3600], Loss: 0.1032\n",
            "Epoch [8/100], Step [2700/3600], Loss: 0.1706\n",
            "Epoch [8/100], Step [2800/3600], Loss: 0.2043\n",
            "Epoch [9/100], Step [100/3600], Loss: 0.0115\n",
            "Epoch [9/100], Step [200/3600], Loss: 0.0413\n",
            "Epoch [9/100], Step [300/3600], Loss: 0.2547\n",
            "Epoch [9/100], Step [400/3600], Loss: 0.0596\n",
            "Epoch [9/100], Step [500/3600], Loss: 0.2036\n",
            "Epoch [9/100], Step [600/3600], Loss: 0.0004\n",
            "Epoch [9/100], Step [700/3600], Loss: 0.0095\n",
            "Epoch [9/100], Step [800/3600], Loss: 0.0169\n",
            "Epoch [9/100], Step [900/3600], Loss: 0.0065\n",
            "Epoch [9/100], Step [1000/3600], Loss: 0.1168\n",
            "Epoch [9/100], Step [1100/3600], Loss: 0.0255\n",
            "Epoch [9/100], Step [1200/3600], Loss: 0.6301\n",
            "Epoch [9/100], Step [1300/3600], Loss: 0.0040\n",
            "Epoch [9/100], Step [1400/3600], Loss: 0.0271\n",
            "Epoch [9/100], Step [1500/3600], Loss: 0.0861\n",
            "Epoch [9/100], Step [1600/3600], Loss: 0.0098\n",
            "Epoch [9/100], Step [1700/3600], Loss: 0.0446\n",
            "Epoch [9/100], Step [1800/3600], Loss: 0.0378\n",
            "Epoch [9/100], Step [1900/3600], Loss: 0.0716\n",
            "Epoch [9/100], Step [2000/3600], Loss: 0.0017\n",
            "Epoch [9/100], Step [2100/3600], Loss: 0.0204\n",
            "Epoch [9/100], Step [2200/3600], Loss: 0.3425\n",
            "Epoch [9/100], Step [2300/3600], Loss: 0.0351\n",
            "Epoch [9/100], Step [2400/3600], Loss: 0.0430\n",
            "Epoch [9/100], Step [2500/3600], Loss: 0.0463\n",
            "Epoch [9/100], Step [2600/3600], Loss: 0.1097\n",
            "Epoch [9/100], Step [2700/3600], Loss: 0.1789\n",
            "Epoch [9/100], Step [2800/3600], Loss: 0.3226\n",
            "Epoch [10/100], Step [100/3600], Loss: 0.0027\n",
            "Epoch [10/100], Step [200/3600], Loss: 0.1031\n",
            "Epoch [10/100], Step [300/3600], Loss: 0.2036\n",
            "Epoch [10/100], Step [400/3600], Loss: 0.0871\n",
            "Epoch [10/100], Step [500/3600], Loss: 0.0203\n",
            "Epoch [10/100], Step [600/3600], Loss: 0.0001\n",
            "Epoch [10/100], Step [700/3600], Loss: 0.0771\n",
            "Epoch [10/100], Step [800/3600], Loss: 0.0023\n",
            "Epoch [10/100], Step [900/3600], Loss: 0.0013\n",
            "Epoch [10/100], Step [1000/3600], Loss: 0.0541\n",
            "Epoch [10/100], Step [1100/3600], Loss: 0.0479\n",
            "Epoch [10/100], Step [1200/3600], Loss: 0.0899\n",
            "Epoch [10/100], Step [1300/3600], Loss: 0.0020\n",
            "Epoch [10/100], Step [1400/3600], Loss: 0.0419\n",
            "Epoch [10/100], Step [1500/3600], Loss: 0.0085\n",
            "Epoch [10/100], Step [1600/3600], Loss: 0.0033\n",
            "Epoch [10/100], Step [1700/3600], Loss: 0.0940\n",
            "Epoch [10/100], Step [1800/3600], Loss: 0.0779\n",
            "Epoch [10/100], Step [1900/3600], Loss: 0.0337\n",
            "Epoch [10/100], Step [2000/3600], Loss: 0.0004\n",
            "Epoch [10/100], Step [2100/3600], Loss: 0.0122\n",
            "Epoch [10/100], Step [2200/3600], Loss: 0.4934\n",
            "Epoch [10/100], Step [2300/3600], Loss: 0.3307\n",
            "Epoch [10/100], Step [2400/3600], Loss: 0.0068\n",
            "Epoch [10/100], Step [2500/3600], Loss: 0.0335\n",
            "Epoch [10/100], Step [2600/3600], Loss: 0.0418\n",
            "Epoch [10/100], Step [2700/3600], Loss: 0.1287\n",
            "Epoch [10/100], Step [2800/3600], Loss: 0.2453\n",
            "Epoch [11/100], Step [100/3600], Loss: 0.0012\n",
            "Epoch [11/100], Step [200/3600], Loss: 0.0201\n",
            "Epoch [11/100], Step [300/3600], Loss: 0.1907\n",
            "Epoch [11/100], Step [400/3600], Loss: 0.1002\n",
            "Epoch [11/100], Step [500/3600], Loss: 0.0244\n",
            "Epoch [11/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [11/100], Step [700/3600], Loss: 0.0140\n",
            "Epoch [11/100], Step [800/3600], Loss: 0.0015\n",
            "Epoch [11/100], Step [900/3600], Loss: 0.0007\n",
            "Epoch [11/100], Step [1000/3600], Loss: 0.0614\n",
            "Epoch [11/100], Step [1100/3600], Loss: 0.0798\n",
            "Epoch [11/100], Step [1200/3600], Loss: 0.1688\n",
            "Epoch [11/100], Step [1300/3600], Loss: 0.0769\n",
            "Epoch [11/100], Step [1400/3600], Loss: 1.0646\n",
            "Epoch [11/100], Step [1500/3600], Loss: 0.0101\n",
            "Epoch [11/100], Step [1600/3600], Loss: 0.0011\n",
            "Epoch [11/100], Step [1700/3600], Loss: 0.0023\n",
            "Epoch [11/100], Step [1800/3600], Loss: 0.0609\n",
            "Epoch [11/100], Step [1900/3600], Loss: 0.0133\n",
            "Epoch [11/100], Step [2000/3600], Loss: 0.0001\n",
            "Epoch [11/100], Step [2100/3600], Loss: 0.0118\n",
            "Epoch [11/100], Step [2200/3600], Loss: 0.0651\n",
            "Epoch [11/100], Step [2300/3600], Loss: 0.0168\n",
            "Epoch [11/100], Step [2400/3600], Loss: 0.0041\n",
            "Epoch [11/100], Step [2500/3600], Loss: 0.0159\n",
            "Epoch [11/100], Step [2600/3600], Loss: 0.2258\n",
            "Epoch [11/100], Step [2700/3600], Loss: 0.0070\n",
            "Epoch [11/100], Step [2800/3600], Loss: 0.0702\n",
            "Epoch [12/100], Step [100/3600], Loss: 0.0004\n",
            "Epoch [12/100], Step [200/3600], Loss: 0.1210\n",
            "Epoch [12/100], Step [300/3600], Loss: 0.0086\n",
            "Epoch [12/100], Step [400/3600], Loss: 0.0231\n",
            "Epoch [12/100], Step [500/3600], Loss: 0.0296\n",
            "Epoch [12/100], Step [600/3600], Loss: 0.0001\n",
            "Epoch [12/100], Step [700/3600], Loss: 0.0044\n",
            "Epoch [12/100], Step [800/3600], Loss: 0.0120\n",
            "Epoch [12/100], Step [900/3600], Loss: 0.0004\n",
            "Epoch [12/100], Step [1000/3600], Loss: 0.0028\n",
            "Epoch [12/100], Step [1100/3600], Loss: 0.0429\n",
            "Epoch [12/100], Step [1200/3600], Loss: 0.3794\n",
            "Epoch [12/100], Step [1300/3600], Loss: 0.0071\n",
            "Epoch [12/100], Step [1400/3600], Loss: 0.1086\n",
            "Epoch [12/100], Step [1500/3600], Loss: 0.0097\n",
            "Epoch [12/100], Step [1600/3600], Loss: 0.0567\n",
            "Epoch [12/100], Step [1700/3600], Loss: 0.0070\n",
            "Epoch [12/100], Step [1800/3600], Loss: 0.2035\n",
            "Epoch [12/100], Step [1900/3600], Loss: 0.0937\n",
            "Epoch [12/100], Step [2000/3600], Loss: 0.0003\n",
            "Epoch [12/100], Step [2100/3600], Loss: 0.0368\n",
            "Epoch [12/100], Step [2200/3600], Loss: 0.1518\n",
            "Epoch [12/100], Step [2300/3600], Loss: 0.0251\n",
            "Epoch [12/100], Step [2400/3600], Loss: 0.0001\n",
            "Epoch [12/100], Step [2500/3600], Loss: 0.0123\n",
            "Epoch [12/100], Step [2600/3600], Loss: 0.0603\n",
            "Epoch [12/100], Step [2700/3600], Loss: 0.0010\n",
            "Epoch [12/100], Step [2800/3600], Loss: 0.2112\n",
            "Epoch [13/100], Step [100/3600], Loss: 0.0001\n",
            "Epoch [13/100], Step [200/3600], Loss: 0.0686\n",
            "Epoch [13/100], Step [300/3600], Loss: 0.0042\n",
            "Epoch [13/100], Step [400/3600], Loss: 0.0232\n",
            "Epoch [13/100], Step [500/3600], Loss: 0.0065\n",
            "Epoch [13/100], Step [600/3600], Loss: 0.0001\n",
            "Epoch [13/100], Step [700/3600], Loss: 0.0034\n",
            "Epoch [13/100], Step [800/3600], Loss: 0.0011\n",
            "Epoch [13/100], Step [900/3600], Loss: 0.0002\n",
            "Epoch [13/100], Step [1000/3600], Loss: 0.0014\n",
            "Epoch [13/100], Step [1100/3600], Loss: 0.0024\n",
            "Epoch [13/100], Step [1200/3600], Loss: 0.0722\n",
            "Epoch [13/100], Step [1300/3600], Loss: 0.0002\n",
            "Epoch [13/100], Step [1400/3600], Loss: 0.0847\n",
            "Epoch [13/100], Step [1500/3600], Loss: 0.0120\n",
            "Epoch [13/100], Step [1600/3600], Loss: 0.0062\n",
            "Epoch [13/100], Step [1700/3600], Loss: 0.0068\n",
            "Epoch [13/100], Step [1800/3600], Loss: 0.0058\n",
            "Epoch [13/100], Step [1900/3600], Loss: 0.0530\n",
            "Epoch [13/100], Step [2000/3600], Loss: 0.0023\n",
            "Epoch [13/100], Step [2100/3600], Loss: 0.0015\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [13/100], Step [2200/3600], Loss: 0.2465\n",
            "Epoch [13/100], Step [2300/3600], Loss: 0.3589\n",
            "Epoch [13/100], Step [2400/3600], Loss: 0.0049\n",
            "Epoch [13/100], Step [2500/3600], Loss: 0.0007\n",
            "Epoch [13/100], Step [2600/3600], Loss: 0.2163\n",
            "Epoch [13/100], Step [2700/3600], Loss: 0.0001\n",
            "Epoch [13/100], Step [2800/3600], Loss: 0.2403\n",
            "Epoch [14/100], Step [100/3600], Loss: 0.0184\n",
            "Epoch [14/100], Step [200/3600], Loss: 0.0060\n",
            "Epoch [14/100], Step [300/3600], Loss: 0.0912\n",
            "Epoch [14/100], Step [400/3600], Loss: 0.0382\n",
            "Epoch [14/100], Step [500/3600], Loss: 0.0518\n",
            "Epoch [14/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [14/100], Step [700/3600], Loss: 0.0076\n",
            "Epoch [14/100], Step [800/3600], Loss: 0.0008\n",
            "Epoch [14/100], Step [900/3600], Loss: 0.0026\n",
            "Epoch [14/100], Step [1000/3600], Loss: 0.0017\n",
            "Epoch [14/100], Step [1100/3600], Loss: 0.1365\n",
            "Epoch [14/100], Step [1200/3600], Loss: 0.1230\n",
            "Epoch [14/100], Step [1300/3600], Loss: 0.0050\n",
            "Epoch [14/100], Step [1400/3600], Loss: 0.4479\n",
            "Epoch [14/100], Step [1500/3600], Loss: 0.0037\n",
            "Epoch [14/100], Step [1600/3600], Loss: 0.0010\n",
            "Epoch [14/100], Step [1700/3600], Loss: 0.0013\n",
            "Epoch [14/100], Step [1800/3600], Loss: 0.0116\n",
            "Epoch [14/100], Step [1900/3600], Loss: 0.0089\n",
            "Epoch [14/100], Step [2000/3600], Loss: 0.0001\n",
            "Epoch [14/100], Step [2100/3600], Loss: 0.0133\n",
            "Epoch [14/100], Step [2200/3600], Loss: 0.0755\n",
            "Epoch [14/100], Step [2300/3600], Loss: 0.1730\n",
            "Epoch [14/100], Step [2400/3600], Loss: 0.0024\n",
            "Epoch [14/100], Step [2500/3600], Loss: 0.0005\n",
            "Epoch [14/100], Step [2600/3600], Loss: 0.0223\n",
            "Epoch [14/100], Step [2700/3600], Loss: 0.0008\n",
            "Epoch [14/100], Step [2800/3600], Loss: 0.0239\n",
            "Epoch [15/100], Step [100/3600], Loss: 0.0027\n",
            "Epoch [15/100], Step [200/3600], Loss: 0.0060\n",
            "Epoch [15/100], Step [300/3600], Loss: 0.0033\n",
            "Epoch [15/100], Step [400/3600], Loss: 0.0256\n",
            "Epoch [15/100], Step [500/3600], Loss: 0.0056\n",
            "Epoch [15/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [15/100], Step [700/3600], Loss: 0.0063\n",
            "Epoch [15/100], Step [800/3600], Loss: 0.0015\n",
            "Epoch [15/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [15/100], Step [1000/3600], Loss: 0.0053\n",
            "Epoch [15/100], Step [1100/3600], Loss: 0.0004\n",
            "Epoch [15/100], Step [1200/3600], Loss: 0.3474\n",
            "Epoch [15/100], Step [1300/3600], Loss: 0.0019\n",
            "Epoch [15/100], Step [1400/3600], Loss: 0.0581\n",
            "Epoch [15/100], Step [1500/3600], Loss: 0.0028\n",
            "Epoch [15/100], Step [1600/3600], Loss: 0.0182\n",
            "Epoch [15/100], Step [1700/3600], Loss: 0.0008\n",
            "Epoch [15/100], Step [1800/3600], Loss: 0.0035\n",
            "Epoch [15/100], Step [1900/3600], Loss: 0.0480\n",
            "Epoch [15/100], Step [2000/3600], Loss: 0.0002\n",
            "Epoch [15/100], Step [2100/3600], Loss: 0.0072\n",
            "Epoch [15/100], Step [2200/3600], Loss: 0.0134\n",
            "Epoch [15/100], Step [2300/3600], Loss: 0.0117\n",
            "Epoch [15/100], Step [2400/3600], Loss: 0.0056\n",
            "Epoch [15/100], Step [2500/3600], Loss: 0.0014\n",
            "Epoch [15/100], Step [2600/3600], Loss: 0.0310\n",
            "Epoch [15/100], Step [2700/3600], Loss: 0.0001\n",
            "Epoch [15/100], Step [2800/3600], Loss: 0.1350\n",
            "Epoch [16/100], Step [100/3600], Loss: 0.0001\n",
            "Epoch [16/100], Step [200/3600], Loss: 0.2058\n",
            "Epoch [16/100], Step [300/3600], Loss: 0.0004\n",
            "Epoch [16/100], Step [400/3600], Loss: 0.0171\n",
            "Epoch [16/100], Step [500/3600], Loss: 0.0565\n",
            "Epoch [16/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [16/100], Step [700/3600], Loss: 0.0686\n",
            "Epoch [16/100], Step [800/3600], Loss: 0.0005\n",
            "Epoch [16/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [16/100], Step [1000/3600], Loss: 0.0010\n",
            "Epoch [16/100], Step [1100/3600], Loss: 0.0003\n",
            "Epoch [16/100], Step [1200/3600], Loss: 0.0216\n",
            "Epoch [16/100], Step [1300/3600], Loss: 0.0022\n",
            "Epoch [16/100], Step [1400/3600], Loss: 0.0408\n",
            "Epoch [16/100], Step [1500/3600], Loss: 0.0528\n",
            "Epoch [16/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [16/100], Step [1700/3600], Loss: 0.0076\n",
            "Epoch [16/100], Step [1800/3600], Loss: 0.0375\n",
            "Epoch [16/100], Step [1900/3600], Loss: 0.0181\n",
            "Epoch [16/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [16/100], Step [2100/3600], Loss: 0.0031\n",
            "Epoch [16/100], Step [2200/3600], Loss: 0.0018\n",
            "Epoch [16/100], Step [2300/3600], Loss: 0.0039\n",
            "Epoch [16/100], Step [2400/3600], Loss: 0.0017\n",
            "Epoch [16/100], Step [2500/3600], Loss: 0.0038\n",
            "Epoch [16/100], Step [2600/3600], Loss: 0.0604\n",
            "Epoch [16/100], Step [2700/3600], Loss: 0.0001\n",
            "Epoch [16/100], Step [2800/3600], Loss: 0.0865\n",
            "Epoch [17/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [17/100], Step [200/3600], Loss: 0.0123\n",
            "Epoch [17/100], Step [300/3600], Loss: 0.0032\n",
            "Epoch [17/100], Step [400/3600], Loss: 0.2375\n",
            "Epoch [17/100], Step [500/3600], Loss: 0.1665\n",
            "Epoch [17/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [17/100], Step [700/3600], Loss: 0.0266\n",
            "Epoch [17/100], Step [800/3600], Loss: 0.0004\n",
            "Epoch [17/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [17/100], Step [1000/3600], Loss: 0.0001\n",
            "Epoch [17/100], Step [1100/3600], Loss: 0.0018\n",
            "Epoch [17/100], Step [1200/3600], Loss: 0.0717\n",
            "Epoch [17/100], Step [1300/3600], Loss: 0.0010\n",
            "Epoch [17/100], Step [1400/3600], Loss: 0.8896\n",
            "Epoch [17/100], Step [1500/3600], Loss: 0.0050\n",
            "Epoch [17/100], Step [1600/3600], Loss: 0.0005\n",
            "Epoch [17/100], Step [1700/3600], Loss: 0.0026\n",
            "Epoch [17/100], Step [1800/3600], Loss: 0.0541\n",
            "Epoch [17/100], Step [1900/3600], Loss: 0.0247\n",
            "Epoch [17/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [17/100], Step [2100/3600], Loss: 0.0094\n",
            "Epoch [17/100], Step [2200/3600], Loss: 0.0574\n",
            "Epoch [17/100], Step [2300/3600], Loss: 0.0114\n",
            "Epoch [17/100], Step [2400/3600], Loss: 0.0055\n",
            "Epoch [17/100], Step [2500/3600], Loss: 0.0367\n",
            "Epoch [17/100], Step [2600/3600], Loss: 0.2388\n",
            "Epoch [17/100], Step [2700/3600], Loss: 0.0072\n",
            "Epoch [17/100], Step [2800/3600], Loss: 0.1905\n",
            "Epoch [18/100], Step [100/3600], Loss: 0.0001\n",
            "Epoch [18/100], Step [200/3600], Loss: 0.0109\n",
            "Epoch [18/100], Step [300/3600], Loss: 0.0153\n",
            "Epoch [18/100], Step [400/3600], Loss: 0.0027\n",
            "Epoch [18/100], Step [500/3600], Loss: 0.0733\n",
            "Epoch [18/100], Step [600/3600], Loss: 0.0001\n",
            "Epoch [18/100], Step [700/3600], Loss: 0.0020\n",
            "Epoch [18/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [18/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [18/100], Step [1000/3600], Loss: 0.0003\n",
            "Epoch [18/100], Step [1100/3600], Loss: 0.0001\n",
            "Epoch [18/100], Step [1200/3600], Loss: 0.0175\n",
            "Epoch [18/100], Step [1300/3600], Loss: 0.0002\n",
            "Epoch [18/100], Step [1400/3600], Loss: 0.0411\n",
            "Epoch [18/100], Step [1500/3600], Loss: 0.0285\n",
            "Epoch [18/100], Step [1600/3600], Loss: 0.0019\n",
            "Epoch [18/100], Step [1700/3600], Loss: 0.0106\n",
            "Epoch [18/100], Step [1800/3600], Loss: 0.0003\n",
            "Epoch [18/100], Step [1900/3600], Loss: 0.0110\n",
            "Epoch [18/100], Step [2000/3600], Loss: 0.0001\n",
            "Epoch [18/100], Step [2100/3600], Loss: 0.0037\n",
            "Epoch [18/100], Step [2200/3600], Loss: 0.0236\n",
            "Epoch [18/100], Step [2300/3600], Loss: 0.0236\n",
            "Epoch [18/100], Step [2400/3600], Loss: 0.0026\n",
            "Epoch [18/100], Step [2500/3600], Loss: 0.0075\n",
            "Epoch [18/100], Step [2600/3600], Loss: 0.0731\n",
            "Epoch [18/100], Step [2700/3600], Loss: 0.0006\n",
            "Epoch [18/100], Step [2800/3600], Loss: 0.0948\n",
            "Epoch [19/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [19/100], Step [200/3600], Loss: 0.0205\n",
            "Epoch [19/100], Step [300/3600], Loss: 0.0014\n",
            "Epoch [19/100], Step [400/3600], Loss: 0.0044\n",
            "Epoch [19/100], Step [500/3600], Loss: 0.0379\n",
            "Epoch [19/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [19/100], Step [700/3600], Loss: 0.0089\n",
            "Epoch [19/100], Step [800/3600], Loss: 0.0003\n",
            "Epoch [19/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [19/100], Step [1000/3600], Loss: 0.0377\n",
            "Epoch [19/100], Step [1100/3600], Loss: 0.0001\n",
            "Epoch [19/100], Step [1200/3600], Loss: 0.3010\n",
            "Epoch [19/100], Step [1300/3600], Loss: 0.0006\n",
            "Epoch [19/100], Step [1400/3600], Loss: 0.0542\n",
            "Epoch [19/100], Step [1500/3600], Loss: 0.1625\n",
            "Epoch [19/100], Step [1600/3600], Loss: 0.0002\n",
            "Epoch [19/100], Step [1700/3600], Loss: 0.0002\n",
            "Epoch [19/100], Step [1800/3600], Loss: 0.0004\n",
            "Epoch [19/100], Step [1900/3600], Loss: 0.0098\n",
            "Epoch [19/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [19/100], Step [2100/3600], Loss: 0.0019\n",
            "Epoch [19/100], Step [2200/3600], Loss: 0.0957\n",
            "Epoch [19/100], Step [2300/3600], Loss: 0.0100\n",
            "Epoch [19/100], Step [2400/3600], Loss: 0.0005\n",
            "Epoch [19/100], Step [2500/3600], Loss: 0.0032\n",
            "Epoch [19/100], Step [2600/3600], Loss: 0.0104\n",
            "Epoch [19/100], Step [2700/3600], Loss: 0.0006\n",
            "Epoch [19/100], Step [2800/3600], Loss: 0.2400\n",
            "Epoch [20/100], Step [100/3600], Loss: 0.0000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [20/100], Step [200/3600], Loss: 0.0046\n",
            "Epoch [20/100], Step [300/3600], Loss: 0.0022\n",
            "Epoch [20/100], Step [400/3600], Loss: 0.0019\n",
            "Epoch [20/100], Step [500/3600], Loss: 0.0214\n",
            "Epoch [20/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [20/100], Step [700/3600], Loss: 0.0025\n",
            "Epoch [20/100], Step [800/3600], Loss: 0.0052\n",
            "Epoch [20/100], Step [900/3600], Loss: 0.0004\n",
            "Epoch [20/100], Step [1000/3600], Loss: 0.0003\n",
            "Epoch [20/100], Step [1100/3600], Loss: 0.0008\n",
            "Epoch [20/100], Step [1200/3600], Loss: 0.0007\n",
            "Epoch [20/100], Step [1300/3600], Loss: 0.0001\n",
            "Epoch [20/100], Step [1400/3600], Loss: 0.1294\n",
            "Epoch [20/100], Step [1500/3600], Loss: 0.0090\n",
            "Epoch [20/100], Step [1600/3600], Loss: 0.0003\n",
            "Epoch [20/100], Step [1700/3600], Loss: 0.0248\n",
            "Epoch [20/100], Step [1800/3600], Loss: 0.0001\n",
            "Epoch [20/100], Step [1900/3600], Loss: 0.0092\n",
            "Epoch [20/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [20/100], Step [2100/3600], Loss: 0.0402\n",
            "Epoch [20/100], Step [2200/3600], Loss: 0.0054\n",
            "Epoch [20/100], Step [2300/3600], Loss: 0.0002\n",
            "Epoch [20/100], Step [2400/3600], Loss: 0.0001\n",
            "Epoch [20/100], Step [2500/3600], Loss: 0.0031\n",
            "Epoch [20/100], Step [2600/3600], Loss: 0.0113\n",
            "Epoch [20/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [20/100], Step [2800/3600], Loss: 0.3415\n",
            "Epoch [21/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [21/100], Step [200/3600], Loss: 0.0197\n",
            "Epoch [21/100], Step [300/3600], Loss: 0.0062\n",
            "Epoch [21/100], Step [400/3600], Loss: 0.0013\n",
            "Epoch [21/100], Step [500/3600], Loss: 0.1033\n",
            "Epoch [21/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [21/100], Step [700/3600], Loss: 0.0030\n",
            "Epoch [21/100], Step [800/3600], Loss: 0.0010\n",
            "Epoch [21/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [21/100], Step [1000/3600], Loss: 0.0013\n",
            "Epoch [21/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [21/100], Step [1200/3600], Loss: 0.0020\n",
            "Epoch [21/100], Step [1300/3600], Loss: 0.0002\n",
            "Epoch [21/100], Step [1400/3600], Loss: 0.0032\n",
            "Epoch [21/100], Step [1500/3600], Loss: 0.0012\n",
            "Epoch [21/100], Step [1600/3600], Loss: 0.0014\n",
            "Epoch [21/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [21/100], Step [1800/3600], Loss: 0.0011\n",
            "Epoch [21/100], Step [1900/3600], Loss: 0.0252\n",
            "Epoch [21/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [21/100], Step [2100/3600], Loss: 0.0573\n",
            "Epoch [21/100], Step [2200/3600], Loss: 0.0047\n",
            "Epoch [21/100], Step [2300/3600], Loss: 0.0123\n",
            "Epoch [21/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [21/100], Step [2500/3600], Loss: 0.0219\n",
            "Epoch [21/100], Step [2600/3600], Loss: 0.0031\n",
            "Epoch [21/100], Step [2700/3600], Loss: 0.0003\n",
            "Epoch [21/100], Step [2800/3600], Loss: 0.2312\n",
            "Epoch [22/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [22/100], Step [200/3600], Loss: 0.0016\n",
            "Epoch [22/100], Step [300/3600], Loss: 0.0027\n",
            "Epoch [22/100], Step [400/3600], Loss: 0.0030\n",
            "Epoch [22/100], Step [500/3600], Loss: 0.0004\n",
            "Epoch [22/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [22/100], Step [700/3600], Loss: 0.0027\n",
            "Epoch [22/100], Step [800/3600], Loss: 0.0001\n",
            "Epoch [22/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [22/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [22/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [22/100], Step [1200/3600], Loss: 0.0010\n",
            "Epoch [22/100], Step [1300/3600], Loss: 0.0002\n",
            "Epoch [22/100], Step [1400/3600], Loss: 0.0100\n",
            "Epoch [22/100], Step [1500/3600], Loss: 0.0127\n",
            "Epoch [22/100], Step [1600/3600], Loss: 0.0070\n",
            "Epoch [22/100], Step [1700/3600], Loss: 0.0004\n",
            "Epoch [22/100], Step [1800/3600], Loss: 0.0001\n",
            "Epoch [22/100], Step [1900/3600], Loss: 0.0269\n",
            "Epoch [22/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [22/100], Step [2100/3600], Loss: 0.0104\n",
            "Epoch [22/100], Step [2200/3600], Loss: 0.0021\n",
            "Epoch [22/100], Step [2300/3600], Loss: 0.4417\n",
            "Epoch [22/100], Step [2400/3600], Loss: 0.0274\n",
            "Epoch [22/100], Step [2500/3600], Loss: 0.0286\n",
            "Epoch [22/100], Step [2600/3600], Loss: 0.0764\n",
            "Epoch [22/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [22/100], Step [2800/3600], Loss: 0.0439\n",
            "Epoch [23/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [23/100], Step [200/3600], Loss: 0.0084\n",
            "Epoch [23/100], Step [300/3600], Loss: 0.0001\n",
            "Epoch [23/100], Step [400/3600], Loss: 0.0054\n",
            "Epoch [23/100], Step [500/3600], Loss: 0.0126\n",
            "Epoch [23/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [23/100], Step [700/3600], Loss: 0.0017\n",
            "Epoch [23/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [23/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [23/100], Step [1000/3600], Loss: 0.0001\n",
            "Epoch [23/100], Step [1100/3600], Loss: 0.0017\n",
            "Epoch [23/100], Step [1200/3600], Loss: 0.0029\n",
            "Epoch [23/100], Step [1300/3600], Loss: 0.0007\n",
            "Epoch [23/100], Step [1400/3600], Loss: 0.0888\n",
            "Epoch [23/100], Step [1500/3600], Loss: 0.0084\n",
            "Epoch [23/100], Step [1600/3600], Loss: 0.0001\n",
            "Epoch [23/100], Step [1700/3600], Loss: 0.0613\n",
            "Epoch [23/100], Step [1800/3600], Loss: 0.0004\n",
            "Epoch [23/100], Step [1900/3600], Loss: 0.0026\n",
            "Epoch [23/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [23/100], Step [2100/3600], Loss: 0.0196\n",
            "Epoch [23/100], Step [2200/3600], Loss: 0.0004\n",
            "Epoch [23/100], Step [2300/3600], Loss: 0.0040\n",
            "Epoch [23/100], Step [2400/3600], Loss: 0.0023\n",
            "Epoch [23/100], Step [2500/3600], Loss: 0.0027\n",
            "Epoch [23/100], Step [2600/3600], Loss: 0.0215\n",
            "Epoch [23/100], Step [2700/3600], Loss: 0.0029\n",
            "Epoch [23/100], Step [2800/3600], Loss: 0.2215\n",
            "Epoch [24/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [24/100], Step [200/3600], Loss: 0.0013\n",
            "Epoch [24/100], Step [300/3600], Loss: 0.0009\n",
            "Epoch [24/100], Step [400/3600], Loss: 0.0015\n",
            "Epoch [24/100], Step [500/3600], Loss: 0.0258\n",
            "Epoch [24/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [24/100], Step [700/3600], Loss: 0.0007\n",
            "Epoch [24/100], Step [800/3600], Loss: 0.0037\n",
            "Epoch [24/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [24/100], Step [1000/3600], Loss: 0.0002\n",
            "Epoch [24/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [24/100], Step [1200/3600], Loss: 0.0019\n",
            "Epoch [24/100], Step [1300/3600], Loss: 0.0004\n",
            "Epoch [24/100], Step [1400/3600], Loss: 0.0096\n",
            "Epoch [24/100], Step [1500/3600], Loss: 0.0005\n",
            "Epoch [24/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [24/100], Step [1700/3600], Loss: 0.0004\n",
            "Epoch [24/100], Step [1800/3600], Loss: 0.0015\n",
            "Epoch [24/100], Step [1900/3600], Loss: 0.0087\n",
            "Epoch [24/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [24/100], Step [2100/3600], Loss: 0.0037\n",
            "Epoch [24/100], Step [2200/3600], Loss: 0.0150\n",
            "Epoch [24/100], Step [2300/3600], Loss: 0.1752\n",
            "Epoch [24/100], Step [2400/3600], Loss: 0.0103\n",
            "Epoch [24/100], Step [2500/3600], Loss: 0.0120\n",
            "Epoch [24/100], Step [2600/3600], Loss: 0.0038\n",
            "Epoch [24/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [24/100], Step [2800/3600], Loss: 0.0062\n",
            "Epoch [25/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [25/100], Step [200/3600], Loss: 0.0152\n",
            "Epoch [25/100], Step [300/3600], Loss: 0.0816\n",
            "Epoch [25/100], Step [400/3600], Loss: 0.0002\n",
            "Epoch [25/100], Step [500/3600], Loss: 4.9079\n",
            "Epoch [25/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [25/100], Step [700/3600], Loss: 0.0003\n",
            "Epoch [25/100], Step [800/3600], Loss: 0.0007\n",
            "Epoch [25/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [25/100], Step [1000/3600], Loss: 0.0002\n",
            "Epoch [25/100], Step [1100/3600], Loss: 0.0056\n",
            "Epoch [25/100], Step [1200/3600], Loss: 0.0013\n",
            "Epoch [25/100], Step [1300/3600], Loss: 0.0002\n",
            "Epoch [25/100], Step [1400/3600], Loss: 0.0015\n",
            "Epoch [25/100], Step [1500/3600], Loss: 0.0003\n",
            "Epoch [25/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [25/100], Step [1700/3600], Loss: 0.0008\n",
            "Epoch [25/100], Step [1800/3600], Loss: 0.0007\n",
            "Epoch [25/100], Step [1900/3600], Loss: 0.0013\n",
            "Epoch [25/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [25/100], Step [2100/3600], Loss: 0.0001\n",
            "Epoch [25/100], Step [2200/3600], Loss: 0.0035\n",
            "Epoch [25/100], Step [2300/3600], Loss: 0.0021\n",
            "Epoch [25/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [25/100], Step [2500/3600], Loss: 0.0350\n",
            "Epoch [25/100], Step [2600/3600], Loss: 0.1298\n",
            "Epoch [25/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [25/100], Step [2800/3600], Loss: 0.2341\n",
            "Epoch [26/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [26/100], Step [200/3600], Loss: 0.0004\n",
            "Epoch [26/100], Step [300/3600], Loss: 0.0009\n",
            "Epoch [26/100], Step [400/3600], Loss: 0.0003\n",
            "Epoch [26/100], Step [500/3600], Loss: 0.0006\n",
            "Epoch [26/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [26/100], Step [700/3600], Loss: 0.0063\n",
            "Epoch [26/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [26/100], Step [900/3600], Loss: 0.0000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [26/100], Step [1000/3600], Loss: 0.0064\n",
            "Epoch [26/100], Step [1100/3600], Loss: 0.0003\n",
            "Epoch [26/100], Step [1200/3600], Loss: 0.0013\n",
            "Epoch [26/100], Step [1300/3600], Loss: 0.0003\n",
            "Epoch [26/100], Step [1400/3600], Loss: 0.0115\n",
            "Epoch [26/100], Step [1500/3600], Loss: 0.0003\n",
            "Epoch [26/100], Step [1600/3600], Loss: 0.0011\n",
            "Epoch [26/100], Step [1700/3600], Loss: 0.0003\n",
            "Epoch [26/100], Step [1800/3600], Loss: 0.0006\n",
            "Epoch [26/100], Step [1900/3600], Loss: 0.0066\n",
            "Epoch [26/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [26/100], Step [2100/3600], Loss: 0.0002\n",
            "Epoch [26/100], Step [2200/3600], Loss: 0.0044\n",
            "Epoch [26/100], Step [2300/3600], Loss: 0.0089\n",
            "Epoch [26/100], Step [2400/3600], Loss: 0.0006\n",
            "Epoch [26/100], Step [2500/3600], Loss: 0.0458\n",
            "Epoch [26/100], Step [2600/3600], Loss: 0.0045\n",
            "Epoch [26/100], Step [2700/3600], Loss: 0.0518\n",
            "Epoch [26/100], Step [2800/3600], Loss: 0.0048\n",
            "Epoch [27/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [27/100], Step [200/3600], Loss: 0.0005\n",
            "Epoch [27/100], Step [300/3600], Loss: 0.0010\n",
            "Epoch [27/100], Step [400/3600], Loss: 0.0004\n",
            "Epoch [27/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [27/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [27/100], Step [700/3600], Loss: 0.0007\n",
            "Epoch [27/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [27/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [27/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [27/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [27/100], Step [1200/3600], Loss: 0.0023\n",
            "Epoch [27/100], Step [1300/3600], Loss: 0.0002\n",
            "Epoch [27/100], Step [1400/3600], Loss: 0.0009\n",
            "Epoch [27/100], Step [1500/3600], Loss: 0.0430\n",
            "Epoch [27/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [27/100], Step [1700/3600], Loss: 0.0005\n",
            "Epoch [27/100], Step [1800/3600], Loss: 0.0003\n",
            "Epoch [27/100], Step [1900/3600], Loss: 0.0083\n",
            "Epoch [27/100], Step [2000/3600], Loss: 0.0001\n",
            "Epoch [27/100], Step [2100/3600], Loss: 0.0015\n",
            "Epoch [27/100], Step [2200/3600], Loss: 0.0012\n",
            "Epoch [27/100], Step [2300/3600], Loss: 0.0018\n",
            "Epoch [27/100], Step [2400/3600], Loss: 0.0001\n",
            "Epoch [27/100], Step [2500/3600], Loss: 0.0014\n",
            "Epoch [27/100], Step [2600/3600], Loss: 0.0040\n",
            "Epoch [27/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [27/100], Step [2800/3600], Loss: 0.0172\n",
            "Epoch [28/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [28/100], Step [200/3600], Loss: 0.0063\n",
            "Epoch [28/100], Step [300/3600], Loss: 0.0009\n",
            "Epoch [28/100], Step [400/3600], Loss: 0.0097\n",
            "Epoch [28/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [28/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [28/100], Step [700/3600], Loss: 0.0231\n",
            "Epoch [28/100], Step [800/3600], Loss: 0.0001\n",
            "Epoch [28/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [28/100], Step [1000/3600], Loss: 0.0001\n",
            "Epoch [28/100], Step [1100/3600], Loss: 0.0001\n",
            "Epoch [28/100], Step [1200/3600], Loss: 0.0006\n",
            "Epoch [28/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [28/100], Step [1400/3600], Loss: 0.0375\n",
            "Epoch [28/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [28/100], Step [1600/3600], Loss: 0.0001\n",
            "Epoch [28/100], Step [1700/3600], Loss: 0.0004\n",
            "Epoch [28/100], Step [1800/3600], Loss: 0.0001\n",
            "Epoch [28/100], Step [1900/3600], Loss: 0.0076\n",
            "Epoch [28/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [28/100], Step [2100/3600], Loss: 0.0001\n",
            "Epoch [28/100], Step [2200/3600], Loss: 0.0006\n",
            "Epoch [28/100], Step [2300/3600], Loss: 0.0030\n",
            "Epoch [28/100], Step [2400/3600], Loss: 0.0052\n",
            "Epoch [28/100], Step [2500/3600], Loss: 0.0128\n",
            "Epoch [28/100], Step [2600/3600], Loss: 0.0111\n",
            "Epoch [28/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [28/100], Step [2800/3600], Loss: 0.0069\n",
            "Epoch [29/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [29/100], Step [200/3600], Loss: 0.0001\n",
            "Epoch [29/100], Step [300/3600], Loss: 0.0037\n",
            "Epoch [29/100], Step [400/3600], Loss: 0.0012\n",
            "Epoch [29/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [29/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [29/100], Step [700/3600], Loss: 0.0004\n",
            "Epoch [29/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [29/100], Step [900/3600], Loss: 0.0002\n",
            "Epoch [29/100], Step [1000/3600], Loss: 0.0011\n",
            "Epoch [29/100], Step [1100/3600], Loss: 0.0002\n",
            "Epoch [29/100], Step [1200/3600], Loss: 0.0005\n",
            "Epoch [29/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [29/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [29/100], Step [1500/3600], Loss: 0.5884\n",
            "Epoch [29/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [29/100], Step [1700/3600], Loss: 0.0152\n",
            "Epoch [29/100], Step [1800/3600], Loss: 0.0001\n",
            "Epoch [29/100], Step [1900/3600], Loss: 0.0398\n",
            "Epoch [29/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [29/100], Step [2100/3600], Loss: 0.0000\n",
            "Epoch [29/100], Step [2200/3600], Loss: 0.6784\n",
            "Epoch [29/100], Step [2300/3600], Loss: 0.0007\n",
            "Epoch [29/100], Step [2400/3600], Loss: 0.0014\n",
            "Epoch [29/100], Step [2500/3600], Loss: 0.0353\n",
            "Epoch [29/100], Step [2600/3600], Loss: 0.0034\n",
            "Epoch [29/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [29/100], Step [2800/3600], Loss: 0.0020\n",
            "Epoch [30/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [30/100], Step [200/3600], Loss: 0.0016\n",
            "Epoch [30/100], Step [300/3600], Loss: 0.0001\n",
            "Epoch [30/100], Step [400/3600], Loss: 0.3729\n",
            "Epoch [30/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [30/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [30/100], Step [700/3600], Loss: 0.0027\n",
            "Epoch [30/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [30/100], Step [900/3600], Loss: 0.0006\n",
            "Epoch [30/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [30/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [30/100], Step [1200/3600], Loss: 0.0126\n",
            "Epoch [30/100], Step [1300/3600], Loss: 0.0001\n",
            "Epoch [30/100], Step [1400/3600], Loss: 0.0003\n",
            "Epoch [30/100], Step [1500/3600], Loss: 0.0001\n",
            "Epoch [30/100], Step [1600/3600], Loss: 0.0001\n",
            "Epoch [30/100], Step [1700/3600], Loss: 0.0001\n",
            "Epoch [30/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [30/100], Step [1900/3600], Loss: 0.0249\n",
            "Epoch [30/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [30/100], Step [2100/3600], Loss: 0.0006\n",
            "Epoch [30/100], Step [2200/3600], Loss: 0.0006\n",
            "Epoch [30/100], Step [2300/3600], Loss: 0.0201\n",
            "Epoch [30/100], Step [2400/3600], Loss: 0.0170\n",
            "Epoch [30/100], Step [2500/3600], Loss: 0.0041\n",
            "Epoch [30/100], Step [2600/3600], Loss: 0.0978\n",
            "Epoch [30/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [30/100], Step [2800/3600], Loss: 0.0026\n",
            "Epoch [31/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [31/100], Step [200/3600], Loss: 0.0001\n",
            "Epoch [31/100], Step [300/3600], Loss: 0.0005\n",
            "Epoch [31/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [31/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [31/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [31/100], Step [700/3600], Loss: 0.0009\n",
            "Epoch [31/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [31/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [31/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [31/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [31/100], Step [1200/3600], Loss: 0.0032\n",
            "Epoch [31/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [31/100], Step [1400/3600], Loss: 0.0038\n",
            "Epoch [31/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [31/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [31/100], Step [1700/3600], Loss: 0.0001\n",
            "Epoch [31/100], Step [1800/3600], Loss: 0.0010\n",
            "Epoch [31/100], Step [1900/3600], Loss: 0.0078\n",
            "Epoch [31/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [31/100], Step [2100/3600], Loss: 0.0000\n",
            "Epoch [31/100], Step [2200/3600], Loss: 0.0007\n",
            "Epoch [31/100], Step [2300/3600], Loss: 0.0000\n",
            "Epoch [31/100], Step [2400/3600], Loss: 0.0002\n",
            "Epoch [31/100], Step [2500/3600], Loss: 0.0234\n",
            "Epoch [31/100], Step [2600/3600], Loss: 0.2073\n",
            "Epoch [31/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [31/100], Step [2800/3600], Loss: 0.0163\n",
            "Epoch [32/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [32/100], Step [200/3600], Loss: 0.0107\n",
            "Epoch [32/100], Step [300/3600], Loss: 0.0026\n",
            "Epoch [32/100], Step [400/3600], Loss: 0.0001\n",
            "Epoch [32/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [32/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [32/100], Step [700/3600], Loss: 0.0001\n",
            "Epoch [32/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [32/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [32/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [32/100], Step [1100/3600], Loss: 0.0001\n",
            "Epoch [32/100], Step [1200/3600], Loss: 0.0049\n",
            "Epoch [32/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [32/100], Step [1400/3600], Loss: 0.0001\n",
            "Epoch [32/100], Step [1500/3600], Loss: 0.0001\n",
            "Epoch [32/100], Step [1600/3600], Loss: 0.0048\n",
            "Epoch [32/100], Step [1700/3600], Loss: 0.0007\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [32/100], Step [1800/3600], Loss: 0.0010\n",
            "Epoch [32/100], Step [1900/3600], Loss: 0.0034\n",
            "Epoch [32/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [32/100], Step [2100/3600], Loss: 0.0000\n",
            "Epoch [32/100], Step [2200/3600], Loss: 0.0075\n",
            "Epoch [32/100], Step [2300/3600], Loss: 0.0024\n",
            "Epoch [32/100], Step [2400/3600], Loss: 0.0001\n",
            "Epoch [32/100], Step [2500/3600], Loss: 0.0850\n",
            "Epoch [32/100], Step [2600/3600], Loss: 0.0039\n",
            "Epoch [32/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [32/100], Step [2800/3600], Loss: 0.0327\n",
            "Epoch [33/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [33/100], Step [200/3600], Loss: 0.0010\n",
            "Epoch [33/100], Step [300/3600], Loss: 0.0321\n",
            "Epoch [33/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [33/100], Step [500/3600], Loss: 0.0001\n",
            "Epoch [33/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [33/100], Step [700/3600], Loss: 0.0371\n",
            "Epoch [33/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [33/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [33/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [33/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [33/100], Step [1200/3600], Loss: 0.0121\n",
            "Epoch [33/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [33/100], Step [1400/3600], Loss: 0.0104\n",
            "Epoch [33/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [33/100], Step [1600/3600], Loss: 0.0003\n",
            "Epoch [33/100], Step [1700/3600], Loss: 0.0247\n",
            "Epoch [33/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [33/100], Step [1900/3600], Loss: 0.0703\n",
            "Epoch [33/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [33/100], Step [2100/3600], Loss: 0.0011\n",
            "Epoch [33/100], Step [2200/3600], Loss: 0.0054\n",
            "Epoch [33/100], Step [2300/3600], Loss: 0.0029\n",
            "Epoch [33/100], Step [2400/3600], Loss: 0.0002\n",
            "Epoch [33/100], Step [2500/3600], Loss: 0.0335\n",
            "Epoch [33/100], Step [2600/3600], Loss: 0.0017\n",
            "Epoch [33/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [33/100], Step [2800/3600], Loss: 0.0562\n",
            "Epoch [34/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [34/100], Step [200/3600], Loss: 0.0037\n",
            "Epoch [34/100], Step [300/3600], Loss: 0.0002\n",
            "Epoch [34/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [34/100], Step [500/3600], Loss: 0.0004\n",
            "Epoch [34/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [34/100], Step [700/3600], Loss: 0.0011\n",
            "Epoch [34/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [34/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [34/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [34/100], Step [1100/3600], Loss: 0.0005\n",
            "Epoch [34/100], Step [1200/3600], Loss: 0.0174\n",
            "Epoch [34/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [34/100], Step [1400/3600], Loss: 0.0016\n",
            "Epoch [34/100], Step [1500/3600], Loss: 0.0006\n",
            "Epoch [34/100], Step [1600/3600], Loss: 0.0001\n",
            "Epoch [34/100], Step [1700/3600], Loss: 0.0103\n",
            "Epoch [34/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [34/100], Step [1900/3600], Loss: 0.0177\n",
            "Epoch [34/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [34/100], Step [2100/3600], Loss: 0.0002\n",
            "Epoch [34/100], Step [2200/3600], Loss: 0.0463\n",
            "Epoch [34/100], Step [2300/3600], Loss: 0.0101\n",
            "Epoch [34/100], Step [2400/3600], Loss: 0.0034\n",
            "Epoch [34/100], Step [2500/3600], Loss: 0.0078\n",
            "Epoch [34/100], Step [2600/3600], Loss: 0.0015\n",
            "Epoch [34/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [34/100], Step [2800/3600], Loss: 0.0127\n",
            "Epoch [35/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [35/100], Step [200/3600], Loss: 0.0057\n",
            "Epoch [35/100], Step [300/3600], Loss: 0.0001\n",
            "Epoch [35/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [35/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [35/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [35/100], Step [700/3600], Loss: 0.0010\n",
            "Epoch [35/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [35/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [35/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [35/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [35/100], Step [1200/3600], Loss: 0.0006\n",
            "Epoch [35/100], Step [1300/3600], Loss: 0.0003\n",
            "Epoch [35/100], Step [1400/3600], Loss: 0.0016\n",
            "Epoch [35/100], Step [1500/3600], Loss: 0.0009\n",
            "Epoch [35/100], Step [1600/3600], Loss: 0.0204\n",
            "Epoch [35/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [35/100], Step [1800/3600], Loss: 0.0005\n",
            "Epoch [35/100], Step [1900/3600], Loss: 0.0783\n",
            "Epoch [35/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [35/100], Step [2100/3600], Loss: 0.0017\n",
            "Epoch [35/100], Step [2200/3600], Loss: 0.0004\n",
            "Epoch [35/100], Step [2300/3600], Loss: 0.0602\n",
            "Epoch [35/100], Step [2400/3600], Loss: 0.0066\n",
            "Epoch [35/100], Step [2500/3600], Loss: 0.0111\n",
            "Epoch [35/100], Step [2600/3600], Loss: 0.0003\n",
            "Epoch [35/100], Step [2700/3600], Loss: 0.0074\n",
            "Epoch [35/100], Step [2800/3600], Loss: 0.0433\n",
            "Epoch [36/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [36/100], Step [200/3600], Loss: 0.0261\n",
            "Epoch [36/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [36/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [36/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [36/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [36/100], Step [700/3600], Loss: 0.0001\n",
            "Epoch [36/100], Step [800/3600], Loss: 0.0006\n",
            "Epoch [36/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [36/100], Step [1000/3600], Loss: 0.0001\n",
            "Epoch [36/100], Step [1100/3600], Loss: 0.0001\n",
            "Epoch [36/100], Step [1200/3600], Loss: 0.0001\n",
            "Epoch [36/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [36/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [36/100], Step [1500/3600], Loss: 0.0005\n",
            "Epoch [36/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [36/100], Step [1700/3600], Loss: 0.0001\n",
            "Epoch [36/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [36/100], Step [1900/3600], Loss: 0.0217\n",
            "Epoch [36/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [36/100], Step [2100/3600], Loss: 0.0036\n",
            "Epoch [36/100], Step [2200/3600], Loss: 0.0077\n",
            "Epoch [36/100], Step [2300/3600], Loss: 0.7190\n",
            "Epoch [36/100], Step [2400/3600], Loss: 0.0025\n",
            "Epoch [36/100], Step [2500/3600], Loss: 0.0264\n",
            "Epoch [36/100], Step [2600/3600], Loss: 0.0014\n",
            "Epoch [36/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [36/100], Step [2800/3600], Loss: 0.0035\n",
            "Epoch [37/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [37/100], Step [200/3600], Loss: 0.0080\n",
            "Epoch [37/100], Step [300/3600], Loss: 0.0002\n",
            "Epoch [37/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [37/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [37/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [37/100], Step [700/3600], Loss: 0.0016\n",
            "Epoch [37/100], Step [800/3600], Loss: 0.0001\n",
            "Epoch [37/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [37/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [37/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [37/100], Step [1200/3600], Loss: 0.0004\n",
            "Epoch [37/100], Step [1300/3600], Loss: 0.0005\n",
            "Epoch [37/100], Step [1400/3600], Loss: 0.0027\n",
            "Epoch [37/100], Step [1500/3600], Loss: 0.0043\n",
            "Epoch [37/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [37/100], Step [1700/3600], Loss: 0.0008\n",
            "Epoch [37/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [37/100], Step [1900/3600], Loss: 0.0076\n",
            "Epoch [37/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [37/100], Step [2100/3600], Loss: 0.0002\n",
            "Epoch [37/100], Step [2200/3600], Loss: 0.0017\n",
            "Epoch [37/100], Step [2300/3600], Loss: 0.0001\n",
            "Epoch [37/100], Step [2400/3600], Loss: 0.0001\n",
            "Epoch [37/100], Step [2500/3600], Loss: 0.0045\n",
            "Epoch [37/100], Step [2600/3600], Loss: 0.0006\n",
            "Epoch [37/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [37/100], Step [2800/3600], Loss: 0.0028\n",
            "Epoch [38/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [38/100], Step [200/3600], Loss: 0.0006\n",
            "Epoch [38/100], Step [300/3600], Loss: 0.0004\n",
            "Epoch [38/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [38/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [38/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [38/100], Step [700/3600], Loss: 0.0039\n",
            "Epoch [38/100], Step [800/3600], Loss: 0.0001\n",
            "Epoch [38/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [38/100], Step [1000/3600], Loss: 0.0001\n",
            "Epoch [38/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [38/100], Step [1200/3600], Loss: 0.0004\n",
            "Epoch [38/100], Step [1300/3600], Loss: 0.0004\n",
            "Epoch [38/100], Step [1400/3600], Loss: 0.0106\n",
            "Epoch [38/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [38/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [38/100], Step [1700/3600], Loss: 0.0232\n",
            "Epoch [38/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [38/100], Step [1900/3600], Loss: 0.0237\n",
            "Epoch [38/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [38/100], Step [2100/3600], Loss: 0.0003\n",
            "Epoch [38/100], Step [2200/3600], Loss: 0.0022\n",
            "Epoch [38/100], Step [2300/3600], Loss: 0.0000\n",
            "Epoch [38/100], Step [2400/3600], Loss: 0.0012\n",
            "Epoch [38/100], Step [2500/3600], Loss: 0.0024\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [38/100], Step [2600/3600], Loss: 0.0015\n",
            "Epoch [38/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [38/100], Step [2800/3600], Loss: 0.0600\n",
            "Epoch [39/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [39/100], Step [200/3600], Loss: 0.0058\n",
            "Epoch [39/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [39/100], Step [400/3600], Loss: 0.0002\n",
            "Epoch [39/100], Step [500/3600], Loss: 0.0001\n",
            "Epoch [39/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [39/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [39/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [39/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [39/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [39/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [39/100], Step [1200/3600], Loss: 0.0005\n",
            "Epoch [39/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [39/100], Step [1400/3600], Loss: 0.0004\n",
            "Epoch [39/100], Step [1500/3600], Loss: 0.0001\n",
            "Epoch [39/100], Step [1600/3600], Loss: 0.0001\n",
            "Epoch [39/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [39/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [39/100], Step [1900/3600], Loss: 0.1564\n",
            "Epoch [39/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [39/100], Step [2100/3600], Loss: 0.0007\n",
            "Epoch [39/100], Step [2200/3600], Loss: 0.0001\n",
            "Epoch [39/100], Step [2300/3600], Loss: 0.0001\n",
            "Epoch [39/100], Step [2400/3600], Loss: 0.0191\n",
            "Epoch [39/100], Step [2500/3600], Loss: 0.0067\n",
            "Epoch [39/100], Step [2600/3600], Loss: 0.0003\n",
            "Epoch [39/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [39/100], Step [2800/3600], Loss: 0.0236\n",
            "Epoch [40/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [40/100], Step [200/3600], Loss: 0.0006\n",
            "Epoch [40/100], Step [300/3600], Loss: 0.0003\n",
            "Epoch [40/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [40/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [40/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [40/100], Step [700/3600], Loss: 0.0005\n",
            "Epoch [40/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [40/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [40/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [40/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [40/100], Step [1200/3600], Loss: 0.0003\n",
            "Epoch [40/100], Step [1300/3600], Loss: 0.0002\n",
            "Epoch [40/100], Step [1400/3600], Loss: 0.0119\n",
            "Epoch [40/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [40/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [40/100], Step [1700/3600], Loss: 0.0916\n",
            "Epoch [40/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [40/100], Step [1900/3600], Loss: 0.0305\n",
            "Epoch [40/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [40/100], Step [2100/3600], Loss: 0.0015\n",
            "Epoch [40/100], Step [2200/3600], Loss: 0.0006\n",
            "Epoch [40/100], Step [2300/3600], Loss: 0.0001\n",
            "Epoch [40/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [40/100], Step [2500/3600], Loss: 0.0040\n",
            "Epoch [40/100], Step [2600/3600], Loss: 0.0203\n",
            "Epoch [40/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [40/100], Step [2800/3600], Loss: 0.0162\n",
            "Epoch [41/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [41/100], Step [200/3600], Loss: 0.0002\n",
            "Epoch [41/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [41/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [41/100], Step [500/3600], Loss: 0.0005\n",
            "Epoch [41/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [41/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [41/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [41/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [41/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [41/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [41/100], Step [1200/3600], Loss: 0.0002\n",
            "Epoch [41/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [41/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [41/100], Step [1500/3600], Loss: 0.0001\n",
            "Epoch [41/100], Step [1600/3600], Loss: 0.0001\n",
            "Epoch [41/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [41/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [41/100], Step [1900/3600], Loss: 0.0035\n",
            "Epoch [41/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [41/100], Step [2100/3600], Loss: 0.0010\n",
            "Epoch [41/100], Step [2200/3600], Loss: 0.0023\n",
            "Epoch [41/100], Step [2300/3600], Loss: 0.0002\n",
            "Epoch [41/100], Step [2400/3600], Loss: 0.0005\n",
            "Epoch [41/100], Step [2500/3600], Loss: 0.0030\n",
            "Epoch [41/100], Step [2600/3600], Loss: 0.0002\n",
            "Epoch [41/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [41/100], Step [2800/3600], Loss: 0.2774\n",
            "Epoch [42/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [42/100], Step [200/3600], Loss: 0.0041\n",
            "Epoch [42/100], Step [300/3600], Loss: 0.0002\n",
            "Epoch [42/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [42/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [42/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [42/100], Step [700/3600], Loss: 0.0002\n",
            "Epoch [42/100], Step [800/3600], Loss: 0.0220\n",
            "Epoch [42/100], Step [900/3600], Loss: 0.0001\n",
            "Epoch [42/100], Step [1000/3600], Loss: 0.0001\n",
            "Epoch [42/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [42/100], Step [1200/3600], Loss: 0.0042\n",
            "Epoch [42/100], Step [1300/3600], Loss: 0.0003\n",
            "Epoch [42/100], Step [1400/3600], Loss: 0.0341\n",
            "Epoch [42/100], Step [1500/3600], Loss: 0.7601\n",
            "Epoch [42/100], Step [1600/3600], Loss: 0.0116\n",
            "Epoch [42/100], Step [1700/3600], Loss: 0.0002\n",
            "Epoch [42/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [42/100], Step [1900/3600], Loss: 0.1666\n",
            "Epoch [42/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [42/100], Step [2100/3600], Loss: 0.0004\n",
            "Epoch [42/100], Step [2200/3600], Loss: 0.0002\n",
            "Epoch [42/100], Step [2300/3600], Loss: 0.0000\n",
            "Epoch [42/100], Step [2400/3600], Loss: 0.0007\n",
            "Epoch [42/100], Step [2500/3600], Loss: 0.0003\n",
            "Epoch [42/100], Step [2600/3600], Loss: 0.0056\n",
            "Epoch [42/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [42/100], Step [2800/3600], Loss: 0.0038\n",
            "Epoch [43/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [43/100], Step [200/3600], Loss: 0.0012\n",
            "Epoch [43/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [43/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [43/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [43/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [43/100], Step [700/3600], Loss: 0.0162\n",
            "Epoch [43/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [43/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [43/100], Step [1000/3600], Loss: 0.0001\n",
            "Epoch [43/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [43/100], Step [1200/3600], Loss: 0.0024\n",
            "Epoch [43/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [43/100], Step [1400/3600], Loss: 0.0004\n",
            "Epoch [43/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [43/100], Step [1600/3600], Loss: 0.0001\n",
            "Epoch [43/100], Step [1700/3600], Loss: 0.0002\n",
            "Epoch [43/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [43/100], Step [1900/3600], Loss: 0.0374\n",
            "Epoch [43/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [43/100], Step [2100/3600], Loss: 0.0006\n",
            "Epoch [43/100], Step [2200/3600], Loss: 0.0000\n",
            "Epoch [43/100], Step [2300/3600], Loss: 0.0004\n",
            "Epoch [43/100], Step [2400/3600], Loss: 0.0001\n",
            "Epoch [43/100], Step [2500/3600], Loss: 0.0000\n",
            "Epoch [43/100], Step [2600/3600], Loss: 0.0002\n",
            "Epoch [43/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [43/100], Step [2800/3600], Loss: 0.0067\n",
            "Epoch [44/100], Step [100/3600], Loss: 0.0008\n",
            "Epoch [44/100], Step [200/3600], Loss: 0.0165\n",
            "Epoch [44/100], Step [300/3600], Loss: 0.0068\n",
            "Epoch [44/100], Step [400/3600], Loss: 0.0001\n",
            "Epoch [44/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [44/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [44/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [44/100], Step [800/3600], Loss: 0.0002\n",
            "Epoch [44/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [44/100], Step [1000/3600], Loss: 0.0111\n",
            "Epoch [44/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [44/100], Step [1200/3600], Loss: 0.0001\n",
            "Epoch [44/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [44/100], Step [1400/3600], Loss: 0.0001\n",
            "Epoch [44/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [44/100], Step [1600/3600], Loss: 0.0001\n",
            "Epoch [44/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [44/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [44/100], Step [1900/3600], Loss: 0.0252\n",
            "Epoch [44/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [44/100], Step [2100/3600], Loss: 0.0028\n",
            "Epoch [44/100], Step [2200/3600], Loss: 0.0008\n",
            "Epoch [44/100], Step [2300/3600], Loss: 0.0036\n",
            "Epoch [44/100], Step [2400/3600], Loss: 0.0003\n",
            "Epoch [44/100], Step [2500/3600], Loss: 0.0010\n",
            "Epoch [44/100], Step [2600/3600], Loss: 0.0006\n",
            "Epoch [44/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [44/100], Step [2800/3600], Loss: 0.0093\n",
            "Epoch [45/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [45/100], Step [200/3600], Loss: 0.0023\n",
            "Epoch [45/100], Step [300/3600], Loss: 0.0047\n",
            "Epoch [45/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [45/100], Step [500/3600], Loss: 0.0000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [45/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [45/100], Step [700/3600], Loss: 0.0005\n",
            "Epoch [45/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [45/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [45/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [45/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [45/100], Step [1200/3600], Loss: 0.0000\n",
            "Epoch [45/100], Step [1300/3600], Loss: 0.0001\n",
            "Epoch [45/100], Step [1400/3600], Loss: 0.0001\n",
            "Epoch [45/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [45/100], Step [1600/3600], Loss: 0.0001\n",
            "Epoch [45/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [45/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [45/100], Step [1900/3600], Loss: 0.0600\n",
            "Epoch [45/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [45/100], Step [2100/3600], Loss: 0.0007\n",
            "Epoch [45/100], Step [2200/3600], Loss: 0.0173\n",
            "Epoch [45/100], Step [2300/3600], Loss: 0.0114\n",
            "Epoch [45/100], Step [2400/3600], Loss: 0.0057\n",
            "Epoch [45/100], Step [2500/3600], Loss: 0.0288\n",
            "Epoch [45/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [45/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [45/100], Step [2800/3600], Loss: 0.0738\n",
            "Epoch [46/100], Step [100/3600], Loss: 0.0001\n",
            "Epoch [46/100], Step [200/3600], Loss: 0.0023\n",
            "Epoch [46/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [46/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [46/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [46/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [46/100], Step [700/3600], Loss: 0.0003\n",
            "Epoch [46/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [46/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [46/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [46/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [46/100], Step [1200/3600], Loss: 0.0003\n",
            "Epoch [46/100], Step [1300/3600], Loss: 0.0040\n",
            "Epoch [46/100], Step [1400/3600], Loss: 0.0153\n",
            "Epoch [46/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [46/100], Step [1600/3600], Loss: 0.0002\n",
            "Epoch [46/100], Step [1700/3600], Loss: 0.0001\n",
            "Epoch [46/100], Step [1800/3600], Loss: 0.0001\n",
            "Epoch [46/100], Step [1900/3600], Loss: 0.0314\n",
            "Epoch [46/100], Step [2000/3600], Loss: 0.0001\n",
            "Epoch [46/100], Step [2100/3600], Loss: 0.0649\n",
            "Epoch [46/100], Step [2200/3600], Loss: 0.0001\n",
            "Epoch [46/100], Step [2300/3600], Loss: 0.0009\n",
            "Epoch [46/100], Step [2400/3600], Loss: 0.0130\n",
            "Epoch [46/100], Step [2500/3600], Loss: 0.0075\n",
            "Epoch [46/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [46/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [46/100], Step [2800/3600], Loss: 0.0222\n",
            "Epoch [47/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [47/100], Step [200/3600], Loss: 0.0250\n",
            "Epoch [47/100], Step [300/3600], Loss: 0.0002\n",
            "Epoch [47/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [47/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [47/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [47/100], Step [700/3600], Loss: 0.0116\n",
            "Epoch [47/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [47/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [47/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [47/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [47/100], Step [1200/3600], Loss: 0.0000\n",
            "Epoch [47/100], Step [1300/3600], Loss: 0.0006\n",
            "Epoch [47/100], Step [1400/3600], Loss: 0.0019\n",
            "Epoch [47/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [47/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [47/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [47/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [47/100], Step [1900/3600], Loss: 0.0692\n",
            "Epoch [47/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [47/100], Step [2100/3600], Loss: 0.0212\n",
            "Epoch [47/100], Step [2200/3600], Loss: 0.0000\n",
            "Epoch [47/100], Step [2300/3600], Loss: 0.0012\n",
            "Epoch [47/100], Step [2400/3600], Loss: 0.0002\n",
            "Epoch [47/100], Step [2500/3600], Loss: 0.0000\n",
            "Epoch [47/100], Step [2600/3600], Loss: 0.0002\n",
            "Epoch [47/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [47/100], Step [2800/3600], Loss: 0.0052\n",
            "Epoch [48/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [48/100], Step [200/3600], Loss: 0.1029\n",
            "Epoch [48/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [48/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [48/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [48/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [48/100], Step [700/3600], Loss: 0.0006\n",
            "Epoch [48/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [48/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [48/100], Step [1000/3600], Loss: 0.0008\n",
            "Epoch [48/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [48/100], Step [1200/3600], Loss: 0.0001\n",
            "Epoch [48/100], Step [1300/3600], Loss: 0.0065\n",
            "Epoch [48/100], Step [1400/3600], Loss: 0.0068\n",
            "Epoch [48/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [48/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [48/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [48/100], Step [1800/3600], Loss: 0.0110\n",
            "Epoch [48/100], Step [1900/3600], Loss: 0.0186\n",
            "Epoch [48/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [48/100], Step [2100/3600], Loss: 0.0001\n",
            "Epoch [48/100], Step [2200/3600], Loss: 0.0005\n",
            "Epoch [48/100], Step [2300/3600], Loss: 0.0078\n",
            "Epoch [48/100], Step [2400/3600], Loss: 0.0034\n",
            "Epoch [48/100], Step [2500/3600], Loss: 0.0012\n",
            "Epoch [48/100], Step [2600/3600], Loss: 0.0006\n",
            "Epoch [48/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [48/100], Step [2800/3600], Loss: 0.0062\n",
            "Epoch [49/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [49/100], Step [200/3600], Loss: 0.0026\n",
            "Epoch [49/100], Step [300/3600], Loss: 0.0003\n",
            "Epoch [49/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [49/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [49/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [49/100], Step [700/3600], Loss: 0.0020\n",
            "Epoch [49/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [49/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [49/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [49/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [49/100], Step [1200/3600], Loss: 0.0000\n",
            "Epoch [49/100], Step [1300/3600], Loss: 0.0007\n",
            "Epoch [49/100], Step [1400/3600], Loss: 0.0017\n",
            "Epoch [49/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [49/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [49/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [49/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [49/100], Step [1900/3600], Loss: 0.0208\n",
            "Epoch [49/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [49/100], Step [2100/3600], Loss: 0.0017\n",
            "Epoch [49/100], Step [2200/3600], Loss: 0.0000\n",
            "Epoch [49/100], Step [2300/3600], Loss: 0.0015\n",
            "Epoch [49/100], Step [2400/3600], Loss: 0.0004\n",
            "Epoch [49/100], Step [2500/3600], Loss: 0.0036\n",
            "Epoch [49/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [49/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [49/100], Step [2800/3600], Loss: 0.0010\n",
            "Epoch [50/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [50/100], Step [200/3600], Loss: 0.0000\n",
            "Epoch [50/100], Step [300/3600], Loss: 0.0004\n",
            "Epoch [50/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [50/100], Step [500/3600], Loss: 0.0002\n",
            "Epoch [50/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [50/100], Step [700/3600], Loss: 0.0004\n",
            "Epoch [50/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [50/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [50/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [50/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [50/100], Step [1200/3600], Loss: 0.0001\n",
            "Epoch [50/100], Step [1300/3600], Loss: 0.0002\n",
            "Epoch [50/100], Step [1400/3600], Loss: 0.0021\n",
            "Epoch [50/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [50/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [50/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [50/100], Step [1800/3600], Loss: 0.0002\n",
            "Epoch [50/100], Step [1900/3600], Loss: 0.0308\n",
            "Epoch [50/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [50/100], Step [2100/3600], Loss: 0.0002\n",
            "Epoch [50/100], Step [2200/3600], Loss: 0.0001\n",
            "Epoch [50/100], Step [2300/3600], Loss: 0.0000\n",
            "Epoch [50/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [50/100], Step [2500/3600], Loss: 0.2713\n",
            "Epoch [50/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [50/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [50/100], Step [2800/3600], Loss: 0.0007\n",
            "Epoch [51/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [51/100], Step [200/3600], Loss: 0.0001\n",
            "Epoch [51/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [51/100], Step [400/3600], Loss: 0.0056\n",
            "Epoch [51/100], Step [500/3600], Loss: 0.0001\n",
            "Epoch [51/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [51/100], Step [700/3600], Loss: 0.0001\n",
            "Epoch [51/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [51/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [51/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [51/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [51/100], Step [1200/3600], Loss: 0.0002\n",
            "Epoch [51/100], Step [1300/3600], Loss: 0.0001\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [51/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [51/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [51/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [51/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [51/100], Step [1800/3600], Loss: 0.0001\n",
            "Epoch [51/100], Step [1900/3600], Loss: 0.2250\n",
            "Epoch [51/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [51/100], Step [2100/3600], Loss: 0.0000\n",
            "Epoch [51/100], Step [2200/3600], Loss: 0.0001\n",
            "Epoch [51/100], Step [2300/3600], Loss: 0.0024\n",
            "Epoch [51/100], Step [2400/3600], Loss: 0.0015\n",
            "Epoch [51/100], Step [2500/3600], Loss: 0.0052\n",
            "Epoch [51/100], Step [2600/3600], Loss: 0.0010\n",
            "Epoch [51/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [51/100], Step [2800/3600], Loss: 0.0361\n",
            "Epoch [52/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [52/100], Step [200/3600], Loss: 0.0006\n",
            "Epoch [52/100], Step [300/3600], Loss: 0.0002\n",
            "Epoch [52/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [52/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [52/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [52/100], Step [700/3600], Loss: 0.0046\n",
            "Epoch [52/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [52/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [52/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [52/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [52/100], Step [1200/3600], Loss: 0.0000\n",
            "Epoch [52/100], Step [1300/3600], Loss: 0.0115\n",
            "Epoch [52/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [52/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [52/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [52/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [52/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [52/100], Step [1900/3600], Loss: 0.0366\n",
            "Epoch [52/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [52/100], Step [2100/3600], Loss: 0.0000\n",
            "Epoch [52/100], Step [2200/3600], Loss: 0.0030\n",
            "Epoch [52/100], Step [2300/3600], Loss: 0.0035\n",
            "Epoch [52/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [52/100], Step [2500/3600], Loss: 0.0124\n",
            "Epoch [52/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [52/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [52/100], Step [2800/3600], Loss: 0.1438\n",
            "Epoch [53/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [53/100], Step [200/3600], Loss: 0.0059\n",
            "Epoch [53/100], Step [300/3600], Loss: 0.0024\n",
            "Epoch [53/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [53/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [53/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [53/100], Step [700/3600], Loss: 0.0168\n",
            "Epoch [53/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [53/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [53/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [53/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [53/100], Step [1200/3600], Loss: 0.0031\n",
            "Epoch [53/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [53/100], Step [1400/3600], Loss: 0.0267\n",
            "Epoch [53/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [53/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [53/100], Step [1700/3600], Loss: 0.0004\n",
            "Epoch [53/100], Step [1800/3600], Loss: 0.0002\n",
            "Epoch [53/100], Step [1900/3600], Loss: 0.0216\n",
            "Epoch [53/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [53/100], Step [2100/3600], Loss: 0.0239\n",
            "Epoch [53/100], Step [2200/3600], Loss: 0.0000\n",
            "Epoch [53/100], Step [2300/3600], Loss: 0.0413\n",
            "Epoch [53/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [53/100], Step [2500/3600], Loss: 0.0063\n",
            "Epoch [53/100], Step [2600/3600], Loss: 0.2930\n",
            "Epoch [53/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [53/100], Step [2800/3600], Loss: 0.0511\n",
            "Epoch [54/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [54/100], Step [200/3600], Loss: 0.0000\n",
            "Epoch [54/100], Step [300/3600], Loss: 0.0002\n",
            "Epoch [54/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [54/100], Step [500/3600], Loss: 0.0001\n",
            "Epoch [54/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [54/100], Step [700/3600], Loss: 0.0008\n",
            "Epoch [54/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [54/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [54/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [54/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [54/100], Step [1200/3600], Loss: 0.0001\n",
            "Epoch [54/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [54/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [54/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [54/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [54/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [54/100], Step [1800/3600], Loss: 0.0107\n",
            "Epoch [54/100], Step [1900/3600], Loss: 0.0239\n",
            "Epoch [54/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [54/100], Step [2100/3600], Loss: 0.0053\n",
            "Epoch [54/100], Step [2200/3600], Loss: 0.0031\n",
            "Epoch [54/100], Step [2300/3600], Loss: 0.0003\n",
            "Epoch [54/100], Step [2400/3600], Loss: 0.0002\n",
            "Epoch [54/100], Step [2500/3600], Loss: 0.0002\n",
            "Epoch [54/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [54/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [54/100], Step [2800/3600], Loss: 0.0030\n",
            "Epoch [55/100], Step [100/3600], Loss: 0.0001\n",
            "Epoch [55/100], Step [200/3600], Loss: 0.0008\n",
            "Epoch [55/100], Step [300/3600], Loss: 0.1334\n",
            "Epoch [55/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [55/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [55/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [55/100], Step [700/3600], Loss: 0.0107\n",
            "Epoch [55/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [55/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [55/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [55/100], Step [1100/3600], Loss: 0.0004\n",
            "Epoch [55/100], Step [1200/3600], Loss: 0.0001\n",
            "Epoch [55/100], Step [1300/3600], Loss: 0.0002\n",
            "Epoch [55/100], Step [1400/3600], Loss: 0.0002\n",
            "Epoch [55/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [55/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [55/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [55/100], Step [1800/3600], Loss: 0.0012\n",
            "Epoch [55/100], Step [1900/3600], Loss: 0.1447\n",
            "Epoch [55/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [55/100], Step [2100/3600], Loss: 0.0001\n",
            "Epoch [55/100], Step [2200/3600], Loss: 0.0001\n",
            "Epoch [55/100], Step [2300/3600], Loss: 0.0005\n",
            "Epoch [55/100], Step [2400/3600], Loss: 0.0185\n",
            "Epoch [55/100], Step [2500/3600], Loss: 0.0001\n",
            "Epoch [55/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [55/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [55/100], Step [2800/3600], Loss: 0.0134\n",
            "Epoch [56/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [56/100], Step [200/3600], Loss: 0.0002\n",
            "Epoch [56/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [56/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [56/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [56/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [56/100], Step [700/3600], Loss: 0.0180\n",
            "Epoch [56/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [56/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [56/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [56/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [56/100], Step [1200/3600], Loss: 0.0002\n",
            "Epoch [56/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [56/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [56/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [56/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [56/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [56/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [56/100], Step [1900/3600], Loss: 0.1635\n",
            "Epoch [56/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [56/100], Step [2100/3600], Loss: 0.0015\n",
            "Epoch [56/100], Step [2200/3600], Loss: 0.0002\n",
            "Epoch [56/100], Step [2300/3600], Loss: 0.0052\n",
            "Epoch [56/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [56/100], Step [2500/3600], Loss: 0.0035\n",
            "Epoch [56/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [56/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [56/100], Step [2800/3600], Loss: 0.0009\n",
            "Epoch [57/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [57/100], Step [200/3600], Loss: 0.0029\n",
            "Epoch [57/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [57/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [57/100], Step [500/3600], Loss: 0.0225\n",
            "Epoch [57/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [57/100], Step [700/3600], Loss: 1.8858\n",
            "Epoch [57/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [57/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [57/100], Step [1000/3600], Loss: 0.0010\n",
            "Epoch [57/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [57/100], Step [1200/3600], Loss: 0.0000\n",
            "Epoch [57/100], Step [1300/3600], Loss: 0.0001\n",
            "Epoch [57/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [57/100], Step [1500/3600], Loss: 0.0001\n",
            "Epoch [57/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [57/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [57/100], Step [1800/3600], Loss: 0.0013\n",
            "Epoch [57/100], Step [1900/3600], Loss: 0.2183\n",
            "Epoch [57/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [57/100], Step [2100/3600], Loss: 0.0001\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [57/100], Step [2200/3600], Loss: 0.0001\n",
            "Epoch [57/100], Step [2300/3600], Loss: 0.0008\n",
            "Epoch [57/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [57/100], Step [2500/3600], Loss: 0.0000\n",
            "Epoch [57/100], Step [2600/3600], Loss: 0.0005\n",
            "Epoch [57/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [57/100], Step [2800/3600], Loss: 0.0534\n",
            "Epoch [58/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [58/100], Step [200/3600], Loss: 0.0001\n",
            "Epoch [58/100], Step [300/3600], Loss: 0.0001\n",
            "Epoch [58/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [58/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [58/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [58/100], Step [700/3600], Loss: 0.0003\n",
            "Epoch [58/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [58/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [58/100], Step [1000/3600], Loss: 0.0002\n",
            "Epoch [58/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [58/100], Step [1200/3600], Loss: 0.0017\n",
            "Epoch [58/100], Step [1300/3600], Loss: 0.0039\n",
            "Epoch [58/100], Step [1400/3600], Loss: 0.0002\n",
            "Epoch [58/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [58/100], Step [1600/3600], Loss: 0.0022\n",
            "Epoch [58/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [58/100], Step [1800/3600], Loss: 0.0149\n",
            "Epoch [58/100], Step [1900/3600], Loss: 0.2031\n",
            "Epoch [58/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [58/100], Step [2100/3600], Loss: 0.0007\n",
            "Epoch [58/100], Step [2200/3600], Loss: 0.0001\n",
            "Epoch [58/100], Step [2300/3600], Loss: 0.0001\n",
            "Epoch [58/100], Step [2400/3600], Loss: 0.0019\n",
            "Epoch [58/100], Step [2500/3600], Loss: 0.0000\n",
            "Epoch [58/100], Step [2600/3600], Loss: 0.0004\n",
            "Epoch [58/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [58/100], Step [2800/3600], Loss: 0.0011\n",
            "Epoch [59/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [59/100], Step [200/3600], Loss: 0.0004\n",
            "Epoch [59/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [59/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [59/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [59/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [59/100], Step [700/3600], Loss: 0.0003\n",
            "Epoch [59/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [59/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [59/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [59/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [59/100], Step [1200/3600], Loss: 0.0078\n",
            "Epoch [59/100], Step [1300/3600], Loss: 0.0006\n",
            "Epoch [59/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [59/100], Step [1500/3600], Loss: 0.0002\n",
            "Epoch [59/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [59/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [59/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [59/100], Step [1900/3600], Loss: 0.1774\n",
            "Epoch [59/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [59/100], Step [2100/3600], Loss: 0.0045\n",
            "Epoch [59/100], Step [2200/3600], Loss: 0.0004\n",
            "Epoch [59/100], Step [2300/3600], Loss: 0.0009\n",
            "Epoch [59/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [59/100], Step [2500/3600], Loss: 0.0015\n",
            "Epoch [59/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [59/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [59/100], Step [2800/3600], Loss: 0.2876\n",
            "Epoch [60/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [60/100], Step [200/3600], Loss: 0.0006\n",
            "Epoch [60/100], Step [300/3600], Loss: 0.0008\n",
            "Epoch [60/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [60/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [60/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [60/100], Step [700/3600], Loss: 0.0002\n",
            "Epoch [60/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [60/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [60/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [60/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [60/100], Step [1200/3600], Loss: 0.0050\n",
            "Epoch [60/100], Step [1300/3600], Loss: 0.0006\n",
            "Epoch [60/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [60/100], Step [1500/3600], Loss: 0.0007\n",
            "Epoch [60/100], Step [1600/3600], Loss: 0.0027\n",
            "Epoch [60/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [60/100], Step [1800/3600], Loss: 0.0001\n",
            "Epoch [60/100], Step [1900/3600], Loss: 0.0227\n",
            "Epoch [60/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [60/100], Step [2100/3600], Loss: 0.0004\n",
            "Epoch [60/100], Step [2200/3600], Loss: 0.0000\n",
            "Epoch [60/100], Step [2300/3600], Loss: 0.0008\n",
            "Epoch [60/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [60/100], Step [2500/3600], Loss: 0.0016\n",
            "Epoch [60/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [60/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [60/100], Step [2800/3600], Loss: 0.0073\n",
            "Epoch [61/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [61/100], Step [200/3600], Loss: 0.0001\n",
            "Epoch [61/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [61/100], Step [400/3600], Loss: 0.0011\n",
            "Epoch [61/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [61/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [61/100], Step [700/3600], Loss: 0.0002\n",
            "Epoch [61/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [61/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [61/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [61/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [61/100], Step [1200/3600], Loss: 0.0021\n",
            "Epoch [61/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [61/100], Step [1400/3600], Loss: 0.0001\n",
            "Epoch [61/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [61/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [61/100], Step [1700/3600], Loss: 0.0001\n",
            "Epoch [61/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [61/100], Step [1900/3600], Loss: 0.2191\n",
            "Epoch [61/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [61/100], Step [2100/3600], Loss: 0.0185\n",
            "Epoch [61/100], Step [2200/3600], Loss: 0.0000\n",
            "Epoch [61/100], Step [2300/3600], Loss: 0.0012\n",
            "Epoch [61/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [61/100], Step [2500/3600], Loss: 0.0005\n",
            "Epoch [61/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [61/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [61/100], Step [2800/3600], Loss: 0.1801\n",
            "Epoch [62/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [62/100], Step [200/3600], Loss: 0.0005\n",
            "Epoch [62/100], Step [300/3600], Loss: 0.0122\n",
            "Epoch [62/100], Step [400/3600], Loss: 0.0002\n",
            "Epoch [62/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [62/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [62/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [62/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [62/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [62/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [62/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [62/100], Step [1200/3600], Loss: 0.0000\n",
            "Epoch [62/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [62/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [62/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [62/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [62/100], Step [1700/3600], Loss: 0.0002\n",
            "Epoch [62/100], Step [1800/3600], Loss: 0.0001\n",
            "Epoch [62/100], Step [1900/3600], Loss: 0.1851\n",
            "Epoch [62/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [62/100], Step [2100/3600], Loss: 0.0016\n",
            "Epoch [62/100], Step [2200/3600], Loss: 0.0004\n",
            "Epoch [62/100], Step [2300/3600], Loss: 0.0006\n",
            "Epoch [62/100], Step [2400/3600], Loss: 0.0001\n",
            "Epoch [62/100], Step [2500/3600], Loss: 0.0001\n",
            "Epoch [62/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [62/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [62/100], Step [2800/3600], Loss: 0.0002\n",
            "Epoch [63/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [63/100], Step [200/3600], Loss: 0.0153\n",
            "Epoch [63/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [63/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [63/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [63/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [63/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [63/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [63/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [63/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [63/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [63/100], Step [1200/3600], Loss: 0.0003\n",
            "Epoch [63/100], Step [1300/3600], Loss: 0.0001\n",
            "Epoch [63/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [63/100], Step [1500/3600], Loss: 0.0003\n",
            "Epoch [63/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [63/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [63/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [63/100], Step [1900/3600], Loss: 0.1725\n",
            "Epoch [63/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [63/100], Step [2100/3600], Loss: 0.0038\n",
            "Epoch [63/100], Step [2200/3600], Loss: 0.0001\n",
            "Epoch [63/100], Step [2300/3600], Loss: 0.0002\n",
            "Epoch [63/100], Step [2400/3600], Loss: 0.0001\n",
            "Epoch [63/100], Step [2500/3600], Loss: 0.0001\n",
            "Epoch [63/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [63/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [63/100], Step [2800/3600], Loss: 0.0010\n",
            "Epoch [64/100], Step [100/3600], Loss: 0.0000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [64/100], Step [200/3600], Loss: 0.0001\n",
            "Epoch [64/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [64/100], Step [400/3600], Loss: 0.0014\n",
            "Epoch [64/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [64/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [64/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [64/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [64/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [64/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [64/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [64/100], Step [1200/3600], Loss: 0.0000\n",
            "Epoch [64/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [64/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [64/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [64/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [64/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [64/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [64/100], Step [1900/3600], Loss: 0.1722\n",
            "Epoch [64/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [64/100], Step [2100/3600], Loss: 0.0001\n",
            "Epoch [64/100], Step [2200/3600], Loss: 0.0001\n",
            "Epoch [64/100], Step [2300/3600], Loss: 0.0112\n",
            "Epoch [64/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [64/100], Step [2500/3600], Loss: 0.0006\n",
            "Epoch [64/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [64/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [64/100], Step [2800/3600], Loss: 0.0016\n",
            "Epoch [65/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [65/100], Step [200/3600], Loss: 0.0001\n",
            "Epoch [65/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [65/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [65/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [65/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [65/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [65/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [65/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [65/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [65/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [65/100], Step [1200/3600], Loss: 0.0001\n",
            "Epoch [65/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [65/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [65/100], Step [1500/3600], Loss: 0.0014\n",
            "Epoch [65/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [65/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [65/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [65/100], Step [1900/3600], Loss: 0.1838\n",
            "Epoch [65/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [65/100], Step [2100/3600], Loss: 0.0000\n",
            "Epoch [65/100], Step [2200/3600], Loss: 0.0001\n",
            "Epoch [65/100], Step [2300/3600], Loss: 0.0001\n",
            "Epoch [65/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [65/100], Step [2500/3600], Loss: 0.0000\n",
            "Epoch [65/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [65/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [65/100], Step [2800/3600], Loss: 0.0042\n",
            "Epoch [66/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [66/100], Step [200/3600], Loss: 0.0002\n",
            "Epoch [66/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [66/100], Step [400/3600], Loss: 0.0004\n",
            "Epoch [66/100], Step [500/3600], Loss: 0.0001\n",
            "Epoch [66/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [66/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [66/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [66/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [66/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [66/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [66/100], Step [1200/3600], Loss: 0.0004\n",
            "Epoch [66/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [66/100], Step [1400/3600], Loss: 0.7541\n",
            "Epoch [66/100], Step [1500/3600], Loss: 0.0006\n",
            "Epoch [66/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [66/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [66/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [66/100], Step [1900/3600], Loss: 0.0488\n",
            "Epoch [66/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [66/100], Step [2100/3600], Loss: 0.0000\n",
            "Epoch [66/100], Step [2200/3600], Loss: 0.0000\n",
            "Epoch [66/100], Step [2300/3600], Loss: 0.0004\n",
            "Epoch [66/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [66/100], Step [2500/3600], Loss: 0.0007\n",
            "Epoch [66/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [66/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [66/100], Step [2800/3600], Loss: 0.0002\n",
            "Epoch [67/100], Step [100/3600], Loss: 0.0001\n",
            "Epoch [67/100], Step [200/3600], Loss: 0.0001\n",
            "Epoch [67/100], Step [300/3600], Loss: 0.0005\n",
            "Epoch [67/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [67/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [67/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [67/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [67/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [67/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [67/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [67/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [67/100], Step [1200/3600], Loss: 0.0001\n",
            "Epoch [67/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [67/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [67/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [67/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [67/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [67/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [67/100], Step [1900/3600], Loss: 0.1611\n",
            "Epoch [67/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [67/100], Step [2100/3600], Loss: 0.0000\n",
            "Epoch [67/100], Step [2200/3600], Loss: 0.0001\n",
            "Epoch [67/100], Step [2300/3600], Loss: 0.0025\n",
            "Epoch [67/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [67/100], Step [2500/3600], Loss: 0.0008\n",
            "Epoch [67/100], Step [2600/3600], Loss: 0.0101\n",
            "Epoch [67/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [67/100], Step [2800/3600], Loss: 0.0611\n",
            "Epoch [68/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [68/100], Step [200/3600], Loss: 0.0070\n",
            "Epoch [68/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [68/100], Step [400/3600], Loss: 0.0001\n",
            "Epoch [68/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [68/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [68/100], Step [700/3600], Loss: 0.0002\n",
            "Epoch [68/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [68/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [68/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [68/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [68/100], Step [1200/3600], Loss: 0.0011\n",
            "Epoch [68/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [68/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [68/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [68/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [68/100], Step [1700/3600], Loss: 0.0001\n",
            "Epoch [68/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [68/100], Step [1900/3600], Loss: 0.1298\n",
            "Epoch [68/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [68/100], Step [2100/3600], Loss: 0.0000\n",
            "Epoch [68/100], Step [2200/3600], Loss: 0.0002\n",
            "Epoch [68/100], Step [2300/3600], Loss: 0.0009\n",
            "Epoch [68/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [68/100], Step [2500/3600], Loss: 0.0085\n",
            "Epoch [68/100], Step [2600/3600], Loss: 0.0001\n",
            "Epoch [68/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [68/100], Step [2800/3600], Loss: 0.0002\n",
            "Epoch [69/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [69/100], Step [200/3600], Loss: 0.0220\n",
            "Epoch [69/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [69/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [69/100], Step [500/3600], Loss: 0.0006\n",
            "Epoch [69/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [69/100], Step [700/3600], Loss: 0.0002\n",
            "Epoch [69/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [69/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [69/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [69/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [69/100], Step [1200/3600], Loss: 0.0020\n",
            "Epoch [69/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [69/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [69/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [69/100], Step [1600/3600], Loss: 0.0007\n",
            "Epoch [69/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [69/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [69/100], Step [1900/3600], Loss: 0.1591\n",
            "Epoch [69/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [69/100], Step [2100/3600], Loss: 0.0001\n",
            "Epoch [69/100], Step [2200/3600], Loss: 0.0004\n",
            "Epoch [69/100], Step [2300/3600], Loss: 0.0001\n",
            "Epoch [69/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [69/100], Step [2500/3600], Loss: 0.0003\n",
            "Epoch [69/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [69/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [69/100], Step [2800/3600], Loss: 0.0049\n",
            "Epoch [70/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [70/100], Step [200/3600], Loss: 0.0202\n",
            "Epoch [70/100], Step [300/3600], Loss: 0.0001\n",
            "Epoch [70/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [70/100], Step [500/3600], Loss: 0.0004\n",
            "Epoch [70/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [70/100], Step [700/3600], Loss: 0.0006\n",
            "Epoch [70/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [70/100], Step [900/3600], Loss: 0.0000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [70/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [70/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [70/100], Step [1200/3600], Loss: 0.0008\n",
            "Epoch [70/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [70/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [70/100], Step [1500/3600], Loss: 0.0007\n",
            "Epoch [70/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [70/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [70/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [70/100], Step [1900/3600], Loss: 0.1643\n",
            "Epoch [70/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [70/100], Step [2100/3600], Loss: 0.0092\n",
            "Epoch [70/100], Step [2200/3600], Loss: 0.0018\n",
            "Epoch [70/100], Step [2300/3600], Loss: 0.0044\n",
            "Epoch [70/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [70/100], Step [2500/3600], Loss: 0.0004\n",
            "Epoch [70/100], Step [2600/3600], Loss: 0.0057\n",
            "Epoch [70/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [70/100], Step [2800/3600], Loss: 0.0405\n",
            "Epoch [71/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [71/100], Step [200/3600], Loss: 0.0001\n",
            "Epoch [71/100], Step [300/3600], Loss: 0.0002\n",
            "Epoch [71/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [71/100], Step [500/3600], Loss: 0.0006\n",
            "Epoch [71/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [71/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [71/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [71/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [71/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [71/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [71/100], Step [1200/3600], Loss: 0.0004\n",
            "Epoch [71/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [71/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [71/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [71/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [71/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [71/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [71/100], Step [1900/3600], Loss: 0.1484\n",
            "Epoch [71/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [71/100], Step [2100/3600], Loss: 0.0000\n",
            "Epoch [71/100], Step [2200/3600], Loss: 0.0000\n",
            "Epoch [71/100], Step [2300/3600], Loss: 0.0009\n",
            "Epoch [71/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [71/100], Step [2500/3600], Loss: 0.0006\n",
            "Epoch [71/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [71/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [71/100], Step [2800/3600], Loss: 0.0007\n",
            "Epoch [72/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [72/100], Step [200/3600], Loss: 0.0567\n",
            "Epoch [72/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [72/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [72/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [72/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [72/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [72/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [72/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [72/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [72/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [72/100], Step [1200/3600], Loss: 0.0001\n",
            "Epoch [72/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [72/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [72/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [72/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [72/100], Step [1700/3600], Loss: 0.0009\n",
            "Epoch [72/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [72/100], Step [1900/3600], Loss: 0.0218\n",
            "Epoch [72/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [72/100], Step [2100/3600], Loss: 0.0000\n",
            "Epoch [72/100], Step [2200/3600], Loss: 0.0000\n",
            "Epoch [72/100], Step [2300/3600], Loss: 0.0000\n",
            "Epoch [72/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [72/100], Step [2500/3600], Loss: 0.0020\n",
            "Epoch [72/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [72/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [72/100], Step [2800/3600], Loss: 0.0105\n",
            "Epoch [73/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [73/100], Step [200/3600], Loss: 0.0000\n",
            "Epoch [73/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [73/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [73/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [73/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [73/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [73/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [73/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [73/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [73/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [73/100], Step [1200/3600], Loss: 0.0002\n",
            "Epoch [73/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [73/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [73/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [73/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [73/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [73/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [73/100], Step [1900/3600], Loss: 0.1317\n",
            "Epoch [73/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [73/100], Step [2100/3600], Loss: 0.0000\n",
            "Epoch [73/100], Step [2200/3600], Loss: 0.0001\n",
            "Epoch [73/100], Step [2300/3600], Loss: 0.0000\n",
            "Epoch [73/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [73/100], Step [2500/3600], Loss: 0.0006\n",
            "Epoch [73/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [73/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [73/100], Step [2800/3600], Loss: 0.0005\n",
            "Epoch [74/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [74/100], Step [200/3600], Loss: 0.0000\n",
            "Epoch [74/100], Step [300/3600], Loss: 0.0001\n",
            "Epoch [74/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [74/100], Step [500/3600], Loss: 0.0001\n",
            "Epoch [74/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [74/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [74/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [74/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [74/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [74/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [74/100], Step [1200/3600], Loss: 0.0011\n",
            "Epoch [74/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [74/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [74/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [74/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [74/100], Step [1700/3600], Loss: 0.0002\n",
            "Epoch [74/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [74/100], Step [1900/3600], Loss: 0.1630\n",
            "Epoch [74/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [74/100], Step [2100/3600], Loss: 0.0001\n",
            "Epoch [74/100], Step [2200/3600], Loss: 0.0000\n",
            "Epoch [74/100], Step [2300/3600], Loss: 0.0000\n",
            "Epoch [74/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [74/100], Step [2500/3600], Loss: 0.0012\n",
            "Epoch [74/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [74/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [74/100], Step [2800/3600], Loss: 0.0149\n",
            "Epoch [75/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [75/100], Step [200/3600], Loss: 0.0000\n",
            "Epoch [75/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [75/100], Step [400/3600], Loss: 0.0002\n",
            "Epoch [75/100], Step [500/3600], Loss: 0.0094\n",
            "Epoch [75/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [75/100], Step [700/3600], Loss: 0.0001\n",
            "Epoch [75/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [75/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [75/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [75/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [75/100], Step [1200/3600], Loss: 0.0056\n",
            "Epoch [75/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [75/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [75/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [75/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [75/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [75/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [75/100], Step [1900/3600], Loss: 0.2517\n",
            "Epoch [75/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [75/100], Step [2100/3600], Loss: 0.0000\n",
            "Epoch [75/100], Step [2200/3600], Loss: 0.0104\n",
            "Epoch [75/100], Step [2300/3600], Loss: 0.0164\n",
            "Epoch [75/100], Step [2400/3600], Loss: 0.0001\n",
            "Epoch [75/100], Step [2500/3600], Loss: 0.0141\n",
            "Epoch [75/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [75/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [75/100], Step [2800/3600], Loss: 0.0032\n",
            "Epoch [76/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [76/100], Step [200/3600], Loss: 0.0004\n",
            "Epoch [76/100], Step [300/3600], Loss: 0.0002\n",
            "Epoch [76/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [76/100], Step [500/3600], Loss: 0.0001\n",
            "Epoch [76/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [76/100], Step [700/3600], Loss: 0.0001\n",
            "Epoch [76/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [76/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [76/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [76/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [76/100], Step [1200/3600], Loss: 0.0010\n",
            "Epoch [76/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [76/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [76/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [76/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [76/100], Step [1700/3600], Loss: 0.0010\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [76/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [76/100], Step [1900/3600], Loss: 0.2240\n",
            "Epoch [76/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [76/100], Step [2100/3600], Loss: 0.0000\n",
            "Epoch [76/100], Step [2200/3600], Loss: 0.0000\n",
            "Epoch [76/100], Step [2300/3600], Loss: 0.0035\n",
            "Epoch [76/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [76/100], Step [2500/3600], Loss: 0.0006\n",
            "Epoch [76/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [76/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [76/100], Step [2800/3600], Loss: 0.0004\n",
            "Epoch [77/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [77/100], Step [200/3600], Loss: 0.0002\n",
            "Epoch [77/100], Step [300/3600], Loss: 0.0002\n",
            "Epoch [77/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [77/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [77/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [77/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [77/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [77/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [77/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [77/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [77/100], Step [1200/3600], Loss: 0.0000\n",
            "Epoch [77/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [77/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [77/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [77/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [77/100], Step [1700/3600], Loss: 0.0001\n",
            "Epoch [77/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [77/100], Step [1900/3600], Loss: 0.2116\n",
            "Epoch [77/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [77/100], Step [2100/3600], Loss: 0.0000\n",
            "Epoch [77/100], Step [2200/3600], Loss: 0.0000\n",
            "Epoch [77/100], Step [2300/3600], Loss: 0.0005\n",
            "Epoch [77/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [77/100], Step [2500/3600], Loss: 0.0005\n",
            "Epoch [77/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [77/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [77/100], Step [2800/3600], Loss: 0.0014\n",
            "Epoch [78/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [78/100], Step [200/3600], Loss: 0.0001\n",
            "Epoch [78/100], Step [300/3600], Loss: 0.0011\n",
            "Epoch [78/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [78/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [78/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [78/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [78/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [78/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [78/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [78/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [78/100], Step [1200/3600], Loss: 0.0000\n",
            "Epoch [78/100], Step [1300/3600], Loss: 0.0001\n",
            "Epoch [78/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [78/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [78/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [78/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [78/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [78/100], Step [1900/3600], Loss: 0.0389\n",
            "Epoch [78/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [78/100], Step [2100/3600], Loss: 0.0000\n",
            "Epoch [78/100], Step [2200/3600], Loss: 0.0001\n",
            "Epoch [78/100], Step [2300/3600], Loss: 0.0000\n",
            "Epoch [78/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [78/100], Step [2500/3600], Loss: 0.0005\n",
            "Epoch [78/100], Step [2600/3600], Loss: 0.0003\n",
            "Epoch [78/100], Step [2700/3600], Loss: 0.0002\n",
            "Epoch [78/100], Step [2800/3600], Loss: 0.0001\n",
            "Epoch [79/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [79/100], Step [200/3600], Loss: 0.0000\n",
            "Epoch [79/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [79/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [79/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [79/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [79/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [79/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [79/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [79/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [79/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [79/100], Step [1200/3600], Loss: 0.0005\n",
            "Epoch [79/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [79/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [79/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [79/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [79/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [79/100], Step [1800/3600], Loss: 0.0001\n",
            "Epoch [79/100], Step [1900/3600], Loss: 0.0730\n",
            "Epoch [79/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [79/100], Step [2100/3600], Loss: 0.0000\n",
            "Epoch [79/100], Step [2200/3600], Loss: 0.0564\n",
            "Epoch [79/100], Step [2300/3600], Loss: 0.0001\n",
            "Epoch [79/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [79/100], Step [2500/3600], Loss: 0.0014\n",
            "Epoch [79/100], Step [2600/3600], Loss: 0.0001\n",
            "Epoch [79/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [79/100], Step [2800/3600], Loss: 0.0012\n",
            "Epoch [80/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [80/100], Step [200/3600], Loss: 0.0002\n",
            "Epoch [80/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [80/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [80/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [80/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [80/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [80/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [80/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [80/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [80/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [80/100], Step [1200/3600], Loss: 0.0019\n",
            "Epoch [80/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [80/100], Step [1400/3600], Loss: 0.0009\n",
            "Epoch [80/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [80/100], Step [1600/3600], Loss: 0.0001\n",
            "Epoch [80/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [80/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [80/100], Step [1900/3600], Loss: 0.0159\n",
            "Epoch [80/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [80/100], Step [2100/3600], Loss: 0.0000\n",
            "Epoch [80/100], Step [2200/3600], Loss: 0.0000\n",
            "Epoch [80/100], Step [2300/3600], Loss: 0.0174\n",
            "Epoch [80/100], Step [2400/3600], Loss: 0.0001\n",
            "Epoch [80/100], Step [2500/3600], Loss: 0.0012\n",
            "Epoch [80/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [80/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [80/100], Step [2800/3600], Loss: 0.0091\n",
            "Epoch [81/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [81/100], Step [200/3600], Loss: 0.0013\n",
            "Epoch [81/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [81/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [81/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [81/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [81/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [81/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [81/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [81/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [81/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [81/100], Step [1200/3600], Loss: 0.0000\n",
            "Epoch [81/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [81/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [81/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [81/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [81/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [81/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [81/100], Step [1900/3600], Loss: 0.2105\n",
            "Epoch [81/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [81/100], Step [2100/3600], Loss: 0.0000\n",
            "Epoch [81/100], Step [2200/3600], Loss: 0.0000\n",
            "Epoch [81/100], Step [2300/3600], Loss: 0.0090\n",
            "Epoch [81/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [81/100], Step [2500/3600], Loss: 0.0001\n",
            "Epoch [81/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [81/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [81/100], Step [2800/3600], Loss: 0.0005\n",
            "Epoch [82/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [82/100], Step [200/3600], Loss: 0.0001\n",
            "Epoch [82/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [82/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [82/100], Step [500/3600], Loss: 0.0002\n",
            "Epoch [82/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [82/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [82/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [82/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [82/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [82/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [82/100], Step [1200/3600], Loss: 0.0002\n",
            "Epoch [82/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [82/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [82/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [82/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [82/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [82/100], Step [1800/3600], Loss: 0.0001\n",
            "Epoch [82/100], Step [1900/3600], Loss: 0.0093\n",
            "Epoch [82/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [82/100], Step [2100/3600], Loss: 0.0039\n",
            "Epoch [82/100], Step [2200/3600], Loss: 0.0001\n",
            "Epoch [82/100], Step [2300/3600], Loss: 0.0639\n",
            "Epoch [82/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [82/100], Step [2500/3600], Loss: 0.0069\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [82/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [82/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [82/100], Step [2800/3600], Loss: 0.0000\n",
            "Epoch [83/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [83/100], Step [200/3600], Loss: 0.0004\n",
            "Epoch [83/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [83/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [83/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [83/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [83/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [83/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [83/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [83/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [83/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [83/100], Step [1200/3600], Loss: 0.0000\n",
            "Epoch [83/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [83/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [83/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [83/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [83/100], Step [1700/3600], Loss: 0.0001\n",
            "Epoch [83/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [83/100], Step [1900/3600], Loss: 0.0638\n",
            "Epoch [83/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [83/100], Step [2100/3600], Loss: 0.0000\n",
            "Epoch [83/100], Step [2200/3600], Loss: 0.0002\n",
            "Epoch [83/100], Step [2300/3600], Loss: 0.0000\n",
            "Epoch [83/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [83/100], Step [2500/3600], Loss: 0.0022\n",
            "Epoch [83/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [83/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [83/100], Step [2800/3600], Loss: 0.0001\n",
            "Epoch [84/100], Step [100/3600], Loss: 0.0001\n",
            "Epoch [84/100], Step [200/3600], Loss: 0.0005\n",
            "Epoch [84/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [84/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [84/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [84/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [84/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [84/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [84/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [84/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [84/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [84/100], Step [1200/3600], Loss: 0.0003\n",
            "Epoch [84/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [84/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [84/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [84/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [84/100], Step [1700/3600], Loss: 0.0001\n",
            "Epoch [84/100], Step [1800/3600], Loss: 0.0559\n",
            "Epoch [84/100], Step [1900/3600], Loss: 0.1762\n",
            "Epoch [84/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [84/100], Step [2100/3600], Loss: 0.0000\n",
            "Epoch [84/100], Step [2200/3600], Loss: 0.0004\n",
            "Epoch [84/100], Step [2300/3600], Loss: 0.0001\n",
            "Epoch [84/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [84/100], Step [2500/3600], Loss: 0.0003\n",
            "Epoch [84/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [84/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [84/100], Step [2800/3600], Loss: 0.0003\n",
            "Epoch [85/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [85/100], Step [200/3600], Loss: 0.0000\n",
            "Epoch [85/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [85/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [85/100], Step [500/3600], Loss: 0.0009\n",
            "Epoch [85/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [85/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [85/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [85/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [85/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [85/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [85/100], Step [1200/3600], Loss: 0.0005\n",
            "Epoch [85/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [85/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [85/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [85/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [85/100], Step [1700/3600], Loss: 0.0002\n",
            "Epoch [85/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [85/100], Step [1900/3600], Loss: 0.0972\n",
            "Epoch [85/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [85/100], Step [2100/3600], Loss: 0.0000\n",
            "Epoch [85/100], Step [2200/3600], Loss: 0.0002\n",
            "Epoch [85/100], Step [2300/3600], Loss: 0.0001\n",
            "Epoch [85/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [85/100], Step [2500/3600], Loss: 0.0132\n",
            "Epoch [85/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [85/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [85/100], Step [2800/3600], Loss: 0.0029\n",
            "Epoch [86/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [86/100], Step [200/3600], Loss: 0.0015\n",
            "Epoch [86/100], Step [300/3600], Loss: 0.0038\n",
            "Epoch [86/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [86/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [86/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [86/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [86/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [86/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [86/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [86/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [86/100], Step [1200/3600], Loss: 0.0001\n",
            "Epoch [86/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [86/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [86/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [86/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [86/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [86/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [86/100], Step [1900/3600], Loss: 0.1051\n",
            "Epoch [86/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [86/100], Step [2100/3600], Loss: 0.0001\n",
            "Epoch [86/100], Step [2200/3600], Loss: 0.0017\n",
            "Epoch [86/100], Step [2300/3600], Loss: 0.0000\n",
            "Epoch [86/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [86/100], Step [2500/3600], Loss: 0.0011\n",
            "Epoch [86/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [86/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [86/100], Step [2800/3600], Loss: 0.0001\n",
            "Epoch [87/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [87/100], Step [200/3600], Loss: 0.0003\n",
            "Epoch [87/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [87/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [87/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [87/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [87/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [87/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [87/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [87/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [87/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [87/100], Step [1200/3600], Loss: 0.0012\n",
            "Epoch [87/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [87/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [87/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [87/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [87/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [87/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [87/100], Step [1900/3600], Loss: 0.1393\n",
            "Epoch [87/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [87/100], Step [2100/3600], Loss: 0.0252\n",
            "Epoch [87/100], Step [2200/3600], Loss: 0.0019\n",
            "Epoch [87/100], Step [2300/3600], Loss: 0.0000\n",
            "Epoch [87/100], Step [2400/3600], Loss: 0.0082\n",
            "Epoch [87/100], Step [2500/3600], Loss: 0.0016\n",
            "Epoch [87/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [87/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [87/100], Step [2800/3600], Loss: 0.0216\n",
            "Epoch [88/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [88/100], Step [200/3600], Loss: 0.0001\n",
            "Epoch [88/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [88/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [88/100], Step [500/3600], Loss: 0.0001\n",
            "Epoch [88/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [88/100], Step [700/3600], Loss: 0.0001\n",
            "Epoch [88/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [88/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [88/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [88/100], Step [1100/3600], Loss: 0.0001\n",
            "Epoch [88/100], Step [1200/3600], Loss: 0.0121\n",
            "Epoch [88/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [88/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [88/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [88/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [88/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [88/100], Step [1800/3600], Loss: 0.0001\n",
            "Epoch [88/100], Step [1900/3600], Loss: 0.1559\n",
            "Epoch [88/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [88/100], Step [2100/3600], Loss: 0.0000\n",
            "Epoch [88/100], Step [2200/3600], Loss: 0.0000\n",
            "Epoch [88/100], Step [2300/3600], Loss: 0.0000\n",
            "Epoch [88/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [88/100], Step [2500/3600], Loss: 0.0008\n",
            "Epoch [88/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [88/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [88/100], Step [2800/3600], Loss: 0.0001\n",
            "Epoch [89/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [89/100], Step [200/3600], Loss: 0.0025\n",
            "Epoch [89/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [89/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [89/100], Step [500/3600], Loss: 0.0000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [89/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [89/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [89/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [89/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [89/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [89/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [89/100], Step [1200/3600], Loss: 0.0110\n",
            "Epoch [89/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [89/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [89/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [89/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [89/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [89/100], Step [1800/3600], Loss: 0.0001\n",
            "Epoch [89/100], Step [1900/3600], Loss: 0.1679\n",
            "Epoch [89/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [89/100], Step [2100/3600], Loss: 0.0000\n",
            "Epoch [89/100], Step [2200/3600], Loss: 0.0000\n",
            "Epoch [89/100], Step [2300/3600], Loss: 0.0000\n",
            "Epoch [89/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [89/100], Step [2500/3600], Loss: 0.0010\n",
            "Epoch [89/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [89/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [89/100], Step [2800/3600], Loss: 0.0011\n",
            "Epoch [90/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [90/100], Step [200/3600], Loss: 0.0701\n",
            "Epoch [90/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [90/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [90/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [90/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [90/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [90/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [90/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [90/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [90/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [90/100], Step [1200/3600], Loss: 0.0149\n",
            "Epoch [90/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [90/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [90/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [90/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [90/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [90/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [90/100], Step [1900/3600], Loss: 0.2178\n",
            "Epoch [90/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [90/100], Step [2100/3600], Loss: 0.0000\n",
            "Epoch [90/100], Step [2200/3600], Loss: 0.0000\n",
            "Epoch [90/100], Step [2300/3600], Loss: 0.0000\n",
            "Epoch [90/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [90/100], Step [2500/3600], Loss: 0.0000\n",
            "Epoch [90/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [90/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [90/100], Step [2800/3600], Loss: 0.0029\n",
            "Epoch [91/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [91/100], Step [200/3600], Loss: 0.0000\n",
            "Epoch [91/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [91/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [91/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [91/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [91/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [91/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [91/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [91/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [91/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [91/100], Step [1200/3600], Loss: 0.0001\n",
            "Epoch [91/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [91/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [91/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [91/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [91/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [91/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [91/100], Step [1900/3600], Loss: 0.0265\n",
            "Epoch [91/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [91/100], Step [2100/3600], Loss: 0.0001\n",
            "Epoch [91/100], Step [2200/3600], Loss: 0.0001\n",
            "Epoch [91/100], Step [2300/3600], Loss: 0.0000\n",
            "Epoch [91/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [91/100], Step [2500/3600], Loss: 0.0019\n",
            "Epoch [91/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [91/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [91/100], Step [2800/3600], Loss: 0.0002\n",
            "Epoch [92/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [92/100], Step [200/3600], Loss: 0.0000\n",
            "Epoch [92/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [92/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [92/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [92/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [92/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [92/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [92/100], Step [900/3600], Loss: 0.0009\n",
            "Epoch [92/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [92/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [92/100], Step [1200/3600], Loss: 0.0072\n",
            "Epoch [92/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [92/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [92/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [92/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [92/100], Step [1700/3600], Loss: 0.0003\n",
            "Epoch [92/100], Step [1800/3600], Loss: 0.0001\n",
            "Epoch [92/100], Step [1900/3600], Loss: 0.1934\n",
            "Epoch [92/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [92/100], Step [2100/3600], Loss: 0.0001\n",
            "Epoch [92/100], Step [2200/3600], Loss: 0.0000\n",
            "Epoch [92/100], Step [2300/3600], Loss: 0.0001\n",
            "Epoch [92/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [92/100], Step [2500/3600], Loss: 0.0156\n",
            "Epoch [92/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [92/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [92/100], Step [2800/3600], Loss: 0.0003\n",
            "Epoch [93/100], Step [100/3600], Loss: 0.0020\n",
            "Epoch [93/100], Step [200/3600], Loss: 0.0008\n",
            "Epoch [93/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [93/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [93/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [93/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [93/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [93/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [93/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [93/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [93/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [93/100], Step [1200/3600], Loss: 0.0000\n",
            "Epoch [93/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [93/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [93/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [93/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [93/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [93/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [93/100], Step [1900/3600], Loss: 0.1787\n",
            "Epoch [93/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [93/100], Step [2100/3600], Loss: 0.0000\n",
            "Epoch [93/100], Step [2200/3600], Loss: 0.0000\n",
            "Epoch [93/100], Step [2300/3600], Loss: 0.0287\n",
            "Epoch [93/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [93/100], Step [2500/3600], Loss: 0.0032\n",
            "Epoch [93/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [93/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [93/100], Step [2800/3600], Loss: 0.0011\n",
            "Epoch [94/100], Step [100/3600], Loss: 0.0001\n",
            "Epoch [94/100], Step [200/3600], Loss: 0.0001\n",
            "Epoch [94/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [94/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [94/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [94/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [94/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [94/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [94/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [94/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [94/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [94/100], Step [1200/3600], Loss: 0.0000\n",
            "Epoch [94/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [94/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [94/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [94/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [94/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [94/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [94/100], Step [1900/3600], Loss: 0.1444\n",
            "Epoch [94/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [94/100], Step [2100/3600], Loss: 0.0000\n",
            "Epoch [94/100], Step [2200/3600], Loss: 0.0000\n",
            "Epoch [94/100], Step [2300/3600], Loss: 0.0000\n",
            "Epoch [94/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [94/100], Step [2500/3600], Loss: 0.0004\n",
            "Epoch [94/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [94/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [94/100], Step [2800/3600], Loss: 0.0004\n",
            "Epoch [95/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [95/100], Step [200/3600], Loss: 0.0000\n",
            "Epoch [95/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [95/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [95/100], Step [500/3600], Loss: 0.0001\n",
            "Epoch [95/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [95/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [95/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [95/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [95/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [95/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [95/100], Step [1200/3600], Loss: 0.0000\n",
            "Epoch [95/100], Step [1300/3600], Loss: 0.0000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [95/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [95/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [95/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [95/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [95/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [95/100], Step [1900/3600], Loss: 0.0949\n",
            "Epoch [95/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [95/100], Step [2100/3600], Loss: 0.0007\n",
            "Epoch [95/100], Step [2200/3600], Loss: 0.0000\n",
            "Epoch [95/100], Step [2300/3600], Loss: 0.0000\n",
            "Epoch [95/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [95/100], Step [2500/3600], Loss: 0.0002\n",
            "Epoch [95/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [95/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [95/100], Step [2800/3600], Loss: 0.0004\n",
            "Epoch [96/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [96/100], Step [200/3600], Loss: 0.0000\n",
            "Epoch [96/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [96/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [96/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [96/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [96/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [96/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [96/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [96/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [96/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [96/100], Step [1200/3600], Loss: 0.0044\n",
            "Epoch [96/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [96/100], Step [1400/3600], Loss: 0.0003\n",
            "Epoch [96/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [96/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [96/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [96/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [96/100], Step [1900/3600], Loss: 0.0503\n",
            "Epoch [96/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [96/100], Step [2100/3600], Loss: 0.0000\n",
            "Epoch [96/100], Step [2200/3600], Loss: 0.0000\n",
            "Epoch [96/100], Step [2300/3600], Loss: 0.0000\n",
            "Epoch [96/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [96/100], Step [2500/3600], Loss: 0.0065\n",
            "Epoch [96/100], Step [2600/3600], Loss: 0.0001\n",
            "Epoch [96/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [96/100], Step [2800/3600], Loss: 0.0000\n",
            "Epoch [97/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [97/100], Step [200/3600], Loss: 0.0000\n",
            "Epoch [97/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [97/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [97/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [97/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [97/100], Step [700/3600], Loss: 0.0001\n",
            "Epoch [97/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [97/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [97/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [97/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [97/100], Step [1200/3600], Loss: 0.0003\n",
            "Epoch [97/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [97/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [97/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [97/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [97/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [97/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [97/100], Step [1900/3600], Loss: 0.0230\n",
            "Epoch [97/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [97/100], Step [2100/3600], Loss: 0.0119\n",
            "Epoch [97/100], Step [2200/3600], Loss: 0.0000\n",
            "Epoch [97/100], Step [2300/3600], Loss: 0.0000\n",
            "Epoch [97/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [97/100], Step [2500/3600], Loss: 0.0004\n",
            "Epoch [97/100], Step [2600/3600], Loss: 0.1657\n",
            "Epoch [97/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [97/100], Step [2800/3600], Loss: 0.0005\n",
            "Epoch [98/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [98/100], Step [200/3600], Loss: 0.0004\n",
            "Epoch [98/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [98/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [98/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [98/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [98/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [98/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [98/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [98/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [98/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [98/100], Step [1200/3600], Loss: 0.0275\n",
            "Epoch [98/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [98/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [98/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [98/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [98/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [98/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [98/100], Step [1900/3600], Loss: 0.0739\n",
            "Epoch [98/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [98/100], Step [2100/3600], Loss: 0.0002\n",
            "Epoch [98/100], Step [2200/3600], Loss: 0.0000\n",
            "Epoch [98/100], Step [2300/3600], Loss: 0.0000\n",
            "Epoch [98/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [98/100], Step [2500/3600], Loss: 0.0002\n",
            "Epoch [98/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [98/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [98/100], Step [2800/3600], Loss: 0.0004\n",
            "Epoch [99/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [99/100], Step [200/3600], Loss: 0.0000\n",
            "Epoch [99/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [99/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [99/100], Step [500/3600], Loss: 0.0009\n",
            "Epoch [99/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [99/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [99/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [99/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [99/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [99/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [99/100], Step [1200/3600], Loss: 0.0001\n",
            "Epoch [99/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [99/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [99/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [99/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [99/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [99/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [99/100], Step [1900/3600], Loss: 0.0692\n",
            "Epoch [99/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [99/100], Step [2100/3600], Loss: 0.0000\n",
            "Epoch [99/100], Step [2200/3600], Loss: 0.0000\n",
            "Epoch [99/100], Step [2300/3600], Loss: 0.0000\n",
            "Epoch [99/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [99/100], Step [2500/3600], Loss: 0.0001\n",
            "Epoch [99/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [99/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [99/100], Step [2800/3600], Loss: 0.0008\n",
            "Epoch [100/100], Step [100/3600], Loss: 0.0000\n",
            "Epoch [100/100], Step [200/3600], Loss: 0.0000\n",
            "Epoch [100/100], Step [300/3600], Loss: 0.0000\n",
            "Epoch [100/100], Step [400/3600], Loss: 0.0000\n",
            "Epoch [100/100], Step [500/3600], Loss: 0.0000\n",
            "Epoch [100/100], Step [600/3600], Loss: 0.0000\n",
            "Epoch [100/100], Step [700/3600], Loss: 0.0000\n",
            "Epoch [100/100], Step [800/3600], Loss: 0.0000\n",
            "Epoch [100/100], Step [900/3600], Loss: 0.0000\n",
            "Epoch [100/100], Step [1000/3600], Loss: 0.0000\n",
            "Epoch [100/100], Step [1100/3600], Loss: 0.0000\n",
            "Epoch [100/100], Step [1200/3600], Loss: 0.0000\n",
            "Epoch [100/100], Step [1300/3600], Loss: 0.0000\n",
            "Epoch [100/100], Step [1400/3600], Loss: 0.0000\n",
            "Epoch [100/100], Step [1500/3600], Loss: 0.0000\n",
            "Epoch [100/100], Step [1600/3600], Loss: 0.0000\n",
            "Epoch [100/100], Step [1700/3600], Loss: 0.0000\n",
            "Epoch [100/100], Step [1800/3600], Loss: 0.0000\n",
            "Epoch [100/100], Step [1900/3600], Loss: 0.1499\n",
            "Epoch [100/100], Step [2000/3600], Loss: 0.0000\n",
            "Epoch [100/100], Step [2100/3600], Loss: 0.0008\n",
            "Epoch [100/100], Step [2200/3600], Loss: 0.0000\n",
            "Epoch [100/100], Step [2300/3600], Loss: 0.0001\n",
            "Epoch [100/100], Step [2400/3600], Loss: 0.0000\n",
            "Epoch [100/100], Step [2500/3600], Loss: 0.0001\n",
            "Epoch [100/100], Step [2600/3600], Loss: 0.0000\n",
            "Epoch [100/100], Step [2700/3600], Loss: 0.0000\n",
            "Epoch [100/100], Step [2800/3600], Loss: 0.0001\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "class TDLSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(TDLSTMClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm1 = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size*2, num_classes)  # Concatenate output from both LSTMs\n",
        "    \n",
        "    def forward(self, x1,x2):\n",
        "        lstm_out1, _ = self.lstm1(x1)\n",
        "        lstm_out2, _ = self.lstm2(x2)\n",
        "        lstm_out = torch.cat((lstm_out1[:, -1, :], lstm_out2[:, -1, :]), dim=1)  # Concatenate output from both LSTMs\n",
        "        out = self.fc(lstm_out)\n",
        "        return out\n",
        "input_size = 868\n",
        "hidden_size = 64\n",
        "num_classes = 3\n",
        "learning_rate = 0.001\n",
        "num_epochs = 100\n",
        "batch_size = 1\n",
        "\n",
        "# Initialize the model and optimizer\n",
        "tdmodel = TDLSTMClassifier(input_size, hidden_size, num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(tcmodel.parameters(), lr=learning_rate)\n",
        "\n",
        "# assuming you have your data in the form of input pairs and labels:\n",
        "# input_pairs is a list of pairs of input sequences (x1, x2)\n",
        "# labels is a list of corresponding labels\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(len(input_pairs[:2800])):\n",
        "        # get the current input pair and label\n",
        "        x1, x2 = input_pairs[i][0] , input_pairs[i][1]\n",
        "        label1 = label[i]\n",
        "        \n",
        "        # convert input and label to PyTorch tensors\n",
        "        label1 = torch.LongTensor([label1])\n",
        "        \n",
        "        # zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # forward pass through the model\n",
        "        output = tcmodel(torch.FloatTensor(x1).unsqueeze(0), torch.FloatTensor(x2).unsqueeze(0))\n",
        "        \n",
        "        # calculate the loss and backpropagate\n",
        "        loss = criterion(output, label1)\n",
        "        loss.backward()\n",
        "        \n",
        "        # update the model parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        # print the loss every 100 steps\n",
        "        if (i+1) % 100 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, len(input_pairs), loss.item()))\n",
        "\n",
        "torch.save(tcmodel.state_dict(), 'td-lstm.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6T29985DY2R2",
        "outputId": "99e245a4-2d11-4fd1-ad26-fa99cb09458d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3600\n",
            "4400\n"
          ]
        }
      ],
      "source": [
        "P_sentences = []\n",
        "for i in range(2800,3600):\n",
        "    psentence = df['Sentence'][i]\n",
        "    ten1 = np.array(indomain_vector(psentence))\n",
        "    ten2 = bert_to_token(psentence).detach().numpy()\n",
        "    vec = np.concatenate((ten1, ten2), axis=1)\n",
        "    P_sentences.append(vec)\n",
        "def P_pair(i):\n",
        "    s1 = np.array(P_sentences[i-2800])\n",
        "    answer = []\n",
        "    i1 = indices[i][0]\n",
        "    j1 = indices[i][1]\n",
        "    answer.append(s1[:i1+1])\n",
        "    answer.append(s1[j1-1:])\n",
        "    return answer\n",
        "# for all sentences splitting left aspect terms and right aspect terms\n",
        "print(len(input_pairs))\n",
        "for i in range(2800,3600):\n",
        "    input_pairs.append(P_pair(i))\n",
        "print(len(input_pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q41Yo8yshOuJ"
      },
      "outputs": [],
      "source": [
        "def test_tdlstm(x1_new,x2_new):\n",
        "    # Initialize the model with the same parameters as before\n",
        "    tdmodel = TDLSTMClassifier(input_size, hidden_size, num_classes)\n",
        "\n",
        "    # Load the saved state dictionary into the model\n",
        "    tdmodel.load_state_dict(torch.load('td-lstm.pt'))\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    tdmodel.eval()\n",
        "    # make a forward pass through the model to obtain the predicted output\n",
        "    with torch.no_grad():\n",
        "        predicted_output = tdmodel(torch.FloatTensor(x1_new).unsqueeze(0), torch.FloatTensor(x2_new).unsqueeze(0))\n",
        "\n",
        "    # get the predicted class index (the class with the highest score)\n",
        "    predicted_class = torch.argmax(predicted_output, dim=1).item()\n",
        "    return predicted_class\n",
        "\n",
        "predict = []\n",
        "count = 0\n",
        "for i in range(2800,3600):\n",
        "    p = test_tdlstm(input_pairs[i][0] , input_pairs[i][1])\n",
        "    if p!=df['polarity'][i]:\n",
        "        count = count + 1\n",
        "#_______________________________________________________________________________________________________________________________\n",
        "## this below function is use when there is new sentence and aspect_term\n",
        "def predict_tdlstm(sentence,aspect_term):\n",
        "    ten1 = np.array(indomain_vector(sentence))\n",
        "    ten2 = bert_to_token(sentence).detach().numpy()\n",
        "    vec = np.concatenate((ten1, ten2), axis=1)\n",
        "    s2 = np.array(vec)\n",
        "    answer = []\n",
        "    indices = ind(sentence,aspect_term)\n",
        "    i1 = indices[0]\n",
        "    j1 = indices[1]\n",
        "    answer.append(s2[:i1+1])\n",
        "    answer.append(s2[j1-1:])\n",
        "    p = test_tdlstm(answer[0],answer[1])\n",
        "    return p\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYVQ3UNoMoNk",
        "outputId": "de603548-48e3-4aec-aca4-6cff013d4a6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy= 63.75 %\n"
          ]
        }
      ],
      "source": [
        "print(\"accuracy=\",(800-count)*100/800,\"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPKO1GC-Y2R2"
      },
      "source": [
        "### Combining subtask1 with TD-LSTM "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8ev79KzY2R3"
      },
      "outputs": [],
      "source": [
        "def aspect_term_predict(predict_sentence):\n",
        "    classification_layer = torch.nn.Linear(768, 3)\n",
        "\n",
        "    # Load the saved weights\n",
        "    classification_layer.load_state_dict(torch.load('classification_layer_weights1.pth'))\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    classification_layer.eval()\n",
        "\n",
        "    # Obtain token vectors for the new input data\n",
        "    token_vectors = bert_to_token(predict_sentence)\n",
        "\n",
        "    # Feed the token vectors through the classification layer to obtain logits\n",
        "    logits = classification_layer(token_vectors)\n",
        "\n",
        "    # Convert the logits to probabilities using softmax\n",
        "    probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
        "    print(probabilities)\n",
        "    # Get the predicted class index\n",
        "    predicted_class_index = torch.argmax(probabilities, dim=1)\n",
        "    \n",
        "    return predicted_class_index\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "591Id5VVY2R3",
        "outputId": "998da2fd-ccc7-4d39-cfee-b4bf1b782b83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.5888, 0.3166, 0.0946],\n",
            "        [0.3327, 0.4392, 0.2281],\n",
            "        [0.3862, 0.1801, 0.4337],\n",
            "        [0.2663, 0.3343, 0.3994],\n",
            "        [0.8483, 0.0258, 0.1259],\n",
            "        [0.8611, 0.0568, 0.0821],\n",
            "        [0.5902, 0.2206, 0.1891],\n",
            "        [0.8188, 0.0478, 0.1334],\n",
            "        [0.8720, 0.0260, 0.1020],\n",
            "        [0.5321, 0.2402, 0.2277],\n",
            "        [0.2832, 0.3990, 0.3178],\n",
            "        [0.9100, 0.0140, 0.0760],\n",
            "        [0.8135, 0.0845, 0.1020]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0])"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence = \"the chicken fried rice is so good, and also mutton curry is bad\"\n",
        "aspect_term_predict(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVD7lVa6Y2R3"
      },
      "outputs": [],
      "source": [
        "def predict_tdlstm(sentence,aspect_term):\n",
        "    ten1 = np.array(indomain_vector(sentence))\n",
        "    ten2 = bert_to_token(sentence).detach().numpy()\n",
        "    vec = np.concatenate((ten1, ten2), axis=1)\n",
        "    s2 = np.array(vec)\n",
        "    answer = []\n",
        "    indices = ind(sentence,aspect_term)\n",
        "    i1 = indices[0]\n",
        "    j1 = indices[1]\n",
        "    answer.append(s2[:i1+1])\n",
        "    answer.append(s2[j1-1:])\n",
        "    p = test_tdlstm(answer[0],answer[1])\n",
        "    return p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXIKW4mJY2R3"
      },
      "source": [
        "# TC-LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1LSkSdvY2R3"
      },
      "source": [
        "###  function for adding appending aspect term to all words in sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VZcJ1mKY2R3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vRluC_eY2R4"
      },
      "outputs": [],
      "source": [
        "tc_sentences = []\n",
        "for i in range(len(sentences)):\n",
        "    a = splitSentence(df['Aspect Term'][i])\n",
        "    s = 0\n",
        "    count = 0\n",
        "    for j in a:\n",
        "        s =  s + bert_to_token(j)\n",
        "        count = count + 1\n",
        "    tensor1 = torch.tensor(sentences[i])\n",
        "    tensor2 = s/count\n",
        "\n",
        "    tensor2_repeated = tensor2.repeat(tensor1.shape[0], 1)\n",
        "    tensor3 = torch.cat([tensor1, tensor2_repeated], dim=1)\n",
        "    tc_sentences.append(tensor3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-uXVYC0Y2R4"
      },
      "outputs": [],
      "source": [
        "def pair_Tc(i):\n",
        "    s1 = tc_sentences[i].detach().numpy()\n",
        "    answer = []\n",
        "    i1 = indices[i][0]\n",
        "    j1 = indices[i][1]\n",
        "    answer.append(s1[:i1+1])\n",
        "    answer.append(s1[j1-1:])\n",
        "    return answer\n",
        "# for all sentences splitting left aspect terms and right aspect terms\n",
        "inputPairs = []\n",
        "for i in range(len(sentences)):\n",
        "    inputPairs.append(pair_Tc(i))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_8BMRjLY2R4"
      },
      "source": [
        "#### classes imbalance issue "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBwuUc9wY2R4",
        "outputId": "7f7cac73-10fd-4bbd-aea0-ebc9b150f410"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[531, 1636, 633]\n",
            "tensor([2.6365, 0.8557, 2.2117])\n"
          ]
        }
      ],
      "source": [
        "C_0 = 0\n",
        "C_1=0\n",
        "C_2 = 0\n",
        "for i in range(2800):\n",
        "    if label[i]==0:\n",
        "        C_0 = C_0 + 1\n",
        "    if label[i]==1:\n",
        "        C_1 = C_1 + 1\n",
        "    if label[i]==2:\n",
        "        C_2 = C_2 + 1\n",
        "class_counts = [C_0, C_1, C_2] \n",
        "class_weights = torch.tensor([sum(class_counts) / (2 * class_counts[0]), sum(class_counts) / (2 * class_counts[1]), sum(class_counts) / (2 * class_counts[2])])\n",
        "print(class_counts)\n",
        "print(class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4EQVQWgY2R4",
        "outputId": "c70ce1e2-a215-4988-b712-946da8bd794e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Step [100/2800], Loss: 0.3199\n",
            "Epoch [1/100], Step [200/2800], Loss: 0.3698\n",
            "Epoch [1/100], Step [300/2800], Loss: 0.6291\n",
            "Epoch [1/100], Step [400/2800], Loss: 1.4763\n",
            "Epoch [1/100], Step [500/2800], Loss: 1.3244\n",
            "Epoch [1/100], Step [600/2800], Loss: 0.0276\n",
            "Epoch [1/100], Step [700/2800], Loss: 0.4609\n",
            "Epoch [1/100], Step [800/2800], Loss: 0.2351\n",
            "Epoch [1/100], Step [900/2800], Loss: 0.3024\n",
            "Epoch [1/100], Step [1000/2800], Loss: 0.7798\n",
            "Epoch [1/100], Step [1100/2800], Loss: 0.4683\n",
            "Epoch [1/100], Step [1200/2800], Loss: 0.4498\n",
            "Epoch [1/100], Step [1300/2800], Loss: 0.7494\n",
            "Epoch [1/100], Step [1400/2800], Loss: 0.4240\n",
            "Epoch [1/100], Step [1500/2800], Loss: 1.5347\n",
            "Epoch [1/100], Step [1600/2800], Loss: 1.6611\n",
            "Epoch [1/100], Step [1700/2800], Loss: 1.1703\n",
            "Epoch [1/100], Step [1800/2800], Loss: 0.6255\n",
            "Epoch [1/100], Step [1900/2800], Loss: 0.2816\n",
            "Epoch [1/100], Step [2000/2800], Loss: 0.3346\n",
            "Epoch [1/100], Step [2100/2800], Loss: 1.8848\n",
            "Epoch [1/100], Step [2200/2800], Loss: 0.6615\n",
            "Epoch [1/100], Step [2300/2800], Loss: 1.4744\n",
            "Epoch [1/100], Step [2400/2800], Loss: 0.3072\n",
            "Epoch [1/100], Step [2500/2800], Loss: 0.8794\n",
            "Epoch [1/100], Step [2600/2800], Loss: 1.6206\n",
            "Epoch [1/100], Step [2700/2800], Loss: 0.7220\n",
            "Epoch [1/100], Step [2800/2800], Loss: 1.6502\n",
            "Epoch [2/100], Step [100/2800], Loss: 0.1823\n",
            "Epoch [2/100], Step [200/2800], Loss: 0.1883\n",
            "Epoch [2/100], Step [300/2800], Loss: 0.5535\n",
            "Epoch [2/100], Step [400/2800], Loss: 1.2566\n",
            "Epoch [2/100], Step [500/2800], Loss: 0.8402\n",
            "Epoch [2/100], Step [600/2800], Loss: 0.0442\n",
            "Epoch [2/100], Step [700/2800], Loss: 0.4073\n",
            "Epoch [2/100], Step [800/2800], Loss: 0.1156\n",
            "Epoch [2/100], Step [900/2800], Loss: 0.3704\n",
            "Epoch [2/100], Step [1000/2800], Loss: 0.8473\n",
            "Epoch [2/100], Step [1100/2800], Loss: 0.3328\n",
            "Epoch [2/100], Step [1200/2800], Loss: 0.3018\n",
            "Epoch [2/100], Step [1300/2800], Loss: 0.4211\n",
            "Epoch [2/100], Step [1400/2800], Loss: 0.3715\n",
            "Epoch [2/100], Step [1500/2800], Loss: 1.7892\n",
            "Epoch [2/100], Step [1600/2800], Loss: 1.6652\n",
            "Epoch [2/100], Step [1700/2800], Loss: 0.7473\n",
            "Epoch [2/100], Step [1800/2800], Loss: 0.6098\n",
            "Epoch [2/100], Step [1900/2800], Loss: 0.3709\n",
            "Epoch [2/100], Step [2000/2800], Loss: 0.4816\n",
            "Epoch [2/100], Step [2100/2800], Loss: 1.6365\n",
            "Epoch [2/100], Step [2200/2800], Loss: 0.9751\n",
            "Epoch [2/100], Step [2300/2800], Loss: 1.4365\n",
            "Epoch [2/100], Step [2400/2800], Loss: 0.1100\n",
            "Epoch [2/100], Step [2500/2800], Loss: 0.8965\n",
            "Epoch [2/100], Step [2600/2800], Loss: 1.1836\n",
            "Epoch [2/100], Step [2700/2800], Loss: 0.2557\n",
            "Epoch [2/100], Step [2800/2800], Loss: 1.1567\n",
            "Epoch [3/100], Step [100/2800], Loss: 0.1039\n",
            "Epoch [3/100], Step [200/2800], Loss: 0.2209\n",
            "Epoch [3/100], Step [300/2800], Loss: 0.4652\n",
            "Epoch [3/100], Step [400/2800], Loss: 0.8458\n",
            "Epoch [3/100], Step [500/2800], Loss: 0.3672\n",
            "Epoch [3/100], Step [600/2800], Loss: 0.0575\n",
            "Epoch [3/100], Step [700/2800], Loss: 0.2883\n",
            "Epoch [3/100], Step [800/2800], Loss: 0.0823\n",
            "Epoch [3/100], Step [900/2800], Loss: 0.3122\n",
            "Epoch [3/100], Step [1000/2800], Loss: 0.8678\n",
            "Epoch [3/100], Step [1100/2800], Loss: 0.3078\n",
            "Epoch [3/100], Step [1200/2800], Loss: 0.2160\n",
            "Epoch [3/100], Step [1300/2800], Loss: 0.2766\n",
            "Epoch [3/100], Step [1400/2800], Loss: 0.3901\n",
            "Epoch [3/100], Step [1500/2800], Loss: 1.9347\n",
            "Epoch [3/100], Step [1600/2800], Loss: 0.9986\n",
            "Epoch [3/100], Step [1700/2800], Loss: 0.4070\n",
            "Epoch [3/100], Step [1800/2800], Loss: 0.4699\n",
            "Epoch [3/100], Step [1900/2800], Loss: 0.3942\n",
            "Epoch [3/100], Step [2000/2800], Loss: 0.3804\n",
            "Epoch [3/100], Step [2100/2800], Loss: 1.2643\n",
            "Epoch [3/100], Step [2200/2800], Loss: 1.4622\n",
            "Epoch [3/100], Step [2300/2800], Loss: 1.1057\n",
            "Epoch [3/100], Step [2400/2800], Loss: 0.0973\n",
            "Epoch [3/100], Step [2500/2800], Loss: 0.9767\n",
            "Epoch [3/100], Step [2600/2800], Loss: 0.9204\n",
            "Epoch [3/100], Step [2700/2800], Loss: 0.1785\n",
            "Epoch [3/100], Step [2800/2800], Loss: 1.2176\n",
            "Epoch [4/100], Step [100/2800], Loss: 0.0567\n",
            "Epoch [4/100], Step [200/2800], Loss: 0.1451\n",
            "Epoch [4/100], Step [300/2800], Loss: 0.4398\n",
            "Epoch [4/100], Step [400/2800], Loss: 0.8774\n",
            "Epoch [4/100], Step [500/2800], Loss: 0.3620\n",
            "Epoch [4/100], Step [600/2800], Loss: 0.0419\n",
            "Epoch [4/100], Step [700/2800], Loss: 0.2904\n",
            "Epoch [4/100], Step [800/2800], Loss: 0.1096\n",
            "Epoch [4/100], Step [900/2800], Loss: 0.3091\n",
            "Epoch [4/100], Step [1000/2800], Loss: 1.0163\n",
            "Epoch [4/100], Step [1100/2800], Loss: 0.3093\n",
            "Epoch [4/100], Step [1200/2800], Loss: 0.1801\n",
            "Epoch [4/100], Step [1300/2800], Loss: 0.3181\n",
            "Epoch [4/100], Step [1400/2800], Loss: 0.4827\n",
            "Epoch [4/100], Step [1500/2800], Loss: 1.5447\n",
            "Epoch [4/100], Step [1600/2800], Loss: 0.6061\n",
            "Epoch [4/100], Step [1700/2800], Loss: 0.2240\n",
            "Epoch [4/100], Step [1800/2800], Loss: 0.4323\n",
            "Epoch [4/100], Step [1900/2800], Loss: 0.3021\n",
            "Epoch [4/100], Step [2000/2800], Loss: 0.3968\n",
            "Epoch [4/100], Step [2100/2800], Loss: 1.1483\n",
            "Epoch [4/100], Step [2200/2800], Loss: 1.3821\n",
            "Epoch [4/100], Step [2300/2800], Loss: 1.0860\n",
            "Epoch [4/100], Step [2400/2800], Loss: 0.1697\n",
            "Epoch [4/100], Step [2500/2800], Loss: 1.0175\n",
            "Epoch [4/100], Step [2600/2800], Loss: 0.6223\n",
            "Epoch [4/100], Step [2700/2800], Loss: 0.2873\n",
            "Epoch [4/100], Step [2800/2800], Loss: 1.3811\n",
            "Epoch [5/100], Step [100/2800], Loss: 0.0321\n",
            "Epoch [5/100], Step [200/2800], Loss: 0.1311\n",
            "Epoch [5/100], Step [300/2800], Loss: 0.2449\n",
            "Epoch [5/100], Step [400/2800], Loss: 0.7631\n",
            "Epoch [5/100], Step [500/2800], Loss: 0.3598\n",
            "Epoch [5/100], Step [600/2800], Loss: 0.0458\n",
            "Epoch [5/100], Step [700/2800], Loss: 0.2553\n",
            "Epoch [5/100], Step [800/2800], Loss: 0.1113\n",
            "Epoch [5/100], Step [900/2800], Loss: 0.1825\n",
            "Epoch [5/100], Step [1000/2800], Loss: 0.4850\n",
            "Epoch [5/100], Step [1100/2800], Loss: 0.3912\n",
            "Epoch [5/100], Step [1200/2800], Loss: 0.1431\n",
            "Epoch [5/100], Step [1300/2800], Loss: 0.2340\n",
            "Epoch [5/100], Step [1400/2800], Loss: 0.4248\n",
            "Epoch [5/100], Step [1500/2800], Loss: 1.5637\n",
            "Epoch [5/100], Step [1600/2800], Loss: 0.8007\n",
            "Epoch [5/100], Step [1700/2800], Loss: 0.1449\n",
            "Epoch [5/100], Step [1800/2800], Loss: 0.2212\n",
            "Epoch [5/100], Step [1900/2800], Loss: 0.4398\n",
            "Epoch [5/100], Step [2000/2800], Loss: 0.2465\n",
            "Epoch [5/100], Step [2100/2800], Loss: 0.9011\n",
            "Epoch [5/100], Step [2200/2800], Loss: 1.4599\n",
            "Epoch [5/100], Step [2300/2800], Loss: 0.9267\n",
            "Epoch [5/100], Step [2400/2800], Loss: 0.0534\n",
            "Epoch [5/100], Step [2500/2800], Loss: 1.3464\n",
            "Epoch [5/100], Step [2600/2800], Loss: 1.2701\n",
            "Epoch [5/100], Step [2700/2800], Loss: 0.3073\n",
            "Epoch [5/100], Step [2800/2800], Loss: 1.3461\n",
            "Epoch [6/100], Step [100/2800], Loss: 0.0275\n",
            "Epoch [6/100], Step [200/2800], Loss: 0.0464\n",
            "Epoch [6/100], Step [300/2800], Loss: 0.3745\n",
            "Epoch [6/100], Step [400/2800], Loss: 0.7006\n",
            "Epoch [6/100], Step [500/2800], Loss: 0.3569\n",
            "Epoch [6/100], Step [600/2800], Loss: 0.0150\n",
            "Epoch [6/100], Step [700/2800], Loss: 0.1213\n",
            "Epoch [6/100], Step [800/2800], Loss: 0.0638\n",
            "Epoch [6/100], Step [900/2800], Loss: 0.1887\n",
            "Epoch [6/100], Step [1000/2800], Loss: 0.8410\n",
            "Epoch [6/100], Step [1100/2800], Loss: 0.2785\n",
            "Epoch [6/100], Step [1200/2800], Loss: 0.2215\n",
            "Epoch [6/100], Step [1300/2800], Loss: 0.3058\n",
            "Epoch [6/100], Step [1400/2800], Loss: 0.3229\n",
            "Epoch [6/100], Step [1500/2800], Loss: 1.6292\n",
            "Epoch [6/100], Step [1600/2800], Loss: 0.2381\n",
            "Epoch [6/100], Step [1700/2800], Loss: 0.1902\n",
            "Epoch [6/100], Step [1800/2800], Loss: 0.2361\n",
            "Epoch [6/100], Step [1900/2800], Loss: 0.1641\n",
            "Epoch [6/100], Step [2000/2800], Loss: 0.2571\n",
            "Epoch [6/100], Step [2100/2800], Loss: 0.7461\n",
            "Epoch [6/100], Step [2200/2800], Loss: 1.4985\n",
            "Epoch [6/100], Step [2300/2800], Loss: 0.7730\n",
            "Epoch [6/100], Step [2400/2800], Loss: 0.0222\n",
            "Epoch [6/100], Step [2500/2800], Loss: 0.6341\n",
            "Epoch [6/100], Step [2600/2800], Loss: 1.1175\n",
            "Epoch [6/100], Step [2700/2800], Loss: 0.1982\n",
            "Epoch [6/100], Step [2800/2800], Loss: 1.6303\n",
            "Epoch [7/100], Step [100/2800], Loss: 0.0226\n",
            "Epoch [7/100], Step [200/2800], Loss: 0.0322\n",
            "Epoch [7/100], Step [300/2800], Loss: 0.2231\n",
            "Epoch [7/100], Step [400/2800], Loss: 0.6698\n",
            "Epoch [7/100], Step [500/2800], Loss: 0.4817\n",
            "Epoch [7/100], Step [600/2800], Loss: 0.0143\n",
            "Epoch [7/100], Step [700/2800], Loss: 0.0840\n",
            "Epoch [7/100], Step [800/2800], Loss: 0.0756\n",
            "Epoch [7/100], Step [900/2800], Loss: 0.0926\n",
            "Epoch [7/100], Step [1000/2800], Loss: 0.6297\n",
            "Epoch [7/100], Step [1100/2800], Loss: 0.1294\n",
            "Epoch [7/100], Step [1200/2800], Loss: 0.1089\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [7/100], Step [1300/2800], Loss: 0.3075\n",
            "Epoch [7/100], Step [1400/2800], Loss: 0.3361\n",
            "Epoch [7/100], Step [1500/2800], Loss: 1.4725\n",
            "Epoch [7/100], Step [1600/2800], Loss: 0.6627\n",
            "Epoch [7/100], Step [1700/2800], Loss: 0.1579\n",
            "Epoch [7/100], Step [1800/2800], Loss: 0.0708\n",
            "Epoch [7/100], Step [1900/2800], Loss: 0.2455\n",
            "Epoch [7/100], Step [2000/2800], Loss: 0.1546\n",
            "Epoch [7/100], Step [2100/2800], Loss: 0.8388\n",
            "Epoch [7/100], Step [2200/2800], Loss: 1.2942\n",
            "Epoch [7/100], Step [2300/2800], Loss: 0.6805\n",
            "Epoch [7/100], Step [2400/2800], Loss: 0.0496\n",
            "Epoch [7/100], Step [2500/2800], Loss: 0.7018\n",
            "Epoch [7/100], Step [2600/2800], Loss: 0.9076\n",
            "Epoch [7/100], Step [2700/2800], Loss: 0.3345\n",
            "Epoch [7/100], Step [2800/2800], Loss: 1.4179\n",
            "Epoch [8/100], Step [100/2800], Loss: 0.0156\n",
            "Epoch [8/100], Step [200/2800], Loss: 0.0279\n",
            "Epoch [8/100], Step [300/2800], Loss: 0.1961\n",
            "Epoch [8/100], Step [400/2800], Loss: 0.2364\n",
            "Epoch [8/100], Step [500/2800], Loss: 0.3198\n",
            "Epoch [8/100], Step [600/2800], Loss: 0.0111\n",
            "Epoch [8/100], Step [700/2800], Loss: 0.0781\n",
            "Epoch [8/100], Step [800/2800], Loss: 0.0451\n",
            "Epoch [8/100], Step [900/2800], Loss: 0.1572\n",
            "Epoch [8/100], Step [1000/2800], Loss: 0.6430\n",
            "Epoch [8/100], Step [1100/2800], Loss: 0.1259\n",
            "Epoch [8/100], Step [1200/2800], Loss: 0.1356\n",
            "Epoch [8/100], Step [1300/2800], Loss: 0.8616\n",
            "Epoch [8/100], Step [1400/2800], Loss: 0.5559\n",
            "Epoch [8/100], Step [1500/2800], Loss: 1.4089\n",
            "Epoch [8/100], Step [1600/2800], Loss: 0.1605\n",
            "Epoch [8/100], Step [1700/2800], Loss: 0.1579\n",
            "Epoch [8/100], Step [1800/2800], Loss: 0.0387\n",
            "Epoch [8/100], Step [1900/2800], Loss: 0.1292\n",
            "Epoch [8/100], Step [2000/2800], Loss: 0.0975\n",
            "Epoch [8/100], Step [2100/2800], Loss: 0.8077\n",
            "Epoch [8/100], Step [2200/2800], Loss: 1.1836\n",
            "Epoch [8/100], Step [2300/2800], Loss: 0.4702\n",
            "Epoch [8/100], Step [2400/2800], Loss: 0.0912\n",
            "Epoch [8/100], Step [2500/2800], Loss: 0.9537\n",
            "Epoch [8/100], Step [2600/2800], Loss: 1.0458\n",
            "Epoch [8/100], Step [2700/2800], Loss: 0.1761\n",
            "Epoch [8/100], Step [2800/2800], Loss: 0.9002\n",
            "Epoch [9/100], Step [100/2800], Loss: 0.0159\n",
            "Epoch [9/100], Step [200/2800], Loss: 0.0317\n",
            "Epoch [9/100], Step [300/2800], Loss: 0.1854\n",
            "Epoch [9/100], Step [400/2800], Loss: 0.2162\n",
            "Epoch [9/100], Step [500/2800], Loss: 0.1416\n",
            "Epoch [9/100], Step [600/2800], Loss: 0.0085\n",
            "Epoch [9/100], Step [700/2800], Loss: 0.1223\n",
            "Epoch [9/100], Step [800/2800], Loss: 0.0169\n",
            "Epoch [9/100], Step [900/2800], Loss: 0.1247\n",
            "Epoch [9/100], Step [1000/2800], Loss: 0.3523\n",
            "Epoch [9/100], Step [1100/2800], Loss: 0.0939\n",
            "Epoch [9/100], Step [1200/2800], Loss: 0.0469\n",
            "Epoch [9/100], Step [1300/2800], Loss: 0.3470\n",
            "Epoch [9/100], Step [1400/2800], Loss: 0.3033\n",
            "Epoch [9/100], Step [1500/2800], Loss: 1.4096\n",
            "Epoch [9/100], Step [1600/2800], Loss: 0.1440\n",
            "Epoch [9/100], Step [1700/2800], Loss: 0.1969\n",
            "Epoch [9/100], Step [1800/2800], Loss: 0.0240\n",
            "Epoch [9/100], Step [1900/2800], Loss: 0.0667\n",
            "Epoch [9/100], Step [2000/2800], Loss: 0.0699\n",
            "Epoch [9/100], Step [2100/2800], Loss: 0.5729\n",
            "Epoch [9/100], Step [2200/2800], Loss: 1.2480\n",
            "Epoch [9/100], Step [2300/2800], Loss: 0.1978\n",
            "Epoch [9/100], Step [2400/2800], Loss: 0.0514\n",
            "Epoch [9/100], Step [2500/2800], Loss: 0.2715\n",
            "Epoch [9/100], Step [2600/2800], Loss: 1.2093\n",
            "Epoch [9/100], Step [2700/2800], Loss: 0.1587\n",
            "Epoch [9/100], Step [2800/2800], Loss: 0.9809\n",
            "Epoch [10/100], Step [100/2800], Loss: 0.0138\n",
            "Epoch [10/100], Step [200/2800], Loss: 0.0376\n",
            "Epoch [10/100], Step [300/2800], Loss: 0.0888\n",
            "Epoch [10/100], Step [400/2800], Loss: 0.2711\n",
            "Epoch [10/100], Step [500/2800], Loss: 0.1381\n",
            "Epoch [10/100], Step [600/2800], Loss: 0.0168\n",
            "Epoch [10/100], Step [700/2800], Loss: 0.0439\n",
            "Epoch [10/100], Step [800/2800], Loss: 0.0182\n",
            "Epoch [10/100], Step [900/2800], Loss: 0.0642\n",
            "Epoch [10/100], Step [1000/2800], Loss: 0.3362\n",
            "Epoch [10/100], Step [1100/2800], Loss: 0.0510\n",
            "Epoch [10/100], Step [1200/2800], Loss: 0.1433\n",
            "Epoch [10/100], Step [1300/2800], Loss: 0.3400\n",
            "Epoch [10/100], Step [1400/2800], Loss: 0.8868\n",
            "Epoch [10/100], Step [1500/2800], Loss: 1.1709\n",
            "Epoch [10/100], Step [1600/2800], Loss: 0.0835\n",
            "Epoch [10/100], Step [1700/2800], Loss: 0.1114\n",
            "Epoch [10/100], Step [1800/2800], Loss: 0.0174\n",
            "Epoch [10/100], Step [1900/2800], Loss: 0.1544\n",
            "Epoch [10/100], Step [2000/2800], Loss: 0.0679\n",
            "Epoch [10/100], Step [2100/2800], Loss: 0.7543\n",
            "Epoch [10/100], Step [2200/2800], Loss: 0.8698\n",
            "Epoch [10/100], Step [2300/2800], Loss: 0.1584\n",
            "Epoch [10/100], Step [2400/2800], Loss: 0.0595\n",
            "Epoch [10/100], Step [2500/2800], Loss: 0.9114\n",
            "Epoch [10/100], Step [2600/2800], Loss: 1.4248\n",
            "Epoch [10/100], Step [2700/2800], Loss: 0.0414\n",
            "Epoch [10/100], Step [2800/2800], Loss: 1.2513\n",
            "Epoch [11/100], Step [100/2800], Loss: 0.0130\n",
            "Epoch [11/100], Step [200/2800], Loss: 0.0687\n",
            "Epoch [11/100], Step [300/2800], Loss: 0.0524\n",
            "Epoch [11/100], Step [400/2800], Loss: 0.1402\n",
            "Epoch [11/100], Step [500/2800], Loss: 0.2113\n",
            "Epoch [11/100], Step [600/2800], Loss: 0.0195\n",
            "Epoch [11/100], Step [700/2800], Loss: 0.1855\n",
            "Epoch [11/100], Step [800/2800], Loss: 0.0169\n",
            "Epoch [11/100], Step [900/2800], Loss: 0.0904\n",
            "Epoch [11/100], Step [1000/2800], Loss: 0.2141\n",
            "Epoch [11/100], Step [1100/2800], Loss: 0.0621\n",
            "Epoch [11/100], Step [1200/2800], Loss: 0.1853\n",
            "Epoch [11/100], Step [1300/2800], Loss: 0.2266\n",
            "Epoch [11/100], Step [1400/2800], Loss: 0.4623\n",
            "Epoch [11/100], Step [1500/2800], Loss: 1.1804\n",
            "Epoch [11/100], Step [1600/2800], Loss: 0.0735\n",
            "Epoch [11/100], Step [1700/2800], Loss: 0.1853\n",
            "Epoch [11/100], Step [1800/2800], Loss: 0.0498\n",
            "Epoch [11/100], Step [1900/2800], Loss: 0.1341\n",
            "Epoch [11/100], Step [2000/2800], Loss: 0.0551\n",
            "Epoch [11/100], Step [2100/2800], Loss: 0.7523\n",
            "Epoch [11/100], Step [2200/2800], Loss: 0.8040\n",
            "Epoch [11/100], Step [2300/2800], Loss: 0.1317\n",
            "Epoch [11/100], Step [2400/2800], Loss: 0.0954\n",
            "Epoch [11/100], Step [2500/2800], Loss: 0.4846\n",
            "Epoch [11/100], Step [2600/2800], Loss: 0.5033\n",
            "Epoch [11/100], Step [2700/2800], Loss: 0.0606\n",
            "Epoch [11/100], Step [2800/2800], Loss: 0.9976\n",
            "Epoch [12/100], Step [100/2800], Loss: 0.0047\n",
            "Epoch [12/100], Step [200/2800], Loss: 0.0136\n",
            "Epoch [12/100], Step [300/2800], Loss: 0.1370\n",
            "Epoch [12/100], Step [400/2800], Loss: 0.1167\n",
            "Epoch [12/100], Step [500/2800], Loss: 0.2275\n",
            "Epoch [12/100], Step [600/2800], Loss: 0.0010\n",
            "Epoch [12/100], Step [700/2800], Loss: 0.0377\n",
            "Epoch [12/100], Step [800/2800], Loss: 0.0371\n",
            "Epoch [12/100], Step [900/2800], Loss: 0.0149\n",
            "Epoch [12/100], Step [1000/2800], Loss: 0.3856\n",
            "Epoch [12/100], Step [1100/2800], Loss: 0.0845\n",
            "Epoch [12/100], Step [1200/2800], Loss: 0.0654\n",
            "Epoch [12/100], Step [1300/2800], Loss: 0.8281\n",
            "Epoch [12/100], Step [1400/2800], Loss: 0.2445\n",
            "Epoch [12/100], Step [1500/2800], Loss: 1.4177\n",
            "Epoch [12/100], Step [1600/2800], Loss: 0.0711\n",
            "Epoch [12/100], Step [1700/2800], Loss: 0.2516\n",
            "Epoch [12/100], Step [1800/2800], Loss: 0.0724\n",
            "Epoch [12/100], Step [1900/2800], Loss: 0.1358\n",
            "Epoch [12/100], Step [2000/2800], Loss: 0.0122\n",
            "Epoch [12/100], Step [2100/2800], Loss: 0.6617\n",
            "Epoch [12/100], Step [2200/2800], Loss: 0.6660\n",
            "Epoch [12/100], Step [2300/2800], Loss: 0.0611\n",
            "Epoch [12/100], Step [2400/2800], Loss: 0.0205\n",
            "Epoch [12/100], Step [2500/2800], Loss: 0.3443\n",
            "Epoch [12/100], Step [2600/2800], Loss: 0.7367\n",
            "Epoch [12/100], Step [2700/2800], Loss: 0.0157\n",
            "Epoch [12/100], Step [2800/2800], Loss: 0.4606\n",
            "Epoch [13/100], Step [100/2800], Loss: 0.0035\n",
            "Epoch [13/100], Step [200/2800], Loss: 0.0224\n",
            "Epoch [13/100], Step [300/2800], Loss: 0.1235\n",
            "Epoch [13/100], Step [400/2800], Loss: 0.1685\n",
            "Epoch [13/100], Step [500/2800], Loss: 0.6075\n",
            "Epoch [13/100], Step [600/2800], Loss: 0.0035\n",
            "Epoch [13/100], Step [700/2800], Loss: 0.0882\n",
            "Epoch [13/100], Step [800/2800], Loss: 0.0065\n",
            "Epoch [13/100], Step [900/2800], Loss: 0.0216\n",
            "Epoch [13/100], Step [1000/2800], Loss: 0.3146\n",
            "Epoch [13/100], Step [1100/2800], Loss: 0.0630\n",
            "Epoch [13/100], Step [1200/2800], Loss: 0.1342\n",
            "Epoch [13/100], Step [1300/2800], Loss: 0.2260\n",
            "Epoch [13/100], Step [1400/2800], Loss: 0.2954\n",
            "Epoch [13/100], Step [1500/2800], Loss: 0.7821\n",
            "Epoch [13/100], Step [1600/2800], Loss: 0.0523\n",
            "Epoch [13/100], Step [1700/2800], Loss: 0.5081\n",
            "Epoch [13/100], Step [1800/2800], Loss: 0.0236\n",
            "Epoch [13/100], Step [1900/2800], Loss: 0.0345\n",
            "Epoch [13/100], Step [2000/2800], Loss: 0.0478\n",
            "Epoch [13/100], Step [2100/2800], Loss: 0.9713\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [13/100], Step [2200/2800], Loss: 0.2933\n",
            "Epoch [13/100], Step [2300/2800], Loss: 0.0318\n",
            "Epoch [13/100], Step [2400/2800], Loss: 0.0682\n",
            "Epoch [13/100], Step [2500/2800], Loss: 0.0596\n",
            "Epoch [13/100], Step [2600/2800], Loss: 0.3637\n",
            "Epoch [13/100], Step [2700/2800], Loss: 0.0257\n",
            "Epoch [13/100], Step [2800/2800], Loss: 0.5216\n",
            "Epoch [14/100], Step [100/2800], Loss: 0.0157\n",
            "Epoch [14/100], Step [200/2800], Loss: 0.0086\n",
            "Epoch [14/100], Step [300/2800], Loss: 0.1278\n",
            "Epoch [14/100], Step [400/2800], Loss: 0.0740\n",
            "Epoch [14/100], Step [500/2800], Loss: 0.3028\n",
            "Epoch [14/100], Step [600/2800], Loss: 0.0014\n",
            "Epoch [14/100], Step [700/2800], Loss: 0.0119\n",
            "Epoch [14/100], Step [800/2800], Loss: 0.0104\n",
            "Epoch [14/100], Step [900/2800], Loss: 0.0128\n",
            "Epoch [14/100], Step [1000/2800], Loss: 0.4439\n",
            "Epoch [14/100], Step [1100/2800], Loss: 0.0063\n",
            "Epoch [14/100], Step [1200/2800], Loss: 0.2552\n",
            "Epoch [14/100], Step [1300/2800], Loss: 0.1820\n",
            "Epoch [14/100], Step [1400/2800], Loss: 0.5587\n",
            "Epoch [14/100], Step [1500/2800], Loss: 0.8574\n",
            "Epoch [14/100], Step [1600/2800], Loss: 0.0830\n",
            "Epoch [14/100], Step [1700/2800], Loss: 0.4363\n",
            "Epoch [14/100], Step [1800/2800], Loss: 0.0336\n",
            "Epoch [14/100], Step [1900/2800], Loss: 0.0078\n",
            "Epoch [14/100], Step [2000/2800], Loss: 0.2743\n",
            "Epoch [14/100], Step [2100/2800], Loss: 0.7223\n",
            "Epoch [14/100], Step [2200/2800], Loss: 0.2454\n",
            "Epoch [14/100], Step [2300/2800], Loss: 0.0360\n",
            "Epoch [14/100], Step [2400/2800], Loss: 0.0156\n",
            "Epoch [14/100], Step [2500/2800], Loss: 0.0197\n",
            "Epoch [14/100], Step [2600/2800], Loss: 0.1110\n",
            "Epoch [14/100], Step [2700/2800], Loss: 0.0276\n",
            "Epoch [14/100], Step [2800/2800], Loss: 0.2099\n",
            "Epoch [15/100], Step [100/2800], Loss: 0.0052\n",
            "Epoch [15/100], Step [200/2800], Loss: 0.0071\n",
            "Epoch [15/100], Step [300/2800], Loss: 0.2379\n",
            "Epoch [15/100], Step [400/2800], Loss: 0.1182\n",
            "Epoch [15/100], Step [500/2800], Loss: 0.1381\n",
            "Epoch [15/100], Step [600/2800], Loss: 0.0013\n",
            "Epoch [15/100], Step [700/2800], Loss: 0.0669\n",
            "Epoch [15/100], Step [800/2800], Loss: 0.0577\n",
            "Epoch [15/100], Step [900/2800], Loss: 0.0549\n",
            "Epoch [15/100], Step [1000/2800], Loss: 0.1049\n",
            "Epoch [15/100], Step [1100/2800], Loss: 0.0061\n",
            "Epoch [15/100], Step [1200/2800], Loss: 0.1278\n",
            "Epoch [15/100], Step [1300/2800], Loss: 0.7671\n",
            "Epoch [15/100], Step [1400/2800], Loss: 0.2229\n",
            "Epoch [15/100], Step [1500/2800], Loss: 0.9032\n",
            "Epoch [15/100], Step [1600/2800], Loss: 0.0187\n",
            "Epoch [15/100], Step [1700/2800], Loss: 0.0611\n",
            "Epoch [15/100], Step [1800/2800], Loss: 0.0379\n",
            "Epoch [15/100], Step [1900/2800], Loss: 0.0775\n",
            "Epoch [15/100], Step [2000/2800], Loss: 0.0101\n",
            "Epoch [15/100], Step [2100/2800], Loss: 0.9962\n",
            "Epoch [15/100], Step [2200/2800], Loss: 0.1361\n",
            "Epoch [15/100], Step [2300/2800], Loss: 0.0283\n",
            "Epoch [15/100], Step [2400/2800], Loss: 0.0624\n",
            "Epoch [15/100], Step [2500/2800], Loss: 0.0535\n",
            "Epoch [15/100], Step [2600/2800], Loss: 0.4265\n",
            "Epoch [15/100], Step [2700/2800], Loss: 1.4656\n",
            "Epoch [15/100], Step [2800/2800], Loss: 1.3932\n",
            "Epoch [16/100], Step [100/2800], Loss: 0.0053\n",
            "Epoch [16/100], Step [200/2800], Loss: 0.0109\n",
            "Epoch [16/100], Step [300/2800], Loss: 0.0927\n",
            "Epoch [16/100], Step [400/2800], Loss: 0.0583\n",
            "Epoch [16/100], Step [500/2800], Loss: 0.1867\n",
            "Epoch [16/100], Step [600/2800], Loss: 0.0044\n",
            "Epoch [16/100], Step [700/2800], Loss: 0.0142\n",
            "Epoch [16/100], Step [800/2800], Loss: 0.0086\n",
            "Epoch [16/100], Step [900/2800], Loss: 0.0859\n",
            "Epoch [16/100], Step [1000/2800], Loss: 0.1449\n",
            "Epoch [16/100], Step [1100/2800], Loss: 0.0076\n",
            "Epoch [16/100], Step [1200/2800], Loss: 0.2372\n",
            "Epoch [16/100], Step [1300/2800], Loss: 0.0593\n",
            "Epoch [16/100], Step [1400/2800], Loss: 0.2107\n",
            "Epoch [16/100], Step [1500/2800], Loss: 0.4272\n",
            "Epoch [16/100], Step [1600/2800], Loss: 0.0448\n",
            "Epoch [16/100], Step [1700/2800], Loss: 0.2411\n",
            "Epoch [16/100], Step [1800/2800], Loss: 0.0192\n",
            "Epoch [16/100], Step [1900/2800], Loss: 0.0817\n",
            "Epoch [16/100], Step [2000/2800], Loss: 0.1081\n",
            "Epoch [16/100], Step [2100/2800], Loss: 0.5492\n",
            "Epoch [16/100], Step [2200/2800], Loss: 0.1436\n",
            "Epoch [16/100], Step [2300/2800], Loss: 0.0492\n",
            "Epoch [16/100], Step [2400/2800], Loss: 0.0618\n",
            "Epoch [16/100], Step [2500/2800], Loss: 0.0368\n",
            "Epoch [16/100], Step [2600/2800], Loss: 0.6574\n",
            "Epoch [16/100], Step [2700/2800], Loss: 0.0004\n",
            "Epoch [16/100], Step [2800/2800], Loss: 0.2574\n",
            "Epoch [17/100], Step [100/2800], Loss: 0.0042\n",
            "Epoch [17/100], Step [200/2800], Loss: 0.0005\n",
            "Epoch [17/100], Step [300/2800], Loss: 0.0800\n",
            "Epoch [17/100], Step [400/2800], Loss: 0.0891\n",
            "Epoch [17/100], Step [500/2800], Loss: 0.0874\n",
            "Epoch [17/100], Step [600/2800], Loss: 0.0027\n",
            "Epoch [17/100], Step [700/2800], Loss: 0.1453\n",
            "Epoch [17/100], Step [800/2800], Loss: 0.0077\n",
            "Epoch [17/100], Step [900/2800], Loss: 0.0376\n",
            "Epoch [17/100], Step [1000/2800], Loss: 0.0223\n",
            "Epoch [17/100], Step [1100/2800], Loss: 0.0354\n",
            "Epoch [17/100], Step [1200/2800], Loss: 0.0329\n",
            "Epoch [17/100], Step [1300/2800], Loss: 0.0693\n",
            "Epoch [17/100], Step [1400/2800], Loss: 0.4135\n",
            "Epoch [17/100], Step [1500/2800], Loss: 1.1004\n",
            "Epoch [17/100], Step [1600/2800], Loss: 0.0349\n",
            "Epoch [17/100], Step [1700/2800], Loss: 0.2892\n",
            "Epoch [17/100], Step [1800/2800], Loss: 0.0051\n",
            "Epoch [17/100], Step [1900/2800], Loss: 0.0635\n",
            "Epoch [17/100], Step [2000/2800], Loss: 0.0284\n",
            "Epoch [17/100], Step [2100/2800], Loss: 0.5587\n",
            "Epoch [17/100], Step [2200/2800], Loss: 0.0998\n",
            "Epoch [17/100], Step [2300/2800], Loss: 0.0043\n",
            "Epoch [17/100], Step [2400/2800], Loss: 0.0104\n",
            "Epoch [17/100], Step [2500/2800], Loss: 0.0113\n",
            "Epoch [17/100], Step [2600/2800], Loss: 0.0849\n",
            "Epoch [17/100], Step [2700/2800], Loss: 0.0000\n",
            "Epoch [17/100], Step [2800/2800], Loss: 0.0807\n",
            "Epoch [18/100], Step [100/2800], Loss: 0.0197\n",
            "Epoch [18/100], Step [200/2800], Loss: 0.0166\n",
            "Epoch [18/100], Step [300/2800], Loss: 0.0170\n",
            "Epoch [18/100], Step [400/2800], Loss: 0.2481\n",
            "Epoch [18/100], Step [500/2800], Loss: 0.0112\n",
            "Epoch [18/100], Step [600/2800], Loss: 0.0003\n",
            "Epoch [18/100], Step [700/2800], Loss: 0.3766\n",
            "Epoch [18/100], Step [800/2800], Loss: 0.0027\n",
            "Epoch [18/100], Step [900/2800], Loss: 0.0823\n",
            "Epoch [18/100], Step [1000/2800], Loss: 0.0398\n",
            "Epoch [18/100], Step [1100/2800], Loss: 0.0819\n",
            "Epoch [18/100], Step [1200/2800], Loss: 0.3431\n",
            "Epoch [18/100], Step [1300/2800], Loss: 0.3408\n",
            "Epoch [18/100], Step [1400/2800], Loss: 0.3046\n",
            "Epoch [18/100], Step [1500/2800], Loss: 0.9110\n",
            "Epoch [18/100], Step [1600/2800], Loss: 0.0442\n",
            "Epoch [18/100], Step [1700/2800], Loss: 0.1460\n",
            "Epoch [18/100], Step [1800/2800], Loss: 0.0037\n",
            "Epoch [18/100], Step [1900/2800], Loss: 0.0911\n",
            "Epoch [18/100], Step [2000/2800], Loss: 0.0149\n",
            "Epoch [18/100], Step [2100/2800], Loss: 0.7100\n",
            "Epoch [18/100], Step [2200/2800], Loss: 0.1674\n",
            "Epoch [18/100], Step [2300/2800], Loss: 0.0076\n",
            "Epoch [18/100], Step [2400/2800], Loss: 0.1509\n",
            "Epoch [18/100], Step [2500/2800], Loss: 0.0217\n",
            "Epoch [18/100], Step [2600/2800], Loss: 0.1546\n",
            "Epoch [18/100], Step [2700/2800], Loss: 0.0001\n",
            "Epoch [18/100], Step [2800/2800], Loss: 0.1545\n",
            "Epoch [19/100], Step [100/2800], Loss: 0.0005\n",
            "Epoch [19/100], Step [200/2800], Loss: 0.0167\n",
            "Epoch [19/100], Step [300/2800], Loss: 0.0323\n",
            "Epoch [19/100], Step [400/2800], Loss: 0.2029\n",
            "Epoch [19/100], Step [500/2800], Loss: 0.1042\n",
            "Epoch [19/100], Step [600/2800], Loss: 0.0012\n",
            "Epoch [19/100], Step [700/2800], Loss: 0.0819\n",
            "Epoch [19/100], Step [800/2800], Loss: 0.0013\n",
            "Epoch [19/100], Step [900/2800], Loss: 0.2570\n",
            "Epoch [19/100], Step [1000/2800], Loss: 0.1218\n",
            "Epoch [19/100], Step [1100/2800], Loss: 0.0467\n",
            "Epoch [19/100], Step [1200/2800], Loss: 0.0334\n",
            "Epoch [19/100], Step [1300/2800], Loss: 0.0084\n",
            "Epoch [19/100], Step [1400/2800], Loss: 0.2130\n",
            "Epoch [19/100], Step [1500/2800], Loss: 0.1717\n",
            "Epoch [19/100], Step [1600/2800], Loss: 0.0110\n",
            "Epoch [19/100], Step [1700/2800], Loss: 0.1626\n",
            "Epoch [19/100], Step [1800/2800], Loss: 0.0528\n",
            "Epoch [19/100], Step [1900/2800], Loss: 0.0398\n",
            "Epoch [19/100], Step [2000/2800], Loss: 0.0236\n",
            "Epoch [19/100], Step [2100/2800], Loss: 1.1850\n",
            "Epoch [19/100], Step [2200/2800], Loss: 0.1285\n",
            "Epoch [19/100], Step [2300/2800], Loss: 0.0003\n",
            "Epoch [19/100], Step [2400/2800], Loss: 0.0128\n",
            "Epoch [19/100], Step [2500/2800], Loss: 0.0094\n",
            "Epoch [19/100], Step [2600/2800], Loss: 0.2477\n",
            "Epoch [19/100], Step [2700/2800], Loss: 0.0004\n",
            "Epoch [19/100], Step [2800/2800], Loss: 0.1658\n",
            "Epoch [20/100], Step [100/2800], Loss: 0.0008\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [20/100], Step [200/2800], Loss: 0.0092\n",
            "Epoch [20/100], Step [300/2800], Loss: 0.1775\n",
            "Epoch [20/100], Step [400/2800], Loss: 0.3749\n",
            "Epoch [20/100], Step [500/2800], Loss: 0.0341\n",
            "Epoch [20/100], Step [600/2800], Loss: 0.0001\n",
            "Epoch [20/100], Step [700/2800], Loss: 0.0455\n",
            "Epoch [20/100], Step [800/2800], Loss: 0.0003\n",
            "Epoch [20/100], Step [900/2800], Loss: 0.1294\n",
            "Epoch [20/100], Step [1000/2800], Loss: 0.0797\n",
            "Epoch [20/100], Step [1100/2800], Loss: 0.0691\n",
            "Epoch [20/100], Step [1200/2800], Loss: 0.0313\n",
            "Epoch [20/100], Step [1300/2800], Loss: 0.0038\n",
            "Epoch [20/100], Step [1400/2800], Loss: 0.3181\n",
            "Epoch [20/100], Step [1500/2800], Loss: 0.1854\n",
            "Epoch [20/100], Step [1600/2800], Loss: 0.0569\n",
            "Epoch [20/100], Step [1700/2800], Loss: 0.2814\n",
            "Epoch [20/100], Step [1800/2800], Loss: 0.0018\n",
            "Epoch [20/100], Step [1900/2800], Loss: 0.1195\n",
            "Epoch [20/100], Step [2000/2800], Loss: 0.0333\n",
            "Epoch [20/100], Step [2100/2800], Loss: 0.5440\n",
            "Epoch [20/100], Step [2200/2800], Loss: 0.2177\n",
            "Epoch [20/100], Step [2300/2800], Loss: 0.0012\n",
            "Epoch [20/100], Step [2400/2800], Loss: 0.0328\n",
            "Epoch [20/100], Step [2500/2800], Loss: 0.0126\n",
            "Epoch [20/100], Step [2600/2800], Loss: 0.1218\n",
            "Epoch [20/100], Step [2700/2800], Loss: 0.0000\n",
            "Epoch [20/100], Step [2800/2800], Loss: 0.0042\n",
            "Epoch [21/100], Step [100/2800], Loss: 0.0008\n",
            "Epoch [21/100], Step [200/2800], Loss: 0.0016\n",
            "Epoch [21/100], Step [300/2800], Loss: 0.0762\n",
            "Epoch [21/100], Step [400/2800], Loss: 0.0662\n",
            "Epoch [21/100], Step [500/2800], Loss: 0.1385\n",
            "Epoch [21/100], Step [600/2800], Loss: 0.0000\n",
            "Epoch [21/100], Step [700/2800], Loss: 0.0264\n",
            "Epoch [21/100], Step [800/2800], Loss: 0.0017\n",
            "Epoch [21/100], Step [900/2800], Loss: 0.0800\n",
            "Epoch [21/100], Step [1000/2800], Loss: 0.2338\n",
            "Epoch [21/100], Step [1100/2800], Loss: 0.0413\n",
            "Epoch [21/100], Step [1200/2800], Loss: 0.0355\n",
            "Epoch [21/100], Step [1300/2800], Loss: 0.0022\n",
            "Epoch [21/100], Step [1400/2800], Loss: 0.5989\n",
            "Epoch [21/100], Step [1500/2800], Loss: 0.0443\n",
            "Epoch [21/100], Step [1600/2800], Loss: 0.1481\n",
            "Epoch [21/100], Step [1700/2800], Loss: 0.1295\n",
            "Epoch [21/100], Step [1800/2800], Loss: 0.0648\n",
            "Epoch [21/100], Step [1900/2800], Loss: 0.0752\n",
            "Epoch [21/100], Step [2000/2800], Loss: 0.0079\n",
            "Epoch [21/100], Step [2100/2800], Loss: 0.2848\n",
            "Epoch [21/100], Step [2200/2800], Loss: 0.1248\n",
            "Epoch [21/100], Step [2300/2800], Loss: 0.0004\n",
            "Epoch [21/100], Step [2400/2800], Loss: 0.0180\n",
            "Epoch [21/100], Step [2500/2800], Loss: 0.0030\n",
            "Epoch [21/100], Step [2600/2800], Loss: 0.1386\n",
            "Epoch [21/100], Step [2700/2800], Loss: 0.0001\n",
            "Epoch [21/100], Step [2800/2800], Loss: 0.1055\n",
            "Epoch [22/100], Step [100/2800], Loss: 0.0041\n",
            "Epoch [22/100], Step [200/2800], Loss: 0.0001\n",
            "Epoch [22/100], Step [300/2800], Loss: 0.0219\n",
            "Epoch [22/100], Step [400/2800], Loss: 0.1890\n",
            "Epoch [22/100], Step [500/2800], Loss: 0.0043\n",
            "Epoch [22/100], Step [600/2800], Loss: 0.0001\n",
            "Epoch [22/100], Step [700/2800], Loss: 0.0220\n",
            "Epoch [22/100], Step [800/2800], Loss: 0.0057\n",
            "Epoch [22/100], Step [900/2800], Loss: 0.4732\n",
            "Epoch [22/100], Step [1000/2800], Loss: 0.3358\n",
            "Epoch [22/100], Step [1100/2800], Loss: 0.0096\n",
            "Epoch [22/100], Step [1200/2800], Loss: 0.0064\n",
            "Epoch [22/100], Step [1300/2800], Loss: 0.0702\n",
            "Epoch [22/100], Step [1400/2800], Loss: 0.0090\n",
            "Epoch [22/100], Step [1500/2800], Loss: 0.1111\n",
            "Epoch [22/100], Step [1600/2800], Loss: 0.0153\n",
            "Epoch [22/100], Step [1700/2800], Loss: 0.0046\n",
            "Epoch [22/100], Step [1800/2800], Loss: 0.0221\n",
            "Epoch [22/100], Step [1900/2800], Loss: 0.0099\n",
            "Epoch [22/100], Step [2000/2800], Loss: 0.0028\n",
            "Epoch [22/100], Step [2100/2800], Loss: 0.2373\n",
            "Epoch [22/100], Step [2200/2800], Loss: 0.1604\n",
            "Epoch [22/100], Step [2300/2800], Loss: 0.0019\n",
            "Epoch [22/100], Step [2400/2800], Loss: 0.0047\n",
            "Epoch [22/100], Step [2500/2800], Loss: 0.0085\n",
            "Epoch [22/100], Step [2600/2800], Loss: 0.0923\n",
            "Epoch [22/100], Step [2700/2800], Loss: 0.0000\n",
            "Epoch [22/100], Step [2800/2800], Loss: 0.0137\n",
            "Epoch [23/100], Step [100/2800], Loss: 0.0004\n",
            "Epoch [23/100], Step [200/2800], Loss: 0.0085\n",
            "Epoch [23/100], Step [300/2800], Loss: 0.1202\n",
            "Epoch [23/100], Step [400/2800], Loss: 0.0055\n",
            "Epoch [23/100], Step [500/2800], Loss: 0.0344\n",
            "Epoch [23/100], Step [600/2800], Loss: 0.0003\n",
            "Epoch [23/100], Step [700/2800], Loss: 0.0438\n",
            "Epoch [23/100], Step [800/2800], Loss: 0.0025\n",
            "Epoch [23/100], Step [900/2800], Loss: 0.0957\n",
            "Epoch [23/100], Step [1000/2800], Loss: 0.0823\n",
            "Epoch [23/100], Step [1100/2800], Loss: 0.0048\n",
            "Epoch [23/100], Step [1200/2800], Loss: 0.0088\n",
            "Epoch [23/100], Step [1300/2800], Loss: 0.0437\n",
            "Epoch [23/100], Step [1400/2800], Loss: 0.1656\n",
            "Epoch [23/100], Step [1500/2800], Loss: 0.7790\n",
            "Epoch [23/100], Step [1600/2800], Loss: 0.0015\n",
            "Epoch [23/100], Step [1700/2800], Loss: 0.0583\n",
            "Epoch [23/100], Step [1800/2800], Loss: 0.0360\n",
            "Epoch [23/100], Step [1900/2800], Loss: 0.0026\n",
            "Epoch [23/100], Step [2000/2800], Loss: 0.0037\n",
            "Epoch [23/100], Step [2100/2800], Loss: 0.1100\n",
            "Epoch [23/100], Step [2200/2800], Loss: 0.1422\n",
            "Epoch [23/100], Step [2300/2800], Loss: 0.0003\n",
            "Epoch [23/100], Step [2400/2800], Loss: 0.0030\n",
            "Epoch [23/100], Step [2500/2800], Loss: 0.0171\n",
            "Epoch [23/100], Step [2600/2800], Loss: 0.0463\n",
            "Epoch [23/100], Step [2700/2800], Loss: 0.0002\n",
            "Epoch [23/100], Step [2800/2800], Loss: 0.0263\n",
            "Epoch [24/100], Step [100/2800], Loss: 0.0001\n",
            "Epoch [24/100], Step [200/2800], Loss: 0.0048\n",
            "Epoch [24/100], Step [300/2800], Loss: 0.0053\n",
            "Epoch [24/100], Step [400/2800], Loss: 0.0920\n",
            "Epoch [24/100], Step [500/2800], Loss: 0.0859\n",
            "Epoch [24/100], Step [600/2800], Loss: 0.0000\n",
            "Epoch [24/100], Step [700/2800], Loss: 0.0274\n",
            "Epoch [24/100], Step [800/2800], Loss: 0.0006\n",
            "Epoch [24/100], Step [900/2800], Loss: 0.0754\n",
            "Epoch [24/100], Step [1000/2800], Loss: 0.0100\n",
            "Epoch [24/100], Step [1100/2800], Loss: 0.0628\n",
            "Epoch [24/100], Step [1200/2800], Loss: 0.2366\n",
            "Epoch [24/100], Step [1300/2800], Loss: 0.0091\n",
            "Epoch [24/100], Step [1400/2800], Loss: 0.1984\n",
            "Epoch [24/100], Step [1500/2800], Loss: 0.4724\n",
            "Epoch [24/100], Step [1600/2800], Loss: 0.0023\n",
            "Epoch [24/100], Step [1700/2800], Loss: 0.0788\n",
            "Epoch [24/100], Step [1800/2800], Loss: 0.0009\n",
            "Epoch [24/100], Step [1900/2800], Loss: 0.0131\n",
            "Epoch [24/100], Step [2000/2800], Loss: 0.0075\n",
            "Epoch [24/100], Step [2100/2800], Loss: 0.0582\n",
            "Epoch [24/100], Step [2200/2800], Loss: 0.1298\n",
            "Epoch [24/100], Step [2300/2800], Loss: 0.0054\n",
            "Epoch [24/100], Step [2400/2800], Loss: 0.0040\n",
            "Epoch [24/100], Step [2500/2800], Loss: 0.0031\n",
            "Epoch [24/100], Step [2600/2800], Loss: 0.3018\n",
            "Epoch [24/100], Step [2700/2800], Loss: 0.0000\n",
            "Epoch [24/100], Step [2800/2800], Loss: 0.0014\n",
            "Epoch [25/100], Step [100/2800], Loss: 0.0006\n",
            "Epoch [25/100], Step [200/2800], Loss: 0.0001\n",
            "Epoch [25/100], Step [300/2800], Loss: 0.0045\n",
            "Epoch [25/100], Step [400/2800], Loss: 0.0145\n",
            "Epoch [25/100], Step [500/2800], Loss: 0.2166\n",
            "Epoch [25/100], Step [600/2800], Loss: 0.0000\n",
            "Epoch [25/100], Step [700/2800], Loss: 0.0008\n",
            "Epoch [25/100], Step [800/2800], Loss: 0.0070\n",
            "Epoch [25/100], Step [900/2800], Loss: 0.0087\n",
            "Epoch [25/100], Step [1000/2800], Loss: 0.2269\n",
            "Epoch [25/100], Step [1100/2800], Loss: 0.0071\n",
            "Epoch [25/100], Step [1200/2800], Loss: 0.0080\n",
            "Epoch [25/100], Step [1300/2800], Loss: 0.2879\n",
            "Epoch [25/100], Step [1400/2800], Loss: 0.1635\n",
            "Epoch [25/100], Step [1500/2800], Loss: 0.0404\n",
            "Epoch [25/100], Step [1600/2800], Loss: 0.1125\n",
            "Epoch [25/100], Step [1700/2800], Loss: 0.0051\n",
            "Epoch [25/100], Step [1800/2800], Loss: 0.0008\n",
            "Epoch [25/100], Step [1900/2800], Loss: 0.0125\n",
            "Epoch [25/100], Step [2000/2800], Loss: 0.0063\n",
            "Epoch [25/100], Step [2100/2800], Loss: 0.0772\n",
            "Epoch [25/100], Step [2200/2800], Loss: 0.1724\n",
            "Epoch [25/100], Step [2300/2800], Loss: 0.0010\n",
            "Epoch [25/100], Step [2400/2800], Loss: 0.0022\n",
            "Epoch [25/100], Step [2500/2800], Loss: 0.0084\n",
            "Epoch [25/100], Step [2600/2800], Loss: 0.1259\n",
            "Epoch [25/100], Step [2700/2800], Loss: 0.0000\n",
            "Epoch [25/100], Step [2800/2800], Loss: 0.0048\n",
            "Epoch [26/100], Step [100/2800], Loss: 0.0002\n",
            "Epoch [26/100], Step [200/2800], Loss: 0.0049\n",
            "Epoch [26/100], Step [300/2800], Loss: 0.0560\n",
            "Epoch [26/100], Step [400/2800], Loss: 0.0483\n",
            "Epoch [26/100], Step [500/2800], Loss: 0.0266\n",
            "Epoch [26/100], Step [600/2800], Loss: 0.0000\n",
            "Epoch [26/100], Step [700/2800], Loss: 0.1006\n",
            "Epoch [26/100], Step [800/2800], Loss: 0.0019\n",
            "Epoch [26/100], Step [900/2800], Loss: 0.0007\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [26/100], Step [1000/2800], Loss: 0.0777\n",
            "Epoch [26/100], Step [1100/2800], Loss: 0.0003\n",
            "Epoch [26/100], Step [1200/2800], Loss: 0.0077\n",
            "Epoch [26/100], Step [1300/2800], Loss: 0.0001\n",
            "Epoch [26/100], Step [1400/2800], Loss: 0.2032\n",
            "Epoch [26/100], Step [1500/2800], Loss: 0.0423\n",
            "Epoch [26/100], Step [1600/2800], Loss: 0.0161\n",
            "Epoch [26/100], Step [1700/2800], Loss: 0.0522\n",
            "Epoch [26/100], Step [1800/2800], Loss: 0.0002\n",
            "Epoch [26/100], Step [1900/2800], Loss: 0.0336\n",
            "Epoch [26/100], Step [2000/2800], Loss: 0.0912\n",
            "Epoch [26/100], Step [2100/2800], Loss: 0.2296\n",
            "Epoch [26/100], Step [2200/2800], Loss: 0.0312\n",
            "Epoch [26/100], Step [2300/2800], Loss: 0.0221\n",
            "Epoch [26/100], Step [2400/2800], Loss: 0.0028\n",
            "Epoch [26/100], Step [2500/2800], Loss: 0.0140\n",
            "Epoch [26/100], Step [2600/2800], Loss: 0.0374\n",
            "Epoch [26/100], Step [2700/2800], Loss: 0.0000\n",
            "Epoch [26/100], Step [2800/2800], Loss: 0.0038\n",
            "Epoch [27/100], Step [100/2800], Loss: 0.0000\n",
            "Epoch [27/100], Step [200/2800], Loss: 0.0003\n",
            "Epoch [27/100], Step [300/2800], Loss: 0.0026\n",
            "Epoch [27/100], Step [400/2800], Loss: 0.0208\n",
            "Epoch [27/100], Step [500/2800], Loss: 0.1255\n",
            "Epoch [27/100], Step [600/2800], Loss: 0.0000\n",
            "Epoch [27/100], Step [700/2800], Loss: 0.1290\n",
            "Epoch [27/100], Step [800/2800], Loss: 0.0002\n",
            "Epoch [27/100], Step [900/2800], Loss: 0.0014\n",
            "Epoch [27/100], Step [1000/2800], Loss: 0.0031\n",
            "Epoch [27/100], Step [1100/2800], Loss: 0.0047\n",
            "Epoch [27/100], Step [1200/2800], Loss: 0.0061\n",
            "Epoch [27/100], Step [1300/2800], Loss: 0.0003\n",
            "Epoch [27/100], Step [1400/2800], Loss: 0.0133\n",
            "Epoch [27/100], Step [1500/2800], Loss: 0.0154\n",
            "Epoch [27/100], Step [1600/2800], Loss: 0.0008\n",
            "Epoch [27/100], Step [1700/2800], Loss: 0.0076\n",
            "Epoch [27/100], Step [1800/2800], Loss: 0.0013\n",
            "Epoch [27/100], Step [1900/2800], Loss: 0.1130\n",
            "Epoch [27/100], Step [2000/2800], Loss: 0.0051\n",
            "Epoch [27/100], Step [2100/2800], Loss: 0.0439\n",
            "Epoch [27/100], Step [2200/2800], Loss: 0.1035\n",
            "Epoch [27/100], Step [2300/2800], Loss: 0.0027\n",
            "Epoch [27/100], Step [2400/2800], Loss: 0.0016\n",
            "Epoch [27/100], Step [2500/2800], Loss: 0.0060\n",
            "Epoch [27/100], Step [2600/2800], Loss: 0.1880\n",
            "Epoch [27/100], Step [2700/2800], Loss: 0.0001\n",
            "Epoch [27/100], Step [2800/2800], Loss: 0.0014\n",
            "Epoch [28/100], Step [100/2800], Loss: 0.0004\n",
            "Epoch [28/100], Step [200/2800], Loss: 0.0006\n",
            "Epoch [28/100], Step [300/2800], Loss: 0.0040\n",
            "Epoch [28/100], Step [400/2800], Loss: 0.2647\n",
            "Epoch [28/100], Step [500/2800], Loss: 0.0240\n",
            "Epoch [28/100], Step [600/2800], Loss: 0.0001\n",
            "Epoch [28/100], Step [700/2800], Loss: 0.0394\n",
            "Epoch [28/100], Step [800/2800], Loss: 0.0067\n",
            "Epoch [28/100], Step [900/2800], Loss: 0.0147\n",
            "Epoch [28/100], Step [1000/2800], Loss: 0.0012\n",
            "Epoch [28/100], Step [1100/2800], Loss: 0.0812\n",
            "Epoch [28/100], Step [1200/2800], Loss: 0.0091\n",
            "Epoch [28/100], Step [1300/2800], Loss: 0.0049\n",
            "Epoch [28/100], Step [1400/2800], Loss: 1.9478\n",
            "Epoch [28/100], Step [1500/2800], Loss: 0.0012\n",
            "Epoch [28/100], Step [1600/2800], Loss: 0.0027\n",
            "Epoch [28/100], Step [1700/2800], Loss: 0.3601\n",
            "Epoch [28/100], Step [1800/2800], Loss: 0.0457\n",
            "Epoch [28/100], Step [1900/2800], Loss: 0.0028\n",
            "Epoch [28/100], Step [2000/2800], Loss: 0.2458\n",
            "Epoch [28/100], Step [2100/2800], Loss: 0.0524\n",
            "Epoch [28/100], Step [2200/2800], Loss: 0.2412\n",
            "Epoch [28/100], Step [2300/2800], Loss: 0.0098\n",
            "Epoch [28/100], Step [2400/2800], Loss: 0.0000\n",
            "Epoch [28/100], Step [2500/2800], Loss: 0.0200\n",
            "Epoch [28/100], Step [2600/2800], Loss: 0.0100\n",
            "Epoch [28/100], Step [2700/2800], Loss: 0.0004\n",
            "Epoch [28/100], Step [2800/2800], Loss: 0.0014\n",
            "Epoch [29/100], Step [100/2800], Loss: 0.0009\n",
            "Epoch [29/100], Step [200/2800], Loss: 0.0003\n",
            "Epoch [29/100], Step [300/2800], Loss: 0.1439\n",
            "Epoch [29/100], Step [400/2800], Loss: 0.0075\n",
            "Epoch [29/100], Step [500/2800], Loss: 0.2125\n",
            "Epoch [29/100], Step [600/2800], Loss: 0.0000\n",
            "Epoch [29/100], Step [700/2800], Loss: 0.0282\n",
            "Epoch [29/100], Step [800/2800], Loss: 0.0004\n",
            "Epoch [29/100], Step [900/2800], Loss: 0.0036\n",
            "Epoch [29/100], Step [1000/2800], Loss: 0.0082\n",
            "Epoch [29/100], Step [1100/2800], Loss: 0.0130\n",
            "Epoch [29/100], Step [1200/2800], Loss: 0.0248\n",
            "Epoch [29/100], Step [1300/2800], Loss: 0.0009\n",
            "Epoch [29/100], Step [1400/2800], Loss: 0.0241\n",
            "Epoch [29/100], Step [1500/2800], Loss: 0.0487\n",
            "Epoch [29/100], Step [1600/2800], Loss: 0.0077\n",
            "Epoch [29/100], Step [1700/2800], Loss: 0.0519\n",
            "Epoch [29/100], Step [1800/2800], Loss: 0.0013\n",
            "Epoch [29/100], Step [1900/2800], Loss: 0.0580\n",
            "Epoch [29/100], Step [2000/2800], Loss: 0.1765\n",
            "Epoch [29/100], Step [2100/2800], Loss: 0.0278\n",
            "Epoch [29/100], Step [2200/2800], Loss: 0.0492\n",
            "Epoch [29/100], Step [2300/2800], Loss: 0.0036\n",
            "Epoch [29/100], Step [2400/2800], Loss: 0.0010\n",
            "Epoch [29/100], Step [2500/2800], Loss: 0.0056\n",
            "Epoch [29/100], Step [2600/2800], Loss: 0.0380\n",
            "Epoch [29/100], Step [2700/2800], Loss: 0.0000\n",
            "Epoch [29/100], Step [2800/2800], Loss: 0.0016\n",
            "Epoch [30/100], Step [100/2800], Loss: 0.0026\n",
            "Epoch [30/100], Step [200/2800], Loss: 0.0070\n",
            "Epoch [30/100], Step [300/2800], Loss: 0.0000\n",
            "Epoch [30/100], Step [400/2800], Loss: 0.0799\n",
            "Epoch [30/100], Step [500/2800], Loss: 0.0657\n",
            "Epoch [30/100], Step [600/2800], Loss: 0.0000\n",
            "Epoch [30/100], Step [700/2800], Loss: 0.0534\n",
            "Epoch [30/100], Step [800/2800], Loss: 0.0002\n",
            "Epoch [30/100], Step [900/2800], Loss: 0.0006\n",
            "Epoch [30/100], Step [1000/2800], Loss: 0.0033\n",
            "Epoch [30/100], Step [1100/2800], Loss: 0.0016\n",
            "Epoch [30/100], Step [1200/2800], Loss: 0.0215\n",
            "Epoch [30/100], Step [1300/2800], Loss: 0.0002\n",
            "Epoch [30/100], Step [1400/2800], Loss: 0.1026\n",
            "Epoch [30/100], Step [1500/2800], Loss: 0.0149\n",
            "Epoch [30/100], Step [1600/2800], Loss: 0.0070\n",
            "Epoch [30/100], Step [1700/2800], Loss: 0.0003\n",
            "Epoch [30/100], Step [1800/2800], Loss: 0.0680\n",
            "Epoch [30/100], Step [1900/2800], Loss: 0.0006\n",
            "Epoch [30/100], Step [2000/2800], Loss: 0.0007\n",
            "Epoch [30/100], Step [2100/2800], Loss: 0.0074\n",
            "Epoch [30/100], Step [2200/2800], Loss: 0.0988\n",
            "Epoch [30/100], Step [2300/2800], Loss: 0.0004\n",
            "Epoch [30/100], Step [2400/2800], Loss: 0.0036\n",
            "Epoch [30/100], Step [2500/2800], Loss: 0.0025\n",
            "Epoch [30/100], Step [2600/2800], Loss: 0.1056\n",
            "Epoch [30/100], Step [2700/2800], Loss: 0.0000\n",
            "Epoch [30/100], Step [2800/2800], Loss: 0.0066\n",
            "Epoch [31/100], Step [100/2800], Loss: 0.0034\n",
            "Epoch [31/100], Step [200/2800], Loss: 0.0001\n",
            "Epoch [31/100], Step [300/2800], Loss: 0.0011\n",
            "Epoch [31/100], Step [400/2800], Loss: 0.2501\n",
            "Epoch [31/100], Step [500/2800], Loss: 0.3314\n",
            "Epoch [31/100], Step [600/2800], Loss: 0.0000\n",
            "Epoch [31/100], Step [700/2800], Loss: 0.1648\n",
            "Epoch [31/100], Step [800/2800], Loss: 0.0065\n",
            "Epoch [31/100], Step [900/2800], Loss: 0.0011\n",
            "Epoch [31/100], Step [1000/2800], Loss: 0.0082\n",
            "Epoch [31/100], Step [1100/2800], Loss: 0.0009\n",
            "Epoch [31/100], Step [1200/2800], Loss: 0.0365\n",
            "Epoch [31/100], Step [1300/2800], Loss: 0.0000\n",
            "Epoch [31/100], Step [1400/2800], Loss: 0.0041\n",
            "Epoch [31/100], Step [1500/2800], Loss: 0.0134\n",
            "Epoch [31/100], Step [1600/2800], Loss: 0.0016\n",
            "Epoch [31/100], Step [1700/2800], Loss: 0.0208\n",
            "Epoch [31/100], Step [1800/2800], Loss: 0.0006\n",
            "Epoch [31/100], Step [1900/2800], Loss: 0.0000\n",
            "Epoch [31/100], Step [2000/2800], Loss: 0.0002\n",
            "Epoch [31/100], Step [2100/2800], Loss: 0.0010\n",
            "Epoch [31/100], Step [2200/2800], Loss: 0.0604\n",
            "Epoch [31/100], Step [2300/2800], Loss: 0.0033\n",
            "Epoch [31/100], Step [2400/2800], Loss: 0.0030\n",
            "Epoch [31/100], Step [2500/2800], Loss: 0.0012\n",
            "Epoch [31/100], Step [2600/2800], Loss: 0.0049\n",
            "Epoch [31/100], Step [2700/2800], Loss: 0.0000\n",
            "Epoch [31/100], Step [2800/2800], Loss: 0.0350\n",
            "Epoch [32/100], Step [100/2800], Loss: 0.0009\n",
            "Epoch [32/100], Step [200/2800], Loss: 0.0001\n",
            "Epoch [32/100], Step [300/2800], Loss: 0.0005\n",
            "Epoch [32/100], Step [400/2800], Loss: 0.0305\n",
            "Epoch [32/100], Step [500/2800], Loss: 0.0308\n",
            "Epoch [32/100], Step [600/2800], Loss: 0.0000\n",
            "Epoch [32/100], Step [700/2800], Loss: 0.5952\n",
            "Epoch [32/100], Step [800/2800], Loss: 0.0013\n",
            "Epoch [32/100], Step [900/2800], Loss: 0.0015\n",
            "Epoch [32/100], Step [1000/2800], Loss: 0.0016\n",
            "Epoch [32/100], Step [1100/2800], Loss: 0.0014\n",
            "Epoch [32/100], Step [1200/2800], Loss: 0.0037\n",
            "Epoch [32/100], Step [1300/2800], Loss: 0.0005\n",
            "Epoch [32/100], Step [1400/2800], Loss: 0.0214\n",
            "Epoch [32/100], Step [1500/2800], Loss: 0.0111\n",
            "Epoch [32/100], Step [1600/2800], Loss: 0.0189\n",
            "Epoch [32/100], Step [1700/2800], Loss: 0.0056\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [32/100], Step [1800/2800], Loss: 0.0099\n",
            "Epoch [32/100], Step [1900/2800], Loss: 0.0012\n",
            "Epoch [32/100], Step [2000/2800], Loss: 0.0002\n",
            "Epoch [32/100], Step [2100/2800], Loss: 0.1719\n",
            "Epoch [32/100], Step [2200/2800], Loss: 0.0080\n",
            "Epoch [32/100], Step [2300/2800], Loss: 0.0001\n",
            "Epoch [32/100], Step [2400/2800], Loss: 0.0201\n",
            "Epoch [32/100], Step [2500/2800], Loss: 0.0034\n",
            "Epoch [32/100], Step [2600/2800], Loss: 1.5619\n",
            "Epoch [32/100], Step [2700/2800], Loss: 0.0000\n",
            "Epoch [32/100], Step [2800/2800], Loss: 0.0012\n",
            "Epoch [33/100], Step [100/2800], Loss: 0.0011\n",
            "Epoch [33/100], Step [200/2800], Loss: 0.0006\n",
            "Epoch [33/100], Step [300/2800], Loss: 0.0025\n",
            "Epoch [33/100], Step [400/2800], Loss: 0.0103\n",
            "Epoch [33/100], Step [500/2800], Loss: 0.1567\n",
            "Epoch [33/100], Step [600/2800], Loss: 0.0040\n",
            "Epoch [33/100], Step [700/2800], Loss: 0.0259\n",
            "Epoch [33/100], Step [800/2800], Loss: 0.0005\n",
            "Epoch [33/100], Step [900/2800], Loss: 0.0013\n",
            "Epoch [33/100], Step [1000/2800], Loss: 0.1949\n",
            "Epoch [33/100], Step [1100/2800], Loss: 0.0194\n",
            "Epoch [33/100], Step [1200/2800], Loss: 0.0041\n",
            "Epoch [33/100], Step [1300/2800], Loss: 0.0000\n",
            "Epoch [33/100], Step [1400/2800], Loss: 0.0891\n",
            "Epoch [33/100], Step [1500/2800], Loss: 0.0008\n",
            "Epoch [33/100], Step [1600/2800], Loss: 0.1699\n",
            "Epoch [33/100], Step [1700/2800], Loss: 0.0078\n",
            "Epoch [33/100], Step [1800/2800], Loss: 0.0017\n",
            "Epoch [33/100], Step [1900/2800], Loss: 0.0003\n",
            "Epoch [33/100], Step [2000/2800], Loss: 0.0001\n",
            "Epoch [33/100], Step [2100/2800], Loss: 0.0056\n",
            "Epoch [33/100], Step [2200/2800], Loss: 0.0183\n",
            "Epoch [33/100], Step [2300/2800], Loss: 0.0011\n",
            "Epoch [33/100], Step [2400/2800], Loss: 0.0012\n",
            "Epoch [33/100], Step [2500/2800], Loss: 0.0013\n",
            "Epoch [33/100], Step [2600/2800], Loss: 0.0234\n",
            "Epoch [33/100], Step [2700/2800], Loss: 0.0000\n",
            "Epoch [33/100], Step [2800/2800], Loss: 0.0009\n",
            "Epoch [34/100], Step [100/2800], Loss: 0.0066\n",
            "Epoch [34/100], Step [200/2800], Loss: 0.0000\n",
            "Epoch [34/100], Step [300/2800], Loss: 0.0034\n",
            "Epoch [34/100], Step [400/2800], Loss: 0.2950\n",
            "Epoch [34/100], Step [500/2800], Loss: 0.0245\n",
            "Epoch [34/100], Step [600/2800], Loss: 0.0000\n",
            "Epoch [34/100], Step [700/2800], Loss: 0.0019\n",
            "Epoch [34/100], Step [800/2800], Loss: 0.0032\n",
            "Epoch [34/100], Step [900/2800], Loss: 0.0004\n",
            "Epoch [34/100], Step [1000/2800], Loss: 0.0003\n",
            "Epoch [34/100], Step [1100/2800], Loss: 0.0000\n",
            "Epoch [34/100], Step [1200/2800], Loss: 0.0027\n",
            "Epoch [34/100], Step [1300/2800], Loss: 0.0004\n",
            "Epoch [34/100], Step [1400/2800], Loss: 0.0177\n",
            "Epoch [34/100], Step [1500/2800], Loss: 0.0018\n",
            "Epoch [34/100], Step [1600/2800], Loss: 0.0000\n",
            "Epoch [34/100], Step [1700/2800], Loss: 0.0006\n",
            "Epoch [34/100], Step [1800/2800], Loss: 0.2457\n",
            "Epoch [34/100], Step [1900/2800], Loss: 0.0060\n",
            "Epoch [34/100], Step [2000/2800], Loss: 0.0005\n",
            "Epoch [34/100], Step [2100/2800], Loss: 0.0035\n",
            "Epoch [34/100], Step [2200/2800], Loss: 0.2938\n",
            "Epoch [34/100], Step [2300/2800], Loss: 0.0083\n",
            "Epoch [34/100], Step [2400/2800], Loss: 0.0004\n",
            "Epoch [34/100], Step [2500/2800], Loss: 0.0005\n",
            "Epoch [34/100], Step [2600/2800], Loss: 0.0166\n",
            "Epoch [34/100], Step [2700/2800], Loss: 0.0000\n",
            "Epoch [34/100], Step [2800/2800], Loss: 0.0035\n",
            "Epoch [35/100], Step [100/2800], Loss: 0.0089\n",
            "Epoch [35/100], Step [200/2800], Loss: 0.0112\n",
            "Epoch [35/100], Step [300/2800], Loss: 0.0011\n",
            "Epoch [35/100], Step [400/2800], Loss: 0.0085\n",
            "Epoch [35/100], Step [500/2800], Loss: 0.0115\n",
            "Epoch [35/100], Step [600/2800], Loss: 0.0000\n",
            "Epoch [35/100], Step [700/2800], Loss: 2.8286\n",
            "Epoch [35/100], Step [800/2800], Loss: 0.0000\n",
            "Epoch [35/100], Step [900/2800], Loss: 0.0007\n",
            "Epoch [35/100], Step [1000/2800], Loss: 0.0019\n",
            "Epoch [35/100], Step [1100/2800], Loss: 0.0001\n",
            "Epoch [35/100], Step [1200/2800], Loss: 0.0040\n",
            "Epoch [35/100], Step [1300/2800], Loss: 0.0416\n",
            "Epoch [35/100], Step [1400/2800], Loss: 0.0399\n",
            "Epoch [35/100], Step [1500/2800], Loss: 0.0003\n",
            "Epoch [35/100], Step [1600/2800], Loss: 0.0016\n",
            "Epoch [35/100], Step [1700/2800], Loss: 0.0127\n",
            "Epoch [35/100], Step [1800/2800], Loss: 0.0000\n",
            "Epoch [35/100], Step [1900/2800], Loss: 0.0087\n",
            "Epoch [35/100], Step [2000/2800], Loss: 0.0001\n",
            "Epoch [35/100], Step [2100/2800], Loss: 0.0117\n",
            "Epoch [35/100], Step [2200/2800], Loss: 0.0091\n",
            "Epoch [35/100], Step [2300/2800], Loss: 0.0007\n",
            "Epoch [35/100], Step [2400/2800], Loss: 0.0308\n",
            "Epoch [35/100], Step [2500/2800], Loss: 0.0018\n",
            "Epoch [35/100], Step [2600/2800], Loss: 0.0846\n",
            "Epoch [35/100], Step [2700/2800], Loss: 0.0000\n",
            "Epoch [35/100], Step [2800/2800], Loss: 0.0001\n",
            "Epoch [36/100], Step [100/2800], Loss: 0.0014\n",
            "Epoch [36/100], Step [200/2800], Loss: 0.0019\n",
            "Epoch [36/100], Step [300/2800], Loss: 0.0032\n",
            "Epoch [36/100], Step [400/2800], Loss: 0.1160\n",
            "Epoch [36/100], Step [500/2800], Loss: 0.0009\n",
            "Epoch [36/100], Step [600/2800], Loss: 0.0000\n",
            "Epoch [36/100], Step [700/2800], Loss: 0.0262\n",
            "Epoch [36/100], Step [800/2800], Loss: 0.0001\n",
            "Epoch [36/100], Step [900/2800], Loss: 0.0037\n",
            "Epoch [36/100], Step [1000/2800], Loss: 0.0255\n",
            "Epoch [36/100], Step [1100/2800], Loss: 0.0057\n",
            "Epoch [36/100], Step [1200/2800], Loss: 0.0028\n",
            "Epoch [36/100], Step [1300/2800], Loss: 0.0000\n",
            "Epoch [36/100], Step [1400/2800], Loss: 0.1488\n",
            "Epoch [36/100], Step [1500/2800], Loss: 0.0010\n",
            "Epoch [36/100], Step [1600/2800], Loss: 1.2389\n",
            "Epoch [36/100], Step [1700/2800], Loss: 0.0003\n",
            "Epoch [36/100], Step [1800/2800], Loss: 0.0067\n",
            "Epoch [36/100], Step [1900/2800], Loss: 0.0001\n",
            "Epoch [36/100], Step [2000/2800], Loss: 0.0003\n",
            "Epoch [36/100], Step [2100/2800], Loss: 0.0043\n",
            "Epoch [36/100], Step [2200/2800], Loss: 0.1859\n",
            "Epoch [36/100], Step [2300/2800], Loss: 0.0132\n",
            "Epoch [36/100], Step [2400/2800], Loss: 0.0000\n",
            "Epoch [36/100], Step [2500/2800], Loss: 0.0165\n",
            "Epoch [36/100], Step [2600/2800], Loss: 0.3597\n",
            "Epoch [36/100], Step [2700/2800], Loss: 0.0001\n",
            "Epoch [36/100], Step [2800/2800], Loss: 0.0018\n",
            "Epoch [37/100], Step [100/2800], Loss: 0.0006\n",
            "Epoch [37/100], Step [200/2800], Loss: 0.0001\n",
            "Epoch [37/100], Step [300/2800], Loss: 0.0053\n",
            "Epoch [37/100], Step [400/2800], Loss: 0.0196\n",
            "Epoch [37/100], Step [500/2800], Loss: 0.0377\n",
            "Epoch [37/100], Step [600/2800], Loss: 0.0000\n",
            "Epoch [37/100], Step [700/2800], Loss: 0.0932\n",
            "Epoch [37/100], Step [800/2800], Loss: 0.0001\n",
            "Epoch [37/100], Step [900/2800], Loss: 0.0011\n",
            "Epoch [37/100], Step [1000/2800], Loss: 0.0025\n",
            "Epoch [37/100], Step [1100/2800], Loss: 0.0006\n",
            "Epoch [37/100], Step [1200/2800], Loss: 0.0114\n",
            "Epoch [37/100], Step [1300/2800], Loss: 0.0000\n",
            "Epoch [37/100], Step [1400/2800], Loss: 0.0140\n",
            "Epoch [37/100], Step [1500/2800], Loss: 0.0028\n",
            "Epoch [37/100], Step [1600/2800], Loss: 0.0047\n",
            "Epoch [37/100], Step [1700/2800], Loss: 0.0007\n",
            "Epoch [37/100], Step [1800/2800], Loss: 0.0016\n",
            "Epoch [37/100], Step [1900/2800], Loss: 0.0016\n",
            "Epoch [37/100], Step [2000/2800], Loss: 0.0004\n",
            "Epoch [37/100], Step [2100/2800], Loss: 0.0059\n",
            "Epoch [37/100], Step [2200/2800], Loss: 0.0089\n",
            "Epoch [37/100], Step [2300/2800], Loss: 0.0003\n",
            "Epoch [37/100], Step [2400/2800], Loss: 0.0005\n",
            "Epoch [37/100], Step [2500/2800], Loss: 0.0011\n",
            "Epoch [37/100], Step [2600/2800], Loss: 0.0072\n",
            "Epoch [37/100], Step [2700/2800], Loss: 0.0000\n",
            "Epoch [37/100], Step [2800/2800], Loss: 0.0002\n",
            "Epoch [38/100], Step [100/2800], Loss: 0.0003\n",
            "Epoch [38/100], Step [200/2800], Loss: 0.0013\n",
            "Epoch [38/100], Step [300/2800], Loss: 0.0108\n",
            "Epoch [38/100], Step [400/2800], Loss: 0.1783\n",
            "Epoch [38/100], Step [500/2800], Loss: 0.3747\n",
            "Epoch [38/100], Step [600/2800], Loss: 0.0000\n",
            "Epoch [38/100], Step [700/2800], Loss: 0.0050\n",
            "Epoch [38/100], Step [800/2800], Loss: 0.0152\n",
            "Epoch [38/100], Step [900/2800], Loss: 0.0001\n",
            "Epoch [38/100], Step [1000/2800], Loss: 0.0003\n",
            "Epoch [38/100], Step [1100/2800], Loss: 0.0006\n",
            "Epoch [38/100], Step [1200/2800], Loss: 0.0147\n",
            "Epoch [38/100], Step [1300/2800], Loss: 0.0004\n",
            "Epoch [38/100], Step [1400/2800], Loss: 0.0334\n",
            "Epoch [38/100], Step [1500/2800], Loss: 0.0009\n",
            "Epoch [38/100], Step [1600/2800], Loss: 0.0025\n",
            "Epoch [38/100], Step [1700/2800], Loss: 0.0001\n",
            "Epoch [38/100], Step [1800/2800], Loss: 0.0810\n",
            "Epoch [38/100], Step [1900/2800], Loss: 0.0104\n",
            "Epoch [38/100], Step [2000/2800], Loss: 0.0001\n",
            "Epoch [38/100], Step [2100/2800], Loss: 0.0343\n",
            "Epoch [38/100], Step [2200/2800], Loss: 0.0158\n",
            "Epoch [38/100], Step [2300/2800], Loss: 0.0620\n",
            "Epoch [38/100], Step [2400/2800], Loss: 0.0300\n",
            "Epoch [38/100], Step [2500/2800], Loss: 0.0009\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [38/100], Step [2600/2800], Loss: 0.0194\n",
            "Epoch [38/100], Step [2700/2800], Loss: 0.0007\n",
            "Epoch [38/100], Step [2800/2800], Loss: 0.0077\n",
            "Epoch [39/100], Step [100/2800], Loss: 0.0044\n",
            "Epoch [39/100], Step [200/2800], Loss: 0.0008\n",
            "Epoch [39/100], Step [300/2800], Loss: 0.0000\n",
            "Epoch [39/100], Step [400/2800], Loss: 0.0027\n",
            "Epoch [39/100], Step [500/2800], Loss: 0.0006\n",
            "Epoch [39/100], Step [600/2800], Loss: 0.0000\n",
            "Epoch [39/100], Step [700/2800], Loss: 0.0497\n",
            "Epoch [39/100], Step [800/2800], Loss: 0.0151\n",
            "Epoch [39/100], Step [900/2800], Loss: 0.0050\n",
            "Epoch [39/100], Step [1000/2800], Loss: 0.0005\n",
            "Epoch [39/100], Step [1100/2800], Loss: 0.0008\n",
            "Epoch [39/100], Step [1200/2800], Loss: 0.3597\n",
            "Epoch [39/100], Step [1300/2800], Loss: 0.0000\n",
            "Epoch [39/100], Step [1400/2800], Loss: 0.2147\n",
            "Epoch [39/100], Step [1500/2800], Loss: 0.0059\n",
            "Epoch [39/100], Step [1600/2800], Loss: 0.0080\n",
            "Epoch [39/100], Step [1700/2800], Loss: 0.0001\n",
            "Epoch [39/100], Step [1800/2800], Loss: 0.0009\n",
            "Epoch [39/100], Step [1900/2800], Loss: 0.0410\n",
            "Epoch [39/100], Step [2000/2800], Loss: 0.0001\n",
            "Epoch [39/100], Step [2100/2800], Loss: 0.0005\n",
            "Epoch [39/100], Step [2200/2800], Loss: 0.0055\n",
            "Epoch [39/100], Step [2300/2800], Loss: 0.0303\n",
            "Epoch [39/100], Step [2400/2800], Loss: 0.0003\n",
            "Epoch [39/100], Step [2500/2800], Loss: 0.0024\n",
            "Epoch [39/100], Step [2600/2800], Loss: 0.0674\n",
            "Epoch [39/100], Step [2700/2800], Loss: 0.0006\n",
            "Epoch [39/100], Step [2800/2800], Loss: 0.0010\n",
            "Epoch [40/100], Step [100/2800], Loss: 0.0001\n",
            "Epoch [40/100], Step [200/2800], Loss: 0.0000\n",
            "Epoch [40/100], Step [300/2800], Loss: 0.0056\n",
            "Epoch [40/100], Step [400/2800], Loss: 0.0059\n",
            "Epoch [40/100], Step [500/2800], Loss: 0.0017\n",
            "Epoch [40/100], Step [600/2800], Loss: 0.0001\n",
            "Epoch [40/100], Step [700/2800], Loss: 0.0274\n",
            "Epoch [40/100], Step [800/2800], Loss: 0.0001\n",
            "Epoch [40/100], Step [900/2800], Loss: 0.0001\n",
            "Epoch [40/100], Step [1000/2800], Loss: 0.0017\n",
            "Epoch [40/100], Step [1100/2800], Loss: 0.0001\n",
            "Epoch [40/100], Step [1200/2800], Loss: 0.0140\n",
            "Epoch [40/100], Step [1300/2800], Loss: 0.0000\n",
            "Epoch [40/100], Step [1400/2800], Loss: 0.0071\n",
            "Epoch [40/100], Step [1500/2800], Loss: 0.0119\n",
            "Epoch [40/100], Step [1600/2800], Loss: 0.0693\n",
            "Epoch [40/100], Step [1700/2800], Loss: 0.0042\n",
            "Epoch [40/100], Step [1800/2800], Loss: 0.0054\n",
            "Epoch [40/100], Step [1900/2800], Loss: 0.0166\n",
            "Epoch [40/100], Step [2000/2800], Loss: 0.0001\n",
            "Epoch [40/100], Step [2100/2800], Loss: 0.0039\n",
            "Epoch [40/100], Step [2200/2800], Loss: 0.0944\n",
            "Epoch [40/100], Step [2300/2800], Loss: 0.0022\n",
            "Epoch [40/100], Step [2400/2800], Loss: 0.0171\n",
            "Epoch [40/100], Step [2500/2800], Loss: 0.0061\n",
            "Epoch [40/100], Step [2600/2800], Loss: 0.0746\n",
            "Epoch [40/100], Step [2700/2800], Loss: 0.0024\n",
            "Epoch [40/100], Step [2800/2800], Loss: 0.0022\n",
            "Epoch [41/100], Step [100/2800], Loss: 0.0038\n",
            "Epoch [41/100], Step [200/2800], Loss: 0.0027\n",
            "Epoch [41/100], Step [300/2800], Loss: 0.0010\n",
            "Epoch [41/100], Step [400/2800], Loss: 0.1964\n",
            "Epoch [41/100], Step [500/2800], Loss: 0.0283\n",
            "Epoch [41/100], Step [600/2800], Loss: 0.0000\n",
            "Epoch [41/100], Step [700/2800], Loss: 0.0021\n",
            "Epoch [41/100], Step [800/2800], Loss: 0.0001\n",
            "Epoch [41/100], Step [900/2800], Loss: 0.0014\n",
            "Epoch [41/100], Step [1000/2800], Loss: 0.0126\n",
            "Epoch [41/100], Step [1100/2800], Loss: 0.0059\n",
            "Epoch [41/100], Step [1200/2800], Loss: 0.1808\n",
            "Epoch [41/100], Step [1300/2800], Loss: 0.0001\n",
            "Epoch [41/100], Step [1400/2800], Loss: 0.0711\n",
            "Epoch [41/100], Step [1500/2800], Loss: 0.0001\n",
            "Epoch [41/100], Step [1600/2800], Loss: 0.0003\n",
            "Epoch [41/100], Step [1700/2800], Loss: 0.0046\n",
            "Epoch [41/100], Step [1800/2800], Loss: 0.0001\n",
            "Epoch [41/100], Step [1900/2800], Loss: 0.0072\n",
            "Epoch [41/100], Step [2000/2800], Loss: 0.0002\n",
            "Epoch [41/100], Step [2100/2800], Loss: 0.0092\n",
            "Epoch [41/100], Step [2200/2800], Loss: 0.0206\n",
            "Epoch [41/100], Step [2300/2800], Loss: 0.0011\n",
            "Epoch [41/100], Step [2400/2800], Loss: 0.0016\n",
            "Epoch [41/100], Step [2500/2800], Loss: 0.0021\n",
            "Epoch [41/100], Step [2600/2800], Loss: 0.0003\n",
            "Epoch [41/100], Step [2700/2800], Loss: 0.0009\n",
            "Epoch [41/100], Step [2800/2800], Loss: 0.0000\n",
            "Epoch [42/100], Step [100/2800], Loss: 0.0000\n",
            "Epoch [42/100], Step [200/2800], Loss: 0.0004\n",
            "Epoch [42/100], Step [300/2800], Loss: 0.0002\n",
            "Epoch [42/100], Step [400/2800], Loss: 0.0010\n",
            "Epoch [42/100], Step [500/2800], Loss: 0.0014\n",
            "Epoch [42/100], Step [600/2800], Loss: 0.0000\n",
            "Epoch [42/100], Step [700/2800], Loss: 0.3931\n",
            "Epoch [42/100], Step [800/2800], Loss: 0.0000\n",
            "Epoch [42/100], Step [900/2800], Loss: 0.0023\n",
            "Epoch [42/100], Step [1000/2800], Loss: 0.4257\n",
            "Epoch [42/100], Step [1100/2800], Loss: 0.0049\n",
            "Epoch [42/100], Step [1200/2800], Loss: 0.0045\n",
            "Epoch [42/100], Step [1300/2800], Loss: 0.0000\n",
            "Epoch [42/100], Step [1400/2800], Loss: 0.4682\n",
            "Epoch [42/100], Step [1500/2800], Loss: 0.0001\n",
            "Epoch [42/100], Step [1600/2800], Loss: 0.0002\n",
            "Epoch [42/100], Step [1700/2800], Loss: 0.0000\n",
            "Epoch [42/100], Step [1800/2800], Loss: 0.0000\n",
            "Epoch [42/100], Step [1900/2800], Loss: 0.0029\n",
            "Epoch [42/100], Step [2000/2800], Loss: 0.0005\n",
            "Epoch [42/100], Step [2100/2800], Loss: 0.1341\n",
            "Epoch [42/100], Step [2200/2800], Loss: 0.0380\n",
            "Epoch [42/100], Step [2300/2800], Loss: 0.0001\n",
            "Epoch [42/100], Step [2400/2800], Loss: 0.0002\n",
            "Epoch [42/100], Step [2500/2800], Loss: 0.0008\n",
            "Epoch [42/100], Step [2600/2800], Loss: 0.0002\n",
            "Epoch [42/100], Step [2700/2800], Loss: 0.0001\n",
            "Epoch [42/100], Step [2800/2800], Loss: 0.0000\n",
            "Epoch [43/100], Step [100/2800], Loss: 0.0001\n",
            "Epoch [43/100], Step [200/2800], Loss: 0.0001\n",
            "Epoch [43/100], Step [300/2800], Loss: 0.0046\n",
            "Epoch [43/100], Step [400/2800], Loss: 0.0032\n",
            "Epoch [43/100], Step [500/2800], Loss: 0.0006\n",
            "Epoch [43/100], Step [600/2800], Loss: 0.0000\n",
            "Epoch [43/100], Step [700/2800], Loss: 0.0017\n",
            "Epoch [43/100], Step [800/2800], Loss: 0.0000\n",
            "Epoch [43/100], Step [900/2800], Loss: 0.0003\n",
            "Epoch [43/100], Step [1000/2800], Loss: 0.0017\n",
            "Epoch [43/100], Step [1100/2800], Loss: 0.0001\n",
            "Epoch [43/100], Step [1200/2800], Loss: 0.0004\n",
            "Epoch [43/100], Step [1300/2800], Loss: 0.0000\n",
            "Epoch [43/100], Step [1400/2800], Loss: 0.0075\n",
            "Epoch [43/100], Step [1500/2800], Loss: 0.0041\n",
            "Epoch [43/100], Step [1600/2800], Loss: 0.0011\n",
            "Epoch [43/100], Step [1700/2800], Loss: 0.0001\n",
            "Epoch [43/100], Step [1800/2800], Loss: 0.0001\n",
            "Epoch [43/100], Step [1900/2800], Loss: 0.0053\n",
            "Epoch [43/100], Step [2000/2800], Loss: 0.0002\n",
            "Epoch [43/100], Step [2100/2800], Loss: 0.0600\n",
            "Epoch [43/100], Step [2200/2800], Loss: 0.0165\n",
            "Epoch [43/100], Step [2300/2800], Loss: 0.0000\n",
            "Epoch [43/100], Step [2400/2800], Loss: 0.1968\n",
            "Epoch [43/100], Step [2500/2800], Loss: 0.0008\n",
            "Epoch [43/100], Step [2600/2800], Loss: 0.0051\n",
            "Epoch [43/100], Step [2700/2800], Loss: 0.0000\n",
            "Epoch [43/100], Step [2800/2800], Loss: 0.0061\n",
            "Epoch [44/100], Step [100/2800], Loss: 0.0005\n",
            "Epoch [44/100], Step [200/2800], Loss: 0.0008\n",
            "Epoch [44/100], Step [300/2800], Loss: 0.0001\n",
            "Epoch [44/100], Step [400/2800], Loss: 0.0049\n",
            "Epoch [44/100], Step [500/2800], Loss: 0.0013\n",
            "Epoch [44/100], Step [600/2800], Loss: 0.0000\n",
            "Epoch [44/100], Step [700/2800], Loss: 0.0288\n",
            "Epoch [44/100], Step [800/2800], Loss: 0.0000\n",
            "Epoch [44/100], Step [900/2800], Loss: 0.0000\n",
            "Epoch [44/100], Step [1000/2800], Loss: 0.0007\n",
            "Epoch [44/100], Step [1100/2800], Loss: 0.0001\n",
            "Epoch [44/100], Step [1200/2800], Loss: 0.0010\n",
            "Epoch [44/100], Step [1300/2800], Loss: 0.0000\n",
            "Epoch [44/100], Step [1400/2800], Loss: 0.0117\n",
            "Epoch [44/100], Step [1500/2800], Loss: 0.0011\n",
            "Epoch [44/100], Step [1600/2800], Loss: 0.0082\n",
            "Epoch [44/100], Step [1700/2800], Loss: 0.0001\n",
            "Epoch [44/100], Step [1800/2800], Loss: 0.0008\n",
            "Epoch [44/100], Step [1900/2800], Loss: 0.0524\n",
            "Epoch [44/100], Step [2000/2800], Loss: 0.0025\n",
            "Epoch [44/100], Step [2100/2800], Loss: 0.0021\n",
            "Epoch [44/100], Step [2200/2800], Loss: 0.0144\n",
            "Epoch [44/100], Step [2300/2800], Loss: 0.0002\n",
            "Epoch [44/100], Step [2400/2800], Loss: 0.0000\n",
            "Epoch [44/100], Step [2500/2800], Loss: 0.0015\n",
            "Epoch [44/100], Step [2600/2800], Loss: 0.0029\n",
            "Epoch [44/100], Step [2700/2800], Loss: 0.1134\n",
            "Epoch [44/100], Step [2800/2800], Loss: 0.0005\n",
            "Epoch [45/100], Step [100/2800], Loss: 0.0001\n",
            "Epoch [45/100], Step [200/2800], Loss: 0.0002\n",
            "Epoch [45/100], Step [300/2800], Loss: 0.0019\n",
            "Epoch [45/100], Step [400/2800], Loss: 0.0236\n",
            "Epoch [45/100], Step [500/2800], Loss: 0.1368\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [45/100], Step [600/2800], Loss: 0.0000\n",
            "Epoch [45/100], Step [700/2800], Loss: 0.3968\n",
            "Epoch [45/100], Step [800/2800], Loss: 0.0002\n",
            "Epoch [45/100], Step [900/2800], Loss: 0.0004\n",
            "Epoch [45/100], Step [1000/2800], Loss: 0.0002\n",
            "Epoch [45/100], Step [1100/2800], Loss: 0.0000\n",
            "Epoch [45/100], Step [1200/2800], Loss: 0.0024\n",
            "Epoch [45/100], Step [1300/2800], Loss: 0.0000\n",
            "Epoch [45/100], Step [1400/2800], Loss: 0.0025\n",
            "Epoch [45/100], Step [1500/2800], Loss: 0.0013\n",
            "Epoch [45/100], Step [1600/2800], Loss: 0.0008\n",
            "Epoch [45/100], Step [1700/2800], Loss: 0.0001\n",
            "Epoch [45/100], Step [1800/2800], Loss: 0.0029\n",
            "Epoch [45/100], Step [1900/2800], Loss: 0.0894\n",
            "Epoch [45/100], Step [2000/2800], Loss: 0.0006\n",
            "Epoch [45/100], Step [2100/2800], Loss: 0.0028\n",
            "Epoch [45/100], Step [2200/2800], Loss: 0.0011\n",
            "Epoch [45/100], Step [2300/2800], Loss: 0.0014\n",
            "Epoch [45/100], Step [2400/2800], Loss: 0.0003\n",
            "Epoch [45/100], Step [2500/2800], Loss: 0.0008\n",
            "Epoch [45/100], Step [2600/2800], Loss: 0.0062\n",
            "Epoch [45/100], Step [2700/2800], Loss: 0.0000\n",
            "Epoch [45/100], Step [2800/2800], Loss: 0.0003\n",
            "Epoch [46/100], Step [100/2800], Loss: 0.0014\n",
            "Epoch [46/100], Step [200/2800], Loss: 0.0046\n",
            "Epoch [46/100], Step [300/2800], Loss: 0.0034\n",
            "Epoch [46/100], Step [400/2800], Loss: 0.0194\n",
            "Epoch [46/100], Step [500/2800], Loss: 0.0555\n",
            "Epoch [46/100], Step [600/2800], Loss: 0.0000\n",
            "Epoch [46/100], Step [700/2800], Loss: 2.0700\n",
            "Epoch [46/100], Step [800/2800], Loss: 0.0005\n",
            "Epoch [46/100], Step [900/2800], Loss: 0.0047\n",
            "Epoch [46/100], Step [1000/2800], Loss: 0.0017\n",
            "Epoch [46/100], Step [1100/2800], Loss: 0.0000\n",
            "Epoch [46/100], Step [1200/2800], Loss: 0.0002\n",
            "Epoch [46/100], Step [1300/2800], Loss: 0.0000\n",
            "Epoch [46/100], Step [1400/2800], Loss: 0.0037\n",
            "Epoch [46/100], Step [1500/2800], Loss: 0.0031\n",
            "Epoch [46/100], Step [1600/2800], Loss: 0.0004\n",
            "Epoch [46/100], Step [1700/2800], Loss: 0.0000\n",
            "Epoch [46/100], Step [1800/2800], Loss: 0.0005\n",
            "Epoch [46/100], Step [1900/2800], Loss: 0.1842\n",
            "Epoch [46/100], Step [2000/2800], Loss: 0.0002\n",
            "Epoch [46/100], Step [2100/2800], Loss: 0.0173\n",
            "Epoch [46/100], Step [2200/2800], Loss: 0.0090\n",
            "Epoch [46/100], Step [2300/2800], Loss: 0.0000\n",
            "Epoch [46/100], Step [2400/2800], Loss: 0.0000\n",
            "Epoch [46/100], Step [2500/2800], Loss: 0.0027\n",
            "Epoch [46/100], Step [2600/2800], Loss: 0.0229\n",
            "Epoch [46/100], Step [2700/2800], Loss: 0.0000\n",
            "Epoch [46/100], Step [2800/2800], Loss: 0.0001\n",
            "Epoch [47/100], Step [100/2800], Loss: 0.0063\n",
            "Epoch [47/100], Step [200/2800], Loss: 0.0001\n",
            "Epoch [47/100], Step [300/2800], Loss: 0.0234\n",
            "Epoch [47/100], Step [400/2800], Loss: 0.0018\n",
            "Epoch [47/100], Step [500/2800], Loss: 0.0026\n",
            "Epoch [47/100], Step [600/2800], Loss: 0.0000\n",
            "Epoch [47/100], Step [700/2800], Loss: 0.5374\n",
            "Epoch [47/100], Step [800/2800], Loss: 0.0000\n",
            "Epoch [47/100], Step [900/2800], Loss: 0.0000\n",
            "Epoch [47/100], Step [1000/2800], Loss: 0.0124\n",
            "Epoch [47/100], Step [1100/2800], Loss: 0.0003\n",
            "Epoch [47/100], Step [1200/2800], Loss: 0.0053\n",
            "Epoch [47/100], Step [1300/2800], Loss: 0.0000\n",
            "Epoch [47/100], Step [1400/2800], Loss: 0.0211\n",
            "Epoch [47/100], Step [1500/2800], Loss: 0.0015\n",
            "Epoch [47/100], Step [1600/2800], Loss: 0.0017\n",
            "Epoch [47/100], Step [1700/2800], Loss: 0.0005\n",
            "Epoch [47/100], Step [1800/2800], Loss: 0.0004\n",
            "Epoch [47/100], Step [1900/2800], Loss: 0.0014\n",
            "Epoch [47/100], Step [2000/2800], Loss: 0.0006\n",
            "Epoch [47/100], Step [2100/2800], Loss: 0.0223\n",
            "Epoch [47/100], Step [2200/2800], Loss: 0.0019\n",
            "Epoch [47/100], Step [2300/2800], Loss: 0.0000\n",
            "Epoch [47/100], Step [2400/2800], Loss: 0.0000\n",
            "Epoch [47/100], Step [2500/2800], Loss: 0.0002\n",
            "Epoch [47/100], Step [2600/2800], Loss: 0.0032\n",
            "Epoch [47/100], Step [2700/2800], Loss: 0.0000\n",
            "Epoch [47/100], Step [2800/2800], Loss: 0.0000\n",
            "Epoch [48/100], Step [100/2800], Loss: 0.0012\n",
            "Epoch [48/100], Step [200/2800], Loss: 0.0003\n",
            "Epoch [48/100], Step [300/2800], Loss: 0.0000\n",
            "Epoch [48/100], Step [400/2800], Loss: 0.0095\n",
            "Epoch [48/100], Step [500/2800], Loss: 0.0000\n",
            "Epoch [48/100], Step [600/2800], Loss: 0.0000\n",
            "Epoch [48/100], Step [700/2800], Loss: 0.0573\n",
            "Epoch [48/100], Step [800/2800], Loss: 0.0001\n",
            "Epoch [48/100], Step [900/2800], Loss: 0.0000\n",
            "Epoch [48/100], Step [1000/2800], Loss: 0.0176\n",
            "Epoch [48/100], Step [1100/2800], Loss: 0.0000\n",
            "Epoch [48/100], Step [1200/2800], Loss: 0.0275\n",
            "Epoch [48/100], Step [1300/2800], Loss: 0.0000\n",
            "Epoch [48/100], Step [1400/2800], Loss: 0.0097\n",
            "Epoch [48/100], Step [1500/2800], Loss: 0.0302\n",
            "Epoch [48/100], Step [1600/2800], Loss: 0.0001\n",
            "Epoch [48/100], Step [1700/2800], Loss: 0.0000\n",
            "Epoch [48/100], Step [1800/2800], Loss: 0.2815\n",
            "Epoch [48/100], Step [1900/2800], Loss: 0.0006\n",
            "Epoch [48/100], Step [2000/2800], Loss: 0.0001\n",
            "Epoch [48/100], Step [2100/2800], Loss: 0.0174\n",
            "Epoch [48/100], Step [2200/2800], Loss: 0.0020\n",
            "Epoch [48/100], Step [2300/2800], Loss: 0.0000\n",
            "Epoch [48/100], Step [2400/2800], Loss: 0.0001\n",
            "Epoch [48/100], Step [2500/2800], Loss: 0.0003\n",
            "Epoch [48/100], Step [2600/2800], Loss: 0.0002\n",
            "Epoch [48/100], Step [2700/2800], Loss: 0.0000\n",
            "Epoch [48/100], Step [2800/2800], Loss: 0.0000\n",
            "Epoch [49/100], Step [100/2800], Loss: 0.0018\n",
            "Epoch [49/100], Step [200/2800], Loss: 0.0704\n",
            "Epoch [49/100], Step [300/2800], Loss: 0.0014\n",
            "Epoch [49/100], Step [400/2800], Loss: 0.0140\n",
            "Epoch [49/100], Step [500/2800], Loss: 0.0055\n",
            "Epoch [49/100], Step [600/2800], Loss: 0.0004\n",
            "Epoch [49/100], Step [700/2800], Loss: 0.0213\n",
            "Epoch [49/100], Step [800/2800], Loss: 0.0001\n",
            "Epoch [49/100], Step [900/2800], Loss: 0.0000\n",
            "Epoch [49/100], Step [1000/2800], Loss: 0.0095\n",
            "Epoch [49/100], Step [1100/2800], Loss: 0.0000\n",
            "Epoch [49/100], Step [1200/2800], Loss: 0.0266\n",
            "Epoch [49/100], Step [1300/2800], Loss: 0.0042\n",
            "Epoch [49/100], Step [1400/2800], Loss: 0.1458\n",
            "Epoch [49/100], Step [1500/2800], Loss: 0.0000\n",
            "Epoch [49/100], Step [1600/2800], Loss: 0.0002\n",
            "Epoch [49/100], Step [1700/2800], Loss: 0.0026\n",
            "Epoch [49/100], Step [1800/2800], Loss: 0.0000\n",
            "Epoch [49/100], Step [1900/2800], Loss: 0.0093\n",
            "Epoch [49/100], Step [2000/2800], Loss: 0.0001\n",
            "Epoch [49/100], Step [2100/2800], Loss: 0.0135\n",
            "Epoch [49/100], Step [2200/2800], Loss: 0.0817\n",
            "Epoch [49/100], Step [2300/2800], Loss: 0.0000\n",
            "Epoch [49/100], Step [2400/2800], Loss: 0.0002\n",
            "Epoch [49/100], Step [2500/2800], Loss: 0.0012\n",
            "Epoch [49/100], Step [2600/2800], Loss: 0.0122\n",
            "Epoch [49/100], Step [2700/2800], Loss: 0.0000\n",
            "Epoch [49/100], Step [2800/2800], Loss: 0.0001\n",
            "Epoch [50/100], Step [100/2800], Loss: 0.0002\n",
            "Epoch [50/100], Step [200/2800], Loss: 0.0052\n",
            "Epoch [50/100], Step [300/2800], Loss: 0.0003\n",
            "Epoch [50/100], Step [400/2800], Loss: 0.0119\n",
            "Epoch [50/100], Step [500/2800], Loss: 0.0009\n",
            "Epoch [50/100], Step [600/2800], Loss: 0.0002\n",
            "Epoch [50/100], Step [700/2800], Loss: 0.0246\n",
            "Epoch [50/100], Step [800/2800], Loss: 0.0001\n",
            "Epoch [50/100], Step [900/2800], Loss: 0.0002\n",
            "Epoch [50/100], Step [1000/2800], Loss: 0.0012\n",
            "Epoch [50/100], Step [1100/2800], Loss: 0.0001\n",
            "Epoch [50/100], Step [1200/2800], Loss: 0.0027\n",
            "Epoch [50/100], Step [1300/2800], Loss: 0.0000\n",
            "Epoch [50/100], Step [1400/2800], Loss: 0.0044\n",
            "Epoch [50/100], Step [1500/2800], Loss: 0.0141\n",
            "Epoch [50/100], Step [1600/2800], Loss: 0.0052\n",
            "Epoch [50/100], Step [1700/2800], Loss: 0.0024\n",
            "Epoch [50/100], Step [1800/2800], Loss: 0.0011\n",
            "Epoch [50/100], Step [1900/2800], Loss: 0.0014\n",
            "Epoch [50/100], Step [2000/2800], Loss: 0.0000\n",
            "Epoch [50/100], Step [2100/2800], Loss: 0.0060\n",
            "Epoch [50/100], Step [2200/2800], Loss: 0.0040\n",
            "Epoch [50/100], Step [2300/2800], Loss: 0.0000\n",
            "Epoch [50/100], Step [2400/2800], Loss: 0.0000\n",
            "Epoch [50/100], Step [2500/2800], Loss: 0.0027\n",
            "Epoch [50/100], Step [2600/2800], Loss: 0.0001\n",
            "Epoch [50/100], Step [2700/2800], Loss: 0.0000\n",
            "Epoch [50/100], Step [2800/2800], Loss: 0.0007\n",
            "Epoch [51/100], Step [100/2800], Loss: 0.0001\n",
            "Epoch [51/100], Step [200/2800], Loss: 0.0009\n",
            "Epoch [51/100], Step [300/2800], Loss: 0.0001\n",
            "Epoch [51/100], Step [400/2800], Loss: 0.0063\n",
            "Epoch [51/100], Step [500/2800], Loss: 0.0004\n",
            "Epoch [51/100], Step [600/2800], Loss: 0.0000\n",
            "Epoch [51/100], Step [700/2800], Loss: 0.0021\n",
            "Epoch [51/100], Step [800/2800], Loss: 0.0002\n",
            "Epoch [51/100], Step [900/2800], Loss: 0.0009\n",
            "Epoch [51/100], Step [1000/2800], Loss: 0.0010\n",
            "Epoch [51/100], Step [1100/2800], Loss: 0.0000\n",
            "Epoch [51/100], Step [1200/2800], Loss: 0.0197\n",
            "Epoch [51/100], Step [1300/2800], Loss: 0.0001\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [51/100], Step [1400/2800], Loss: 0.0102\n",
            "Epoch [51/100], Step [1500/2800], Loss: 1.1842\n",
            "Epoch [51/100], Step [1600/2800], Loss: 0.0000\n",
            "Epoch [51/100], Step [1700/2800], Loss: 0.0000\n",
            "Epoch [51/100], Step [1800/2800], Loss: 0.0001\n",
            "Epoch [51/100], Step [1900/2800], Loss: 0.0008\n",
            "Epoch [51/100], Step [2000/2800], Loss: 0.0000\n",
            "Epoch [51/100], Step [2100/2800], Loss: 0.0063\n",
            "Epoch [51/100], Step [2200/2800], Loss: 0.0010\n",
            "Epoch [51/100], Step [2300/2800], Loss: 0.0002\n",
            "Epoch [51/100], Step [2400/2800], Loss: 0.0001\n",
            "Epoch [51/100], Step [2500/2800], Loss: 0.0019\n",
            "Epoch [51/100], Step [2600/2800], Loss: 0.0049\n",
            "Epoch [51/100], Step [2700/2800], Loss: 0.0000\n",
            "Epoch [51/100], Step [2800/2800], Loss: 0.2765\n",
            "Epoch [52/100], Step [100/2800], Loss: 0.0005\n",
            "Epoch [52/100], Step [200/2800], Loss: 0.0010\n",
            "Epoch [52/100], Step [300/2800], Loss: 0.0043\n",
            "Epoch [52/100], Step [400/2800], Loss: 0.0008\n",
            "Epoch [52/100], Step [500/2800], Loss: 0.0246\n",
            "Epoch [52/100], Step [600/2800], Loss: 0.0000\n",
            "Epoch [52/100], Step [700/2800], Loss: 0.0001\n",
            "Epoch [52/100], Step [800/2800], Loss: 0.0001\n",
            "Epoch [52/100], Step [900/2800], Loss: 0.0000\n",
            "Epoch [52/100], Step [1000/2800], Loss: 0.0002\n",
            "Epoch [52/100], Step [1100/2800], Loss: 0.0000\n",
            "Epoch [52/100], Step [1200/2800], Loss: 0.0032\n",
            "Epoch [52/100], Step [1300/2800], Loss: 0.0000\n",
            "Epoch [52/100], Step [1400/2800], Loss: 0.3071\n",
            "Epoch [52/100], Step [1500/2800], Loss: 0.0019\n",
            "Epoch [52/100], Step [1600/2800], Loss: 0.0058\n",
            "Epoch [52/100], Step [1700/2800], Loss: 0.0040\n",
            "Epoch [52/100], Step [1800/2800], Loss: 0.0020\n",
            "Epoch [52/100], Step [1900/2800], Loss: 0.0124\n",
            "Epoch [52/100], Step [2000/2800], Loss: 0.0001\n",
            "Epoch [52/100], Step [2100/2800], Loss: 0.1202\n",
            "Epoch [52/100], Step [2200/2800], Loss: 0.0000\n",
            "Epoch [52/100], Step [2300/2800], Loss: 0.0212\n",
            "Epoch [52/100], Step [2400/2800], Loss: 0.0000\n",
            "Epoch [52/100], Step [2500/2800], Loss: 0.0001\n",
            "Epoch [52/100], Step [2600/2800], Loss: 1.0591\n",
            "Epoch [52/100], Step [2700/2800], Loss: 0.0000\n",
            "Epoch [52/100], Step [2800/2800], Loss: 0.1522\n",
            "Epoch [53/100], Step [100/2800], Loss: 0.0001\n",
            "Epoch [53/100], Step [200/2800], Loss: 0.0000\n",
            "Epoch [53/100], Step [300/2800], Loss: 0.0572\n",
            "Epoch [53/100], Step [400/2800], Loss: 1.0193\n",
            "Epoch [53/100], Step [500/2800], Loss: 0.0052\n",
            "Epoch [53/100], Step [600/2800], Loss: 0.0001\n",
            "Epoch [53/100], Step [700/2800], Loss: 0.2616\n",
            "Epoch [53/100], Step [800/2800], Loss: 0.0000\n",
            "Epoch [53/100], Step [900/2800], Loss: 0.0000\n",
            "Epoch [53/100], Step [1000/2800], Loss: 0.0170\n",
            "Epoch [53/100], Step [1100/2800], Loss: 0.0000\n",
            "Epoch [53/100], Step [1200/2800], Loss: 0.0432\n",
            "Epoch [53/100], Step [1300/2800], Loss: 0.0003\n",
            "Epoch [53/100], Step [1400/2800], Loss: 0.1547\n",
            "Epoch [53/100], Step [1500/2800], Loss: 0.0001\n",
            "Epoch [53/100], Step [1600/2800], Loss: 0.0008\n",
            "Epoch [53/100], Step [1700/2800], Loss: 0.0002\n",
            "Epoch [53/100], Step [1800/2800], Loss: 0.0000\n",
            "Epoch [53/100], Step [1900/2800], Loss: 0.0010\n",
            "Epoch [53/100], Step [2000/2800], Loss: 0.0015\n",
            "Epoch [53/100], Step [2100/2800], Loss: 0.0010\n",
            "Epoch [53/100], Step [2200/2800], Loss: 0.0559\n",
            "Epoch [53/100], Step [2300/2800], Loss: 0.0001\n",
            "Epoch [53/100], Step [2400/2800], Loss: 0.0000\n",
            "Epoch [53/100], Step [2500/2800], Loss: 0.0000\n",
            "Epoch [53/100], Step [2600/2800], Loss: 0.0060\n",
            "Epoch [53/100], Step [2700/2800], Loss: 0.0000\n",
            "Epoch [53/100], Step [2800/2800], Loss: 0.0085\n",
            "Epoch [54/100], Step [100/2800], Loss: 0.0005\n",
            "Epoch [54/100], Step [200/2800], Loss: 0.0205\n",
            "Epoch [54/100], Step [300/2800], Loss: 0.0042\n",
            "Epoch [54/100], Step [400/2800], Loss: 0.0004\n",
            "Epoch [54/100], Step [500/2800], Loss: 0.0459\n",
            "Epoch [54/100], Step [600/2800], Loss: 0.0027\n",
            "Epoch [54/100], Step [700/2800], Loss: 0.0116\n",
            "Epoch [54/100], Step [800/2800], Loss: 0.0000\n",
            "Epoch [54/100], Step [900/2800], Loss: 0.0020\n",
            "Epoch [54/100], Step [1000/2800], Loss: 0.0001\n",
            "Epoch [54/100], Step [1100/2800], Loss: 0.0000\n",
            "Epoch [54/100], Step [1200/2800], Loss: 0.0019\n",
            "Epoch [54/100], Step [1300/2800], Loss: 0.0000\n",
            "Epoch [54/100], Step [1400/2800], Loss: 0.0025\n",
            "Epoch [54/100], Step [1500/2800], Loss: 0.0008\n",
            "Epoch [54/100], Step [1600/2800], Loss: 0.0020\n",
            "Epoch [54/100], Step [1700/2800], Loss: 0.0086\n",
            "Epoch [54/100], Step [1800/2800], Loss: 0.0000\n",
            "Epoch [54/100], Step [1900/2800], Loss: 0.0011\n",
            "Epoch [54/100], Step [2000/2800], Loss: 0.0002\n",
            "Epoch [54/100], Step [2100/2800], Loss: 0.1461\n",
            "Epoch [54/100], Step [2200/2800], Loss: 0.0886\n",
            "Epoch [54/100], Step [2300/2800], Loss: 0.0000\n",
            "Epoch [54/100], Step [2400/2800], Loss: 0.0006\n",
            "Epoch [54/100], Step [2500/2800], Loss: 0.0058\n",
            "Epoch [54/100], Step [2600/2800], Loss: 0.0085\n",
            "Epoch [54/100], Step [2700/2800], Loss: 0.0000\n",
            "Epoch [54/100], Step [2800/2800], Loss: 0.0104\n",
            "Epoch [55/100], Step [100/2800], Loss: 0.0002\n",
            "Epoch [55/100], Step [200/2800], Loss: 0.0000\n",
            "Epoch [55/100], Step [300/2800], Loss: 0.0028\n",
            "Epoch [55/100], Step [400/2800], Loss: 0.0002\n",
            "Epoch [55/100], Step [500/2800], Loss: 0.1442\n",
            "Epoch [55/100], Step [600/2800], Loss: 0.0001\n",
            "Epoch [55/100], Step [700/2800], Loss: 0.0061\n",
            "Epoch [55/100], Step [800/2800], Loss: 0.0000\n",
            "Epoch [55/100], Step [900/2800], Loss: 0.0002\n",
            "Epoch [55/100], Step [1000/2800], Loss: 0.6952\n",
            "Epoch [55/100], Step [1100/2800], Loss: 0.0000\n",
            "Epoch [55/100], Step [1200/2800], Loss: 0.0002\n",
            "Epoch [55/100], Step [1300/2800], Loss: 0.0000\n",
            "Epoch [55/100], Step [1400/2800], Loss: 0.0000\n",
            "Epoch [55/100], Step [1500/2800], Loss: 0.0000\n",
            "Epoch [55/100], Step [1600/2800], Loss: 0.0032\n",
            "Epoch [55/100], Step [1700/2800], Loss: 0.3777\n",
            "Epoch [55/100], Step [1800/2800], Loss: 0.0009\n",
            "Epoch [55/100], Step [1900/2800], Loss: 0.0002\n",
            "Epoch [55/100], Step [2000/2800], Loss: 0.0001\n",
            "Epoch [55/100], Step [2100/2800], Loss: 0.0360\n",
            "Epoch [55/100], Step [2200/2800], Loss: 0.0041\n",
            "Epoch [55/100], Step [2300/2800], Loss: 0.0000\n",
            "Epoch [55/100], Step [2400/2800], Loss: 0.0000\n",
            "Epoch [55/100], Step [2500/2800], Loss: 0.0033\n",
            "Epoch [55/100], Step [2600/2800], Loss: 0.0018\n",
            "Epoch [55/100], Step [2700/2800], Loss: 0.0001\n",
            "Epoch [55/100], Step [2800/2800], Loss: 0.0036\n",
            "Epoch [56/100], Step [100/2800], Loss: 0.0044\n",
            "Epoch [56/100], Step [200/2800], Loss: 0.0000\n",
            "Epoch [56/100], Step [300/2800], Loss: 0.0404\n",
            "Epoch [56/100], Step [400/2800], Loss: 0.0025\n",
            "Epoch [56/100], Step [500/2800], Loss: 0.0142\n",
            "Epoch [56/100], Step [600/2800], Loss: 0.0000\n",
            "Epoch [56/100], Step [700/2800], Loss: 1.2348\n",
            "Epoch [56/100], Step [800/2800], Loss: 0.0095\n",
            "Epoch [56/100], Step [900/2800], Loss: 0.0001\n",
            "Epoch [56/100], Step [1000/2800], Loss: 0.3122\n",
            "Epoch [56/100], Step [1100/2800], Loss: 0.0000\n",
            "Epoch [56/100], Step [1200/2800], Loss: 0.0000\n",
            "Epoch [56/100], Step [1300/2800], Loss: 0.0007\n",
            "Epoch [56/100], Step [1400/2800], Loss: 0.0032\n",
            "Epoch [56/100], Step [1500/2800], Loss: 0.0001\n",
            "Epoch [56/100], Step [1600/2800], Loss: 0.8264\n",
            "Epoch [56/100], Step [1700/2800], Loss: 0.0000\n",
            "Epoch [56/100], Step [1800/2800], Loss: 0.0000\n",
            "Epoch [56/100], Step [1900/2800], Loss: 0.0012\n",
            "Epoch [56/100], Step [2000/2800], Loss: 0.0013\n",
            "Epoch [56/100], Step [2100/2800], Loss: 0.0558\n",
            "Epoch [56/100], Step [2200/2800], Loss: 0.0153\n",
            "Epoch [56/100], Step [2300/2800], Loss: 0.0000\n",
            "Epoch [56/100], Step [2400/2800], Loss: 0.0005\n",
            "Epoch [56/100], Step [2500/2800], Loss: 0.0021\n",
            "Epoch [56/100], Step [2600/2800], Loss: 0.0048\n",
            "Epoch [56/100], Step [2700/2800], Loss: 0.0017\n",
            "Epoch [56/100], Step [2800/2800], Loss: 0.0242\n",
            "Epoch [57/100], Step [100/2800], Loss: 0.0004\n",
            "Epoch [57/100], Step [200/2800], Loss: 0.0019\n",
            "Epoch [57/100], Step [300/2800], Loss: 0.0000\n",
            "Epoch [57/100], Step [400/2800], Loss: 0.0000\n",
            "Epoch [57/100], Step [500/2800], Loss: 0.1441\n",
            "Epoch [57/100], Step [600/2800], Loss: 0.0000\n",
            "Epoch [57/100], Step [700/2800], Loss: 0.0217\n",
            "Epoch [57/100], Step [800/2800], Loss: 0.0046\n",
            "Epoch [57/100], Step [900/2800], Loss: 0.0001\n",
            "Epoch [57/100], Step [1000/2800], Loss: 0.0003\n",
            "Epoch [57/100], Step [1100/2800], Loss: 0.0018\n",
            "Epoch [57/100], Step [1200/2800], Loss: 0.0902\n",
            "Epoch [57/100], Step [1300/2800], Loss: 0.0003\n",
            "Epoch [57/100], Step [1400/2800], Loss: 0.1001\n",
            "Epoch [57/100], Step [1500/2800], Loss: 0.0727\n",
            "Epoch [57/100], Step [1600/2800], Loss: 0.0005\n",
            "Epoch [57/100], Step [1700/2800], Loss: 0.0000\n",
            "Epoch [57/100], Step [1800/2800], Loss: 0.0000\n",
            "Epoch [57/100], Step [1900/2800], Loss: 0.0016\n",
            "Epoch [57/100], Step [2000/2800], Loss: 0.0002\n",
            "Epoch [57/100], Step [2100/2800], Loss: 0.0025\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [57/100], Step [2200/2800], Loss: 0.0230\n",
            "Epoch [57/100], Step [2300/2800], Loss: 0.0000\n",
            "Epoch [57/100], Step [2400/2800], Loss: 0.0036\n",
            "Epoch [57/100], Step [2500/2800], Loss: 0.0025\n",
            "Epoch [57/100], Step [2600/2800], Loss: 0.0033\n",
            "Epoch [57/100], Step [2700/2800], Loss: 0.0000\n",
            "Epoch [57/100], Step [2800/2800], Loss: 0.0298\n",
            "Epoch [58/100], Step [100/2800], Loss: 0.0004\n",
            "Epoch [58/100], Step [200/2800], Loss: 0.0011\n",
            "Epoch [58/100], Step [300/2800], Loss: 0.0001\n",
            "Epoch [58/100], Step [400/2800], Loss: 0.0026\n",
            "Epoch [58/100], Step [500/2800], Loss: 0.9079\n",
            "Epoch [58/100], Step [600/2800], Loss: 0.0000\n",
            "Epoch [58/100], Step [700/2800], Loss: 0.0022\n",
            "Epoch [58/100], Step [800/2800], Loss: 0.0001\n",
            "Epoch [58/100], Step [900/2800], Loss: 0.0000\n",
            "Epoch [58/100], Step [1000/2800], Loss: 0.0000\n",
            "Epoch [58/100], Step [1100/2800], Loss: 0.0004\n",
            "Epoch [58/100], Step [1200/2800], Loss: 0.0011\n",
            "Epoch [58/100], Step [1300/2800], Loss: 0.0014\n",
            "Epoch [58/100], Step [1400/2800], Loss: 0.1400\n",
            "Epoch [58/100], Step [1500/2800], Loss: 0.0001\n",
            "Epoch [58/100], Step [1600/2800], Loss: 0.0000\n",
            "Epoch [58/100], Step [1700/2800], Loss: 0.0000\n",
            "Epoch [58/100], Step [1800/2800], Loss: 0.0000\n",
            "Epoch [58/100], Step [1900/2800], Loss: 0.0023\n",
            "Epoch [58/100], Step [2000/2800], Loss: 0.0000\n",
            "Epoch [58/100], Step [2100/2800], Loss: 0.0013\n",
            "Epoch [58/100], Step [2200/2800], Loss: 0.0048\n",
            "Epoch [58/100], Step [2300/2800], Loss: 0.0001\n",
            "Epoch [58/100], Step [2400/2800], Loss: 0.0153\n",
            "Epoch [58/100], Step [2500/2800], Loss: 0.0620\n",
            "Epoch [58/100], Step [2600/2800], Loss: 0.0003\n",
            "Epoch [58/100], Step [2700/2800], Loss: 0.0001\n",
            "Epoch [58/100], Step [2800/2800], Loss: 0.0001\n",
            "Epoch [59/100], Step [100/2800], Loss: 0.0001\n",
            "Epoch [59/100], Step [200/2800], Loss: 0.0296\n",
            "Epoch [59/100], Step [300/2800], Loss: 0.0307\n",
            "Epoch [59/100], Step [400/2800], Loss: 0.0011\n",
            "Epoch [59/100], Step [500/2800], Loss: 0.0007\n",
            "Epoch [59/100], Step [600/2800], Loss: 0.0000\n",
            "Epoch [59/100], Step [700/2800], Loss: 0.0027\n",
            "Epoch [59/100], Step [800/2800], Loss: 0.0005\n",
            "Epoch [59/100], Step [900/2800], Loss: 0.0003\n",
            "Epoch [59/100], Step [1000/2800], Loss: 0.0000\n",
            "Epoch [59/100], Step [1100/2800], Loss: 0.0000\n",
            "Epoch [59/100], Step [1200/2800], Loss: 0.0006\n",
            "Epoch [59/100], Step [1300/2800], Loss: 0.0000\n",
            "Epoch [59/100], Step [1400/2800], Loss: 0.1492\n",
            "Epoch [59/100], Step [1500/2800], Loss: 0.0004\n",
            "Epoch [59/100], Step [1600/2800], Loss: 0.0000\n",
            "Epoch [59/100], Step [1700/2800], Loss: 0.0000\n",
            "Epoch [59/100], Step [1800/2800], Loss: 0.0000\n",
            "Epoch [59/100], Step [1900/2800], Loss: 0.0078\n",
            "Epoch [59/100], Step [2000/2800], Loss: 0.0003\n",
            "Epoch [59/100], Step [2100/2800], Loss: 0.0170\n",
            "Epoch [59/100], Step [2200/2800], Loss: 0.0146\n",
            "Epoch [59/100], Step [2300/2800], Loss: 0.0000\n",
            "Epoch [59/100], Step [2400/2800], Loss: 0.0252\n",
            "Epoch [59/100], Step [2500/2800], Loss: 0.0001\n",
            "Epoch [59/100], Step [2600/2800], Loss: 0.0228\n",
            "Epoch [59/100], Step [2700/2800], Loss: 0.0001\n",
            "Epoch [59/100], Step [2800/2800], Loss: 0.0010\n",
            "Epoch [60/100], Step [100/2800], Loss: 0.0020\n",
            "Epoch [60/100], Step [200/2800], Loss: 0.0011\n",
            "Epoch [60/100], Step [300/2800], Loss: 0.0003\n",
            "Epoch [60/100], Step [400/2800], Loss: 0.0275\n",
            "Epoch [60/100], Step [500/2800], Loss: 0.0014\n",
            "Epoch [60/100], Step [600/2800], Loss: 0.0000\n",
            "Epoch [60/100], Step [700/2800], Loss: 0.0002\n",
            "Epoch [60/100], Step [800/2800], Loss: 0.0000\n",
            "Epoch [60/100], Step [900/2800], Loss: 0.0001\n",
            "Epoch [60/100], Step [1000/2800], Loss: 0.0003\n",
            "Epoch [60/100], Step [1100/2800], Loss: 0.0000\n",
            "Epoch [60/100], Step [1200/2800], Loss: 0.0000\n",
            "Epoch [60/100], Step [1300/2800], Loss: 0.0119\n",
            "Epoch [60/100], Step [1400/2800], Loss: 0.0788\n",
            "Epoch [60/100], Step [1500/2800], Loss: 0.0002\n",
            "Epoch [60/100], Step [1600/2800], Loss: 0.0000\n",
            "Epoch [60/100], Step [1700/2800], Loss: 0.0000\n",
            "Epoch [60/100], Step [1800/2800], Loss: 0.0000\n",
            "Epoch [60/100], Step [1900/2800], Loss: 0.0009\n",
            "Epoch [60/100], Step [2000/2800], Loss: 0.0000\n",
            "Epoch [60/100], Step [2100/2800], Loss: 0.0440\n",
            "Epoch [60/100], Step [2200/2800], Loss: 0.0070\n",
            "Epoch [60/100], Step [2300/2800], Loss: 0.0000\n",
            "Epoch [60/100], Step [2400/2800], Loss: 0.0013\n",
            "Epoch [60/100], Step [2500/2800], Loss: 0.0004\n",
            "Epoch [60/100], Step [2600/2800], Loss: 0.1946\n",
            "Epoch [60/100], Step [2700/2800], Loss: 0.0000\n",
            "Epoch [60/100], Step [2800/2800], Loss: 0.0069\n",
            "Epoch [61/100], Step [100/2800], Loss: 0.0004\n",
            "Epoch [61/100], Step [200/2800], Loss: 0.0002\n",
            "Epoch [61/100], Step [300/2800], Loss: 0.0399\n",
            "Epoch [61/100], Step [400/2800], Loss: 0.0004\n",
            "Epoch [61/100], Step [500/2800], Loss: 0.0246\n",
            "Epoch [61/100], Step [600/2800], Loss: 0.0000\n",
            "Epoch [61/100], Step [700/2800], Loss: 0.0078\n",
            "Epoch [61/100], Step [800/2800], Loss: 0.0000\n",
            "Epoch [61/100], Step [900/2800], Loss: 0.0000\n",
            "Epoch [61/100], Step [1000/2800], Loss: 0.0014\n",
            "Epoch [61/100], Step [1100/2800], Loss: 0.0000\n",
            "Epoch [61/100], Step [1200/2800], Loss: 0.0001\n",
            "Epoch [61/100], Step [1300/2800], Loss: 0.0009\n",
            "Epoch [61/100], Step [1400/2800], Loss: 0.0005\n",
            "Epoch [61/100], Step [1500/2800], Loss: 0.0001\n",
            "Epoch [61/100], Step [1600/2800], Loss: 0.0000\n",
            "Epoch [61/100], Step [1700/2800], Loss: 0.0000\n",
            "Epoch [61/100], Step [1800/2800], Loss: 0.0000\n",
            "Epoch [61/100], Step [1900/2800], Loss: 0.0016\n",
            "Epoch [61/100], Step [2000/2800], Loss: 0.0005\n",
            "Epoch [61/100], Step [2100/2800], Loss: 0.0025\n",
            "Epoch [61/100], Step [2200/2800], Loss: 0.0099\n",
            "Epoch [61/100], Step [2300/2800], Loss: 0.0000\n",
            "Epoch [61/100], Step [2400/2800], Loss: 0.0000\n",
            "Epoch [61/100], Step [2500/2800], Loss: 0.0009\n",
            "Epoch [61/100], Step [2600/2800], Loss: 0.0002\n",
            "Epoch [61/100], Step [2700/2800], Loss: 0.0001\n",
            "Epoch [61/100], Step [2800/2800], Loss: 0.0122\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f0edf3cb071b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# calculate the loss and backpropagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# update the model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "class TCLSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(TCLSTMClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm1 = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size*2, num_classes)  # Concatenate output from both LSTMs\n",
        "    \n",
        "    def forward(self, x1,x2):\n",
        "        lstm_out1, _ = self.lstm1(x1)\n",
        "        lstm_out2, _ = self.lstm2(x2)\n",
        "        lstm_out = torch.cat((lstm_out1[:, -1, :], lstm_out2[:, -1, :]), dim=1)  # Concatenate output from both LSTMs\n",
        "        out = self.fc(lstm_out)\n",
        "        return out\n",
        "input_size = 1636\n",
        "hidden_size = 200\n",
        "num_classes = 3\n",
        "learning_rate = 0.001\n",
        "num_epochs = 100\n",
        "batch_size = 1\n",
        "\n",
        "# Initialize the model and optimizer\n",
        "tcmodel = TCLSTMClassifier(input_size, hidden_size, num_classes)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = optim.Adam(tcmodel.parameters(), lr=learning_rate)\n",
        "\n",
        "# assuming you have your data in the form of input pairs and labels:\n",
        "# input_pairs is a list of pairs of input sequences (x1, x2)\n",
        "# labels is a list of corresponding labels\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(len(inputPairs[:2800])):\n",
        "        # get the current input pair and label\n",
        "        x1, x2 = inputPairs[i][0] , inputPairs[i][1]\n",
        "        label1 = label[i]\n",
        "        \n",
        "        # convert input and label to PyTorch tensors\n",
        "        label1 = torch.LongTensor([label1])\n",
        "        \n",
        "        # zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # forward pass through the model\n",
        "        output = tcmodel(torch.FloatTensor(x1).unsqueeze(0), torch.FloatTensor(x2).unsqueeze(0))\n",
        "        \n",
        "        # calculate the loss and backpropagate\n",
        "        loss = criterion(output, label1)\n",
        "        loss.backward(retain_graph=True)\n",
        "        \n",
        "        # update the model parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        # print the loss every 100 steps\n",
        "        if (i+1) % 100 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, len(inputPairs), loss.item()))\n",
        "\n",
        "torch.save(tcmodel.state_dict(), 'tc-lstm.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aktkaTeAY2R5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SI9dFfJY2R5"
      },
      "outputs": [],
      "source": [
        "torch.save(tcmodel.state_dict(), 'tc-lstm.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTtpZGpyY2R5"
      },
      "source": [
        "# Results: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqI9EXSoY2R5"
      },
      "source": [
        "## LSTM "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aCMzmYjY2R5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Define the LSTM model\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        last_hidden_state = lstm_out[:, -1, :]\n",
        "        out = self.fc(last_hidden_state)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Define the hyperparameters\n",
        "input_size = 868\n",
        "hidden_size = 64\n",
        "num_classes = 3\n",
        "learning_rate = 0.001\n",
        "num_epochs = 100\n",
        "batch_size = 32\n",
        "\n",
        "# Initialize the model and optimizer\n",
        "model = LSTMClassifier(input_size, hidden_size, num_classes)\n",
        "def predicted(sen):\n",
        "  # sen = df['Sentence'][1]\n",
        "    t1 = np.array(indomain_vector(sen))\n",
        "    t2 = bert_to_token(sen).detach().numpy()\n",
        "    v = np.concatenate((t1, t2), axis=1)\n",
        "    test_sentence = v\n",
        "    padded_sentence = np.zeros((1, max_length, 868))\n",
        "    padded_sentence[0, :len(test_sentence), :] = test_sentence\n",
        "\n",
        "    # Convert the padded sentence to a PyTorch tensor\n",
        "    padded_sentence = torch.tensor(padded_sentence, dtype=torch.float32)\n",
        "    model.load_state_dict(torch.load('lstm.pt'))\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "    # Pass the padded sentence through the trained model to get the predicted class\n",
        "    with torch.no_grad():\n",
        "        output = model(padded_sentence)\n",
        "        predicted_class = torch.argmax(output).item()\n",
        "\n",
        "    # Print the predicted class\n",
        "    return predicted_class\n",
        "\n",
        "s = df['Sentence']\n",
        "predict = []\n",
        "count = 0\n",
        "actual = []\n",
        "for i in range(2800,3600):\n",
        "    p =predicted(s[i])\n",
        "    predict.append(p)\n",
        "    actual.append(df['polarity'][i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mz2TlA0Y2R5"
      },
      "source": [
        "#### CONFUSION MATRIX (LSTM)  and scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "os7-1bHuY2R6"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# calculate confusion matrix\n",
        "cm = confusion_matrix(actual, predict)\n",
        "\n",
        "# extract TP, TN, FP, FN\n",
        "TP = cm[1,1]\n",
        "TN = cm[0,0]\n",
        "FP = cm[0,1]\n",
        "FN = cm[1,0]\n",
        "\n",
        "# calculate PP and PN\n",
        "PP = TP + FN\n",
        "PN = TN + FP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzO8QogbY2R6",
        "outputId": "a0ce081c-d94c-4332-8e9e-16ca28efd8c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision:  0.39218906249999996\n",
            "Recall:  0.62625\n",
            "F1 Score:  0.48232321291314373\n",
            "Accuracy:  0.7779503105590062\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# calculate precision, recall, and F1 score\n",
        "precision = precision_score(actual, predict,average='weighted')\n",
        "recall = recall_score(actual, predict,average='weighted')\n",
        "f1 = f1_score(actual, predict,average='weighted')\n",
        "\n",
        "# print results\n",
        "print(\"Precision: \", precision)\n",
        "print(\"Recall: \", recall)\n",
        "print(\"F1 Score: \", f1)\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "print(\"Accuracy: \", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SibpShoY2R6"
      },
      "source": [
        "## TD-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTwcaHPWY2R6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "class TDLSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(TDLSTMClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm1 = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size*2, num_classes)  # Concatenate output from both LSTMs\n",
        "    \n",
        "    def forward(self, x1,x2):\n",
        "        lstm_out1, _ = self.lstm1(x1)\n",
        "        lstm_out2, _ = self.lstm2(x2)\n",
        "        lstm_out = torch.cat((lstm_out1[:, -1, :], lstm_out2[:, -1, :]), dim=1)  # Concatenate output from both LSTMs\n",
        "        out = self.fc(lstm_out)\n",
        "        return out\n",
        "#_______________________________________________________________________________________________________________________________\n",
        "input_size = 868\n",
        "hidden_size = 64\n",
        "num_classes = 3\n",
        "learning_rate = 0.001\n",
        "num_epochs = 100\n",
        "batch_size = 1\n",
        "\n",
        "# Initialize the model and optimizer\n",
        "tdmodel = TDLSTMClassifier(input_size, hidden_size, num_classes)\n",
        "def test_tdlstm(x1_new,x2_new):\n",
        "    # Initialize the model with the same parameters as before\n",
        "    tdmodel = TDLSTMClassifier(input_size, hidden_size, num_classes)\n",
        "\n",
        "    # Load the saved state dictionary into the model\n",
        "    tdmodel.load_state_dict(torch.load('td-lstm.pt'))\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    tdmodel.eval()\n",
        "    # make a forward pass through the model to obtain the predicted output\n",
        "    with torch.no_grad():\n",
        "        predicted_output = tdmodel(torch.FloatTensor(x1_new).unsqueeze(0), torch.FloatTensor(x2_new).unsqueeze(0))\n",
        "\n",
        "    # get the predicted class index (the class with the highest score)\n",
        "    predicted_class = torch.argmax(predicted_output, dim=1).item()\n",
        "    return predicted_class\n",
        "#_______________________________________________________________________________________________________________________________________\n",
        "def ind(sentence,aspect_term):\n",
        "    sentence = splitSentence(sentence)\n",
        "    aspect_term = splitSentence(aspect_term)\n",
        "    try:\n",
        "        start_index = sentence.index(aspect_term[0])\n",
        "        end_index = start_index + len(aspect_term) - 1\n",
        "    except ValueError:\n",
        "        start_index = len(sentence) - 1\n",
        "        end_index = start_index -1\n",
        "    return [start_index,end_index]\n",
        "#________________________________________________________________________________________________________________________________________\n",
        "def predict_tdlstm(sentence,aspect_term):\n",
        "    ten1 = np.array(indomain_vector(sentence))\n",
        "    ten2 = bert_to_token(sentence).detach().numpy()\n",
        "    vec = np.concatenate((ten1, ten2), axis=1)\n",
        "    s2 = np.array(vec)\n",
        "    answer = []\n",
        "    indices = ind(sentence,aspect_term)\n",
        "    i1 = indices[0]\n",
        "    j1 = indices[1]\n",
        "    answer.append(s2[:i1+1])\n",
        "    answer.append(s2[j1-1:])\n",
        "    p = test_tdlstm(answer[0],answer[1])\n",
        "    return p\n",
        "\n",
        "td_actual = []\n",
        "td_predict = []\n",
        "count = 0\n",
        "for i in range(2800,3600):\n",
        "    p = predict_tdlstm(df['Sentence'][i] , df['Aspect Term'][i])\n",
        "    td_predict.append(p)\n",
        "    td_actual.append(df['polarity'][i])\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AB0dmIlnY2R6"
      },
      "source": [
        "#### CONFUSION MATRIX (TD-LSTM)  and scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikYPNaW5Y2R7",
        "outputId": "dd4e1565-6cb2-4492-cedf-a1836a3ad4a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True Positives (TP):  378\n",
            "True Negatives (TN):  56\n",
            "False Positives (FP):  53\n",
            "False Negatives (FN):  64\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# calculate confusion matrix\n",
        "cm = confusion_matrix(td_actual, td_predict)\n",
        "\n",
        "# extract TP, TN, FP, FN\n",
        "TP = cm[1,1]\n",
        "TN = cm[0,0]\n",
        "FP = cm[0,1]\n",
        "FN = cm[1,0]\n",
        "\n",
        "# calculate PP and PN\n",
        "PP = TP + FN\n",
        "PN = TN + FP\n",
        "\n",
        "# print results\n",
        "print(\"True Positives (TP): \", TP)\n",
        "print(\"True Negatives (TN): \", TN)\n",
        "print(\"False Positives (FP): \", FP)\n",
        "print(\"False Negatives (FN): \", FN)\n",
        "\n",
        "# create heatmap using seaborn\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aA2VOgbJY2R7",
        "outputId": "59a7437b-79c0-4175-b401-c677adb7860c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision:  0.6456086470066131\n",
            "Recall:  0.635\n",
            "F1 Score:  0.6398643954657887\n",
            "Accuracy:  0.7783882783882784\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "class_weights = {0: 1, 1: 1, 2: 1}\n",
        "# calculate precision, recall, and F1 score\n",
        "precision = precision_score(td_actual, td_predict,average='weighted',sample_weight=[class_weights[label] for label in tc_actual])\n",
        "recall = recall_score(td_actual, td_predict,average='weighted',sample_weight=[class_weights[label] for label in tc_actual])\n",
        "f1 = f1_score(td_actual, td_predict,average='weighted',sample_weight=[class_weights[label] for label in tc_actual])\n",
        "\n",
        "# print results\n",
        "print(\"Precision: \", precision)\n",
        "print(\"Recall: \", recall)\n",
        "print(\"F1 Score: \", f1)\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "print(\"Accuracy: \", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiekUDkGY2R7"
      },
      "source": [
        "## TC-LSTM "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QehwSFYY2R7",
        "outputId": "f1cf4a04-2296-490d-f4fe-fff88b3551a2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-1c5b0a9fae71>:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  tensor1 = torch.tensor(embeddings[0])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "class TCLSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(TCLSTMClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm1 = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size*2, num_classes)  # Concatenate output from both LSTMs\n",
        "    \n",
        "    def forward(self, x1,x2):\n",
        "        lstm_out1, _ = self.lstm1(x1)\n",
        "        lstm_out2, _ = self.lstm2(x2)\n",
        "        lstm_out = torch.cat((lstm_out1[:, -1, :], lstm_out2[:, -1, :]), dim=1)  # Concatenate output from both LSTMs\n",
        "        out = self.fc(lstm_out)\n",
        "        return out\n",
        "#_______________________________________________________________________________________________________________________________\n",
        "input_size = 1636\n",
        "hidden_size = 200\n",
        "num_classes = 3\n",
        "learning_rate = 0.001\n",
        "num_epochs = 100\n",
        "batch_size = 1\n",
        "\n",
        "# Initialize the model and optimizer\n",
        "tcmodel = TCLSTMClassifier(input_size, hidden_size, num_classes)\n",
        "def test_tclstm(x1_new,x2_new):\n",
        "    # Initialize the model with the same parameters as before\n",
        "    tcmodel = TCLSTMClassifier(input_size, hidden_size, num_classes)\n",
        "\n",
        "    # Load the saved state dictionary into the model\n",
        "    tcmodel.load_state_dict(torch.load('tc-lstm.pt'))\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    tcmodel.eval()\n",
        "    # make a forward pass through the model to obtain the predicted output\n",
        "    with torch.no_grad():\n",
        "        predicted_output = tcmodel(torch.FloatTensor(x1_new).unsqueeze(0), torch.FloatTensor(x2_new).unsqueeze(0))\n",
        "\n",
        "    # get the predicted class index (the class with the highest score)\n",
        "    predicted_class = torch.argmax(predicted_output, dim=1).item()\n",
        "    return predicted_class\n",
        "#_______________________________________________________________________________________________________________________________________\n",
        "def ind(sentence,aspect_term):\n",
        "    sentence = splitSentence(sentence)\n",
        "    aspect_term = splitSentence(aspect_term)\n",
        "    try:\n",
        "        start_index = sentence.index(aspect_term[0])\n",
        "        end_index = start_index + len(aspect_term) - 1\n",
        "    except ValueError:\n",
        "        start_index = len(sentence) - 1\n",
        "        end_index = start_index -1\n",
        "    return [start_index,end_index]\n",
        "#________________________________________________________________________________________________________________________________________\n",
        "def create_embeddings(sentences):\n",
        "    embeddings = []\n",
        "    for sentence in sentences:\n",
        "        ten1 = indomain_vector(sentence)\n",
        "        ten2 = bert_to_token(sentence).detach()\n",
        "        vec = torch.cat([ten1, ten2], dim=1).numpy()\n",
        "        embeddings.append(vec)\n",
        "\n",
        "    embeddings_array = np.array(embeddings)\n",
        "    return torch.from_numpy(embeddings_array)\n",
        "#_________________________________________________________________________________________________________________________________\n",
        "def predict_tclstm(sentence,aspect_term):\n",
        "    a = splitSentence(aspect_term)\n",
        "    s = 0\n",
        "    count = 0\n",
        "    for j in a:\n",
        "        s =  s + bert_to_token(j)\n",
        "        count = count + 1\n",
        "    embeddings = create_embeddings([sentence])\n",
        "    tensor1 = torch.tensor(embeddings[0])\n",
        "    tensor2 = s/count\n",
        "\n",
        "    tensor2_repeated = tensor2.repeat(tensor1.shape[0], 1)\n",
        "    tensor3 = torch.cat([tensor1, tensor2_repeated], dim=1)\n",
        "    s2 = tensor3.detach().numpy()\n",
        "    answer = []\n",
        "    indices = ind(sentence,aspect_term)\n",
        "    i1 = indices[0]\n",
        "    j1 = indices[1]\n",
        "    answer.append(s2[:i1+1])\n",
        "    answer.append(s2[j1-1:])\n",
        "    p = test_tclstm(answer[0],answer[1])\n",
        "    return p\n",
        "\n",
        "tc_actual = []\n",
        "tc_predict = []\n",
        "count = 0\n",
        "for i in range(2800,3600):\n",
        "    p = predict_tclstm(df['Sentence'][i] , df['Aspect Term'][i])\n",
        "    tc_predict.append(p)\n",
        "    tc_actual.append(df['polarity'][i])\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtIwlRy0Y2R8"
      },
      "source": [
        "#### CONFUSION MATRIX(TC-LSTM) and scores "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0DwVL9RY2R8",
        "outputId": "722121eb-e900-47cf-f48a-da9d575f5751"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True Positives (TP):  381\n",
            "True Negatives (TN):  44\n",
            "False Positives (FP):  73\n",
            "False Negatives (FN):  48\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# calculate confusion matrix\n",
        "cm = confusion_matrix(tc_actual, tc_predict)\n",
        "\n",
        "# extract TP, TN, FP, FN\n",
        "TP = cm[1,1]\n",
        "TN = cm[0,0]\n",
        "FP = cm[0,1]\n",
        "FN = cm[1,0]\n",
        "\n",
        "# calculate PP and PN\n",
        "PP = TP + FN\n",
        "PN = TN + FP\n",
        "\n",
        "# print results\n",
        "print(\"True Positives (TP): \", TP)\n",
        "print(\"True Negatives (TN): \", TN)\n",
        "print(\"False Positives (FP): \", FP)\n",
        "print(\"False Negatives (FN): \", FN)\n",
        "\n",
        "# create heatmap using seaborn\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqKUtJyIY2R8",
        "outputId": "59ceccf9-4a47-49bb-fce2-fc26b2383fc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision:  0.6854078047630807\n",
            "Recall:  0.6610977644922857\n",
            "F1 Score:  0.6709713628793061\n",
            "Accuracy:  0.7783882783882784\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#_____________________________________________________________________________________________________________________________________\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "class_weights = {0: 1, 1: 1.58, 2: 1}\n",
        "# calculate precision, recall, and F1 score\n",
        "precision = precision_score(tc_actual, tc_predict,average='weighted',sample_weight=[class_weights[label] for label in tc_actual])\n",
        "recall = recall_score(tc_actual, tc_predict,average='weighted',sample_weight=[class_weights[label] for label in tc_actual])\n",
        "f1 = f1_score(tc_actual, tc_predict,average='weighted',sample_weight=[class_weights[label] for label in tc_actual])\n",
        "\n",
        "# print results\n",
        "print(\"Precision: \", precision)\n",
        "print(\"Recall: \", recall)\n",
        "print(\"F1 Score: \", f1)\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "print(\"Accuracy: \", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZc-94rDY2R9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56n6fEIEY2R9"
      },
      "source": [
        "### Finetuning BERT "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNXj75mZY2R9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJUOfLfkY2R9"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Download the pre-trained Word2Vec model from Google\n",
        "w2v_model = api.load('word2vec-google-news-300')\n",
        "\n",
        "# Define the function to split sentences into words/tokens using Word2Vec\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Download the pre-trained Word2Vec model from Google\n",
        "w2v_model = api.load('word2vec-google-news-300')\n",
        "\n",
        "# Define the function to split sentences into words/tokens using Word2Vec\n",
        "def split_sentence(sentence):\n",
        "    words = []\n",
        "    for word in splitSentence(sentence):\n",
        "        if word in w2v_model:\n",
        "            words.append(word)\n",
        "    return words\n",
        "\n",
        "def word2vec(sentence):\n",
        "    words = splitSentence(sentence)\n",
        "\n",
        "    # Get the word vectors for the words in the sentence\n",
        "    vectors = [w2v_model[word] for word in words]\n",
        "    return words,vectors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFiEL7u7Y2R9",
        "outputId": "b2c5fc41-81bf-4fd0-fd33-30023100188d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num_epochs =  5 and loss= 0.6676302552223206\n",
            "num_epochs =  5 and loss= 0.6887586116790771\n",
            "num_epochs =  5 and loss= 0.3197222948074341\n",
            "num_epochs =  5 and loss= 0.32158613204956055\n",
            "num_epochs =  5 and loss= 0.4464018940925598\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification , AutoTokenizer , AutoModelForSequenceClassification\n",
        "\n",
        "# Load the pre-trained BERT model and tokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels=3)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define your training data\n",
        "sentences = list(df['Sentence'][:2800])\n",
        "aspect_terms = list(df['Aspect Term'][:2800])\n",
        "labels = list(df['polarity'][:2800])\n",
        "# Tokenize the sentences and aspect terms\n",
        "inputs = tokenizer(sentences, aspect_terms, padding=True, truncation=True, max_length=90, return_tensors='pt')\n",
        "\n",
        "\n",
        "# Create a PyTorch Dataset\n",
        "dataset = torch.utils.data.TensorDataset(inputs['input_ids'], inputs['attention_mask'], inputs['token_type_ids'], torch.tensor(labels))\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the model\n",
        "model.train()\n",
        "num_epochs=5\n",
        "batch_size = 32\n",
        "for epoch in range(num_epochs):\n",
        "    i = 0\n",
        "    for batch in torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True):\n",
        "        input_ids, attention_mask, token_type_ids, label_ids = batch\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=label_ids)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        i = i + 1\n",
        "    print(\"num_epochs = \",num_epochs,\"and loss=\",loss.item())\n",
        "        \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLmerO9yY2R-"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'bert-finetune.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_QBMLMGY2R-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Use the model to predict the sentiment polarity for new data\n",
        "new_sentences = list(df['Sentence'][2800:])\n",
        "new_aspect_terms = list(df['Aspect Term'][2800:])\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    inputs = tokenizer(new_sentences, new_aspect_terms, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
        "    outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], token_type_ids=inputs['token_type_ids'])\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ci43B1gRY2R-"
      },
      "outputs": [],
      "source": [
        "c = 0\n",
        "actual = list(df['polarity'][2800:])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqEXLE5BY2R-"
      },
      "source": [
        "### Result discussion about BERT finetuning "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZU05EKPHY2R-",
        "outputId": "f6616fd2-1700-48b4-a452-906f3bae3b22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True Positives (TP):  471\n",
            "True Negatives (TN):  65\n",
            "False Positives (FP):  66\n",
            "False Negatives (FN):  22\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# calculate confusion matrix\n",
        "cm = confusion_matrix(actual, predictions)\n",
        "\n",
        "# extract TP, TN, FP, FN\n",
        "TP = cm[1,1]\n",
        "TN = cm[0,0]\n",
        "FP = cm[0,1]\n",
        "FN = cm[1,0]\n",
        "\n",
        "# calculate PP and PN\n",
        "PP = TP + FN\n",
        "PN = TN + FP\n",
        "\n",
        "# print results\n",
        "print(\"True Positives (TP): \", TP)\n",
        "print(\"True Negatives (TN): \", TN)\n",
        "print(\"False Positives (FP): \", FP)\n",
        "print(\"False Negatives (FN): \", FN)\n",
        "\n",
        "# create heatmap using seaborn\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMm04zw4Y2R_",
        "outputId": "adc6cc8f-6c32-4086-e537-59ac5159d10b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision:  0.7207690431543124\n",
            "Recall:  0.716852392586856\n",
            "F1 Score:  0.692946108287391\n",
            "Accuracy:  0.8589743589743589\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "class_weights = {0: 1, 1: 0.58, 2: 1}\n",
        "precision = precision_score(l, predictions,average='weighted',sample_weight=[class_weights[label] for label in actual])\n",
        "recall = recall_score(l, predictions,average='weighted',sample_weight=[class_weights[label] for label in actual])\n",
        "f1 = f1_score(l, predictions,average='weighted',sample_weight=[class_weights[label] for label in actual])\n",
        "\n",
        "# print results\n",
        "print(\"Precision: \", precision)\n",
        "print(\"Recall: \", recall)\n",
        "print(\"F1 Score: \", f1)\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "print(\"Accuracy: \", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUZ3NyPxY2R_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07afa3bae8eb4d499ba44057bf4d1729": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0afcd6aedf2245f4aa4a41f00f7992a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_139e87f0f91647b9a31c2800b59899de",
            "placeholder": "​",
            "style": "IPY_MODEL_07afa3bae8eb4d499ba44057bf4d1729",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "0cc7c03a57ba4569b8af7a4939206bc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0dd48d60237d486c8c0a274e69309e89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d43202fd4d34479db3e1a6f573cbb210",
            "placeholder": "​",
            "style": "IPY_MODEL_2de829e4e6b7462abf294ee5a6b86cf0",
            "value": " 232k/232k [00:00&lt;00:00, 893kB/s]"
          }
        },
        "11183adc0a6b4662983dfb5fe0d86f9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4df0aa828964dcbac960fca72bd88c7",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_28a630d8e5a145d9abbf1ca944809868",
            "value": 570
          }
        },
        "139e87f0f91647b9a31c2800b59899de": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1abf1d792e11463188f323cd0468656c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "24fb0386d894454aa5c75299875fc4f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b88ed6bc4c63441a891d4f7fecae8ff6",
            "placeholder": "​",
            "style": "IPY_MODEL_a6df883a88574552b0daa35e53a0dc55",
            "value": " 28.0/28.0 [00:00&lt;00:00, 631B/s]"
          }
        },
        "28a630d8e5a145d9abbf1ca944809868": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2aa065965e8047fbb6a39dc0d80f30d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c29233516d340288f928763a87667a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2de829e4e6b7462abf294ee5a6b86cf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "335deb6c40f64a63a0347a74b00a8449": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c732965b90e446fba84ad20c5b007cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_421e09afa9b94cc88eb698dfeedb5457",
              "IPY_MODEL_6e719729b0ef4bf0b8d422ed5855c677",
              "IPY_MODEL_24fb0386d894454aa5c75299875fc4f0"
            ],
            "layout": "IPY_MODEL_5bb2ba2488674762b152c3621f0adcd8"
          }
        },
        "421e09afa9b94cc88eb698dfeedb5457": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2aa065965e8047fbb6a39dc0d80f30d1",
            "placeholder": "​",
            "style": "IPY_MODEL_981831e97d174d45a3b5550a71569fa8",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "495b218b6b6a4c59bb2e328ed83c2583": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7379fa65626146fab080586dd3e45995",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d4386ef0bfd4894bc243879d5c14d79",
            "value": 231508
          }
        },
        "49d74b5216f54cb78e7a0802dc11e25e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b612366b70b4d26925a89674395f168": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55749590bc144702ab5eb1de41c32c1d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bb2ba2488674762b152c3621f0adcd8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d4386ef0bfd4894bc243879d5c14d79": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6e719729b0ef4bf0b8d422ed5855c677": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ea01a1d38a54aa2b384fbf1cb4462ec",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a0cba292c6e44af68dde328d9b90a348",
            "value": 28
          }
        },
        "6ea01a1d38a54aa2b384fbf1cb4462ec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ef75cefa44e45c9a6d3fd89b538ec2f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ff01c1322cd4e2ca267be063ca3ce19": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7379fa65626146fab080586dd3e45995": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ec3c94a0f6c4dafa8442ef23e9522da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0afcd6aedf2245f4aa4a41f00f7992a6",
              "IPY_MODEL_11183adc0a6b4662983dfb5fe0d86f9d",
              "IPY_MODEL_bb28c4e3770a4c06bdbc2f5a10a79bbc"
            ],
            "layout": "IPY_MODEL_55749590bc144702ab5eb1de41c32c1d"
          }
        },
        "7ef9684cb3f343048e971f151beea119": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83b56a0d7d274f9ca7feb29cde8c19d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91d2d06fdd624999aa54b226246f8b55",
            "placeholder": "​",
            "style": "IPY_MODEL_4b612366b70b4d26925a89674395f168",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "91d2d06fdd624999aa54b226246f8b55": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "981831e97d174d45a3b5550a71569fa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0cba292c6e44af68dde328d9b90a348": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a6df883a88574552b0daa35e53a0dc55": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4d567d4cd9d413bb19ea4868a66350d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de217e7d59c243b1a902585819ffdc89",
            "placeholder": "​",
            "style": "IPY_MODEL_2c29233516d340288f928763a87667a9",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "b88ed6bc4c63441a891d4f7fecae8ff6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb28c4e3770a4c06bdbc2f5a10a79bbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49d74b5216f54cb78e7a0802dc11e25e",
            "placeholder": "​",
            "style": "IPY_MODEL_cdfe7f3f08a9423eadccefd54736ff6b",
            "value": " 570/570 [00:00&lt;00:00, 5.95kB/s]"
          }
        },
        "c4e23b59cb6f4b079b83300d20e28fd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b4d567d4cd9d413bb19ea4868a66350d",
              "IPY_MODEL_495b218b6b6a4c59bb2e328ed83c2583",
              "IPY_MODEL_0dd48d60237d486c8c0a274e69309e89"
            ],
            "layout": "IPY_MODEL_335deb6c40f64a63a0347a74b00a8449"
          }
        },
        "cdfe7f3f08a9423eadccefd54736ff6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d43202fd4d34479db3e1a6f573cbb210": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d89c852473614128befc36221213b715": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_83b56a0d7d274f9ca7feb29cde8c19d2",
              "IPY_MODEL_e95f67a7379f44c888d993931b685eed",
              "IPY_MODEL_e4024ea7de2347169bb3600a15f2a64a"
            ],
            "layout": "IPY_MODEL_7ef9684cb3f343048e971f151beea119"
          }
        },
        "de217e7d59c243b1a902585819ffdc89": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4024ea7de2347169bb3600a15f2a64a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ff01c1322cd4e2ca267be063ca3ce19",
            "placeholder": "​",
            "style": "IPY_MODEL_0cc7c03a57ba4569b8af7a4939206bc0",
            "value": " 440M/440M [00:04&lt;00:00, 77.4MB/s]"
          }
        },
        "e95f67a7379f44c888d993931b685eed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ef75cefa44e45c9a6d3fd89b538ec2f",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1abf1d792e11463188f323cd0468656c",
            "value": 440473133
          }
        },
        "f4df0aa828964dcbac960fca72bd88c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}